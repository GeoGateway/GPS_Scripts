head	1.15;
access;
symbols;
locks
	granat:1.15; strict;
comment	@ * @;


1.15
date	2000.02.24.18.50.54;	author granat;	state Exp;
branches;
next	1.14;

1.14
date	98.06.30.17.23.23;	author agray;	state Exp;
branches;
next	1.13;

1.13
date	98.04.21.17.07.35;	author roden;	state Exp;
branches;
next	1.12;

1.12
date	98.03.09.00.38.36;	author granat;	state Exp;
branches;
next	1.11;

1.11
date	97.06.05.18.55.46;	author granat;	state Exp;
branches;
next	1.10;

1.10
date	97.06.02.15.35.17;	author granat;	state Exp;
branches;
next	1.9;

1.9
date	97.04.05.19.16.58;	author granat;	state Exp;
branches;
next	1.8;

1.8
date	97.01.29.21.44.05;	author agray;	state Exp;
branches;
next	1.7;

1.7
date	96.10.31.02.06.43;	author agray;	state Exp;
branches;
next	1.6;

1.6
date	96.10.31.00.23.18;	author agray;	state Exp;
branches;
next	1.5;

1.5
date	96.09.23.23.57.23;	author agray;	state Exp;
branches;
next	1.4;

1.4
date	96.09.23.22.40.46;	author agray;	state Exp;
branches;
next	1.3;

1.3
date	96.07.19.17.53.03;	author agray;	state Exp;
branches;
next	1.2;

1.2
date	96.07.17.20.42.25;	author agray;	state Exp;
branches;
next	1.1;

1.1
date	96.07.11.16.42.35;	author agray;	state Exp;
branches;
next	;


desc
@Parallel versions of functions in da_clust.c
@


1.15
log
@changed communications to use MPI_Allreduce
this increases communication speed
@
text
@/*******************************************************************************
MODULE NAME
da_cluster_pp

ONE-LINE SYNOPSIS
Parallel functions related to clustering.

SCOPE OF THIS MODULE
Analogous to da_cluster in scope, except that functions in this module are
written to run on a parallel machine.  

SEE ALSO
Most or all of the functions in this module are mirrors of their serial versions
in da_cluster.

REFERENCE(S)
-

NOTES
-

AG
*******************************************************************************/
#ifndef lint
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.14 1998/06/30 17:23:23 agray Exp granat $";
#endif
/* This string variable allows the RCS identification info to be printed. */

/* 
 * $Log: da_cluster_pp.c,v $
 * Revision 1.14  1998/06/30 17:23:23  agray
 * changed includes.
 *
 * Revision 1.13  1998/04/21 17:07:35  roden
 * Took out #include <pvm3.h> because I think it's not needed here, and it
 * doesn't conform to da_platform.h control over presence/absence of pvm.
 *
 * Revision 1.12  1998/03/09 00:38:36  granat
 * fixed boolean bug
 *
 * Revision 1.11  1997/06/05 18:55:46  granat
 * edited to conform to changes in da_linalg
 *
 * Revision 1.10  1997/06/02 15:35:17  granat
 * changed to use new NR naming convention
 *
 * Revision 1.9  1997/04/05 19:16:58  granat
 * made adjustments to account for changes in da_linalg
 *
 * Revision 1.8  1997/01/29 21:44:05  agray
 * new formatting, cleaning up debugging output using ut_output.
 *
 * Revision 1.7  1996/10/31 02:06:43  agray
 * renamed from "da_clust_pp" to "da_cluster_pp";
 * changed .h and .c formats throughout library;
 * 
 * Revision 1.6  1996/10/31 00:23:18  agray
 * changed naming from "mixmod" to "mixture"; other naming changes.
 * 
 * Revision 1.5  1996/09/23 23:57:23  agray
 * updated utRandomPick() to use char's for selected array.
 * 
 * Revision 1.4  1996/09/23 22:40:46  agray
 * minor changes.
 * 
 * Revision 1.3  1996/07/19 17:53:03  agray
 * added random_means_pp(), started random_means_from_data_pp()
 * 
 * Revision 1.2  1996/07/17 20:42:25  agray
 * cosmetic.
 * 
 * Revision 1.1  1996/07/11 16:42:35  agray
 * Initial revision
 * 
 * */

/* C library */
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <float.h>

/* MPI library */
#include "mpi.h"

/* UT library */
#include "ut_types.h"
#include "ut_error.h"
#include "ut_output.h"

/* NR library */
#include "nr.h"

/* DA library */
#include "da_cluster.h"
#include "da_comm_pp.h"
#include "da_linalg.h"
#include "da_random.h"
#include "da_util.h"
#include "da_io.h"
#include "da_probstat.h"

/* this module's header */
#include "da_cluster_pp.h"


/*******************************************************************************
K_MEANS_PP
Parallel version of k_means().
Perform the k-means clustering algorithm on a dataset of continuous values,
returning the matrix of k mean vectors and a class labelling for each of the
data.  The input matrices and vectors are all assumed to have been allocated
before calling this function.  The argument initmeans should contain the
mean vectors which will start off the k-means iterations.  The data is assumed
to be stored in row vector form (i.e. each row is a datum).
When the function is finished, each of the processors has the final means.
Each processor will end up with its own chunk of the labels file.

NOTE: In practice, it is a good idea to normalize the columns of the data to
the same scale before performing k-means (see the function normalize_data()).
AG
*******************************************************************************/
int k_means_pp(double **data, int numrows, int numcols, double **means, 
               double **initmeans, int *clustsize, int *class, int K,
               int *numiters, int *num_empty_clusters, boolean empty_cluster_ok,
               int pe, int numPE)

{
    double          **oldmeans, **tempmeans, **othermeans;
    int            *otherclustsize;
    int            i, j, k, p, cc;
    int            bestclass;
    double          dist, mindist;
    int            stable, otherstable;
    int            empty_clusters, iters;

    if (verbose_output() && (pe == 0))
      log_printf("%d: beginning k_means_pp\n",pe);

    /* make storage related to message passing */
    otherclustsize = NR_ivector(1,K);
    othermeans = NR_dmatrix(1,K,1,numcols);

    /* initialize before iterating */
    stable = UT_FALSE;
    empty_clusters = 0;
    iters = 0;
    fast_set_ivec(class, numrows, 1);

    /* create storage for temporary information */
    oldmeans = initmeans;

    if (debug_output() && (pe == 0))
      log_printf("%d: beginning k_means_pp iterations\n",pe);

    /* perform k-means iterations */
    while (stable == UT_FALSE)
    {
      if (debug_output() && (pe == 0))
        log_printf("%d: iteration number %d\n",pe,iters);

      stable = UT_TRUE;

      /* zero out new means */
      for (k=1; k<=K; k++)
        for (j=1; j<=numcols; j++)
          means[k][j] = 0;

      /* zero out new cluster sizes */
      for (k=1; k<=K; k++)
        clustsize[k] = 0;

      /* compute distance of each datum to each mean */
      for (i=1; i<=numrows; i++)
      {
        mindist = euclid_dist_dvec(data[i],oldmeans[1],numcols);
        bestclass = 1;
          /* 
            log_printf("item %d to mean %d: %f\n",
                   i,1,euclid_dist(data[i],oldmeans[1],numcols));
             */
        for (k=2; k<=K; k++)
        {
          /*
            log_printf("item %d to mean %d: %f\n",
                   i,k,euclid_dist(data[i],oldmeans[k],numcols));
           */

          if ((dist = euclid_dist_dvec(data[i],oldmeans[k],numcols)) < mindist)
          {
            mindist = dist;
            bestclass = k;
          }
        }

        /* assign class and see if different from last iteration */
        if (class[i] != bestclass)
        {
          class[i] = bestclass;
          stable = UT_FALSE;
        }


        /* contribute to the new mean based on class assignment */
        for (j=1; j<=numcols; j++)
          means[class[i]][j] += data[i][j];
        clustsize[class[i]] += 1;

      }

      /* keep track of num iterations */
      iters++;
      if (verbose_output() && (pe == 0))
        log_printf("iter# %d\n", iters);

      if (debug_output() && (pe == 0))
        log_printf("%d: before parallel comm. to collate clustsize\n", pe);

      /* Do parallel communication to collate cluster sizes */
      MPI_Allreduce(&clustsize[1], &otherclustsize[1], K, MPI_INT, MPI_SUM,
                    MPI_COMM_WORLD);
      copy_ivec(otherclustsize, clustsize, K);

/*
      DA_barrier();
      if (pe != 0) 
      {
        cc = DA_send_msg(&clustsize[1],K,0,pe,DA_INT);

        if (debug_output() && (pe == 0))
          log_printf("%d: after sending local clustsize\n",pe);

        cc = DA_broadcast_msg(&clustsize[1],K,DA_ALL_PES,0,DA_INT);

        if (debug_output() && (pe == 0))
          log_printf("%d: after receiving global clustsize\n",pe);
      }
      else 
      {
        for(p=1;p<numPE;p++)
        {
          cc = DA_recv_msg(&otherclustsize[1],K,p,p,DA_INT);

          if (debug_output() && (pe == 0))
            log_printf("%d: after receiving local clustsize\n",pe);

          for(k=1;k<=K;k++)
            clustsize[k] += otherclustsize[k];
        }
        cc = DA_broadcast_msg(&clustsize[1],K,DA_ALL_PES,0,DA_INT);

        if (debug_output() && (pe == 0))
          log_printf("%d: after sending global clustsize\n",pe);
      }    
*/

      if (debug_output() && (pe == 0))
        log_printf("%d: after parallel comm. to collate clustsize\n",pe);

      /* check for eliminated clusters */
      for (k=1; k<=K; k++)
        if (clustsize[k] == 0)
          empty_clusters++;

      /* decide what to do if a cluster was eliminated, based on flag */
      if (empty_clusters > 0)
      {
        if (empty_cluster_ok == UT_FALSE)
        {
          /* set some values to be returned */
          *num_empty_clusters = empty_clusters;
          *numiters = iters;

          /* free temporary storage */
          NR_free_ivector(otherclustsize,1,K);
          NR_free_dmatrix(othermeans,1,K,1,numcols);

          return (UT_ERROR);
        }
        else
        {
          /* K -= empty_clusters; */ /* caller must also do this */
          /* also get rid of the corresponding means in the means matrix */
	  /*
          log_printf("This option does not work yet.\n");
	  */

          /* set some values to be returned */
          *num_empty_clusters = empty_clusters;
          *numiters = iters;

          /* free temporary storage */
          NR_free_ivector(otherclustsize,1,K);
          NR_free_dmatrix(othermeans,1,K,1,numcols);

          return (UT_ERROR);  /* for now... when this works, just continue
                                 with smaller K */
        }
      }

      if (debug_output() && (pe == 0))
        log_printf("%d: after checking for empty clusters in k_means_pp\n",pe);

      /* normalize the local means by the global cluster sizes */
      for (k=1; k<=K; k++)
        for (j=1; j<=numcols; j++) {
          means[k][j] /= (double) clustsize[k];
	  /*
	  printf("%d: means[%d][%d] = %f\n",pe,k,j,means[k][j]);
	  fflush(stdout);
	  */
	}

      if (debug_output() && (pe == 0))
        log_printf("%d: after normalizing by clustsizes in k_means_pp\n",pe);

      if (debug_output() && (pe == 0))
        log_printf("%d: before doing parallel comm. to collate means\n",pe);

      /* Do parallel communication to collate means */
      MPI_Allreduce(&means[1][1], &othermeans[1][1], K * numcols, MPI_DOUBLE,
                    MPI_SUM, MPI_COMM_WORLD);
      copy_dmat(othermeans, means, K, numcols);

      /* Do parallel communication to calculate global stability */
      MPI_Allreduce(&stable, &otherstable, 1, MPI_INT, MPI_LAND, 
                    MPI_COMM_WORLD);
      stable = otherstable;

/*
      DA_barrier();
      if (pe != 0)
      {
        cc = DA_send_msg(&means[1][1],numcols*K,0,pe,DA_DOUBLE);
        cc = DA_broadcast_msg(&means[1][1],numcols*K,DA_ALL_PES,0,DA_DOUBLE);
        cc = DA_send_msg(&stable,1,0,pe,DA_INT);
        cc = DA_broadcast_msg(&stable,1,DA_ALL_PES,0,DA_INT);
      }
      else
      {
        for(p=1;p<numPE;p++)
        {
          cc = DA_recv_msg(&othermeans[1][1],numcols*K,p,p,DA_DOUBLE);
  
          for(k=1;k<=K;k++)
            for(j=1;j<=numcols;j++)
              means[k][j] += othermeans[k][j];
        }

        cc = DA_broadcast_msg(&means[1][1],numcols*K,DA_ALL_PES,0,DA_DOUBLE);

        for (p=1;p<numPE;p++)
        {
          cc = DA_recv_msg(&otherstable,1,p,p,DA_INT);

          if ((stable != UT_TRUE) || (otherstable != UT_TRUE))
            stable = UT_FALSE;
        }

        cc = DA_broadcast_msg(&stable,1,DA_ALL_PES,0,DA_INT);
      }
*/

      if (debug_output() && (pe == 0))
        log_printf("%d: after doing parallel comm. to collate means\n",pe);

      /* make the new means into the oldmeans for next iteration */
      tempmeans = oldmeans; oldmeans = means; means = tempmeans;


      if (debug_output() && (pe == 0))
        print_dmatrix(ut_log_fp, K, numcols, means);
    }

    if (debug_output() && (pe == 0))
      log_printf("%d: after convergence loop in k_means_pp()\n",pe);

    if (debug_output() && (pe == 0))
      log_printf("%d: empty_clusters = %d in k_means_pp()\n",pe,empty_clusters);

    /* set some values to be returned */
    *num_empty_clusters = empty_clusters;
    *numiters = iters;

    if (verbose_output() && (pe == 0))
      log_printf("%d: num_empty_clusters = %d in k_means_pp()\n",pe,
                 *num_empty_clusters);

    /* free temporary storage */
    NR_free_ivector(otherclustsize,1,K);
    NR_free_dmatrix(othermeans,1,K,1,numcols);

    return (UT_OK);
}


/*******************************************************************************
MIXTURE_LIKELIHOOD_PP
Parallel version of mixture_likelihood().
Given mixture model parameters, a dataset, and allocated space for storing
all the intermediate probabilities, compute the log-likelihood of the model 
given the data.
 
When the function is finished, each processor has the global log-likelihood 
value.
K is the number of components in the mixture.
means is the  matrix of mean vectors, where each row is a mean.
covars is the array of covariance matrices.
weights is the vector of class weights.
data is the dataset.
probs is the matrix to contain all intermediate probabilities.
prob_data is the vector to contain the posterior probability of each datum
given the model.
sum_probs is the vector to contain the posterior probability of each class
given the data.
numrows is the number of data.
numcols is the number of attributes.
rows_or_cols is the string specifying whether data are stored as rows or as
columns.
min_diag is a small number for perturbing ill-conditioned covariance matrices.
pe is the current processing element's id.
numPE is the total number of processing elements.
AG
*******************************************************************************/
double mixture_likelihood_pp(int K, double **means, double ***covars, 
                           double *weights, double **data, double **probs, 
                           double *prob_data, double *sum_probs, int numrows, 
                           int numcols, char *rows_or_cols, double min_diag,
                           int pe, int numPE)

{
  int      i,k,p,cc;
  double    log_likelihood, other_log_likelihood;
  double   *mean_k, **covar_k, *probs_k;
  double   *other_sum_probs;

  double  a,b,c,d,e,f;

  /* make temporary storage related to message passing */
  other_sum_probs = NR_dvector(1,K);

  /* initialize */
  set_dvec(prob_data, numrows, 0.0);
  set_dvec(sum_probs, K, 0.0);
  log_likelihood = 0.0;

  for (k=1; k<=K; k++)
  {
    /* get the parameters corresponding to this class; also get the place
       where the probabilities will be stored */
    mean_k  = means[k];
    covar_k = covars[k];
    probs_k = probs[k];
    
    /* note that the following 3 operations all cycle over the data; they
       should be consolidated for efficiency */

    /* A. first compute the LIKELIHOOD of the data given the parameters for
       this class; store it in the array probs[k] */
    vector_prob_gauss(data, mean_k, covar_k, numcols, numrows, probs_k,
                      min_diag, rows_or_cols);
    
    if (debug_output() && (pe == 0))
    {
      log_printf("First three density values from vector_gauss_eval\n");
      print_drow(ut_log_fp, 3, probs_k);
      log_printf("%f %f %f\n", probs_k[1],probs_k[2],probs_k[3]);
      log_printf("%g %g %g\n", probs_k[1],probs_k[2],probs_k[3]);
    }

    /* B. multiply by the class prior, or weight, to get the JOINT
       probability of the data and the parameters; store it in probs[k] */
    scalar_mult_dvec(probs_k, numrows, weights[k]);

    /* contribute to the probability of each datum across all models */
    for (i=1; i<=numrows; i++)
      prob_data[i] += probs_k[i];
    /* Faster than add_dvec(numrows, probs_k, prob_data, prob_data); */
  }

  if (debug_output() && (pe == 0)) 
  {
    log_printf("First three density values from prob_data\n");
    print_drow(ut_log_fp, 3, prob_data);
  }

  /* C. normalize by the probability of the data across all models to get 
     the POSTERIOR probability of each datum; store it in probs[k] */
  for (k=1; k<=K; k++)
  {
    probs_k = probs[k];
    for (i=1; i<=numrows; i++)
      if (prob_data[i] != 0)
        probs_k[i] /= prob_data[i];
  }
  /* Faster than div_dvec_elt(numrows, probs_k, prob_data, probs_k); */

  if (debug_output() && (pe == 0))
  {
    for (k=1; k<=K; k++)
    {
      log_printf("first three posterior probabilities for class %d\n", k);
      log_printf("    in true class A\n");
      print_drow(ut_log_fp, 3, probs[k]);
      log_printf("first three posterior probabilities for class %d\n", k);
      log_printf("    in true class B\n");
      for (i=251; i<=253; i++)
        log_printf("%g ",probs[k][i]);
      log_printf("\n");
    }
  }

  /* compute the SUM OF THE POSTERIOR probabilities for each class, which
     is used in computing the parameters later */
  for (k=1; k<=K; k++)
    sum_probs[k] = sum_dvec(probs[k], numrows);

  if (debug_output() && (pe == 0)) 
  {
    log_printf("sum of posteriors for all classes\n");
    print_drow(ut_log_fp, K, sum_probs);
    log_printf("first 10 cols of probs\n");
    print_dmatrix(ut_log_fp,K,10,probs);
    a=0; b=0;
    for (i=1;i<=numrows;i++)
      a += probs[1][i];
    for (i=1;i<=numrows;i++)
      b += probs[2][i];
    log_printf("sum of 1st row = %g, sum of 2nd row = %g\n",a,b);
  }

  /* compute the log-likelihood of all the data given the mixture model */
  for (i=1; i<=numrows; i++)
    log_likelihood += (double) log((double) (prob_data[i] + DBL_MIN));

  if (debug_output() && (pe == 0))
    log_printf("log-likelihood = %g\n", log_likelihood);

  /* Do parallel communication to collate sums of posteriors and the global
     log-likelihood */

  MPI_Allreduce(&sum_probs[1], &other_sum_probs[1], K, MPI_DOUBLE, MPI_SUM,
                MPI_COMM_WORLD);
  copy_dvec(other_sum_probs, sum_probs, K);

  MPI_Allreduce(&log_likelihood, &other_log_likelihood, 1, MPI_DOUBLE, MPI_SUM,
                MPI_COMM_WORLD);
  log_likelihood = other_log_likelihood;

/*
  DA_barrier();
  if (pe != 0)
  {
    cc = DA_send_msg(&sum_probs[1],K,0,pe,DA_DOUBLE);
    cc = DA_broadcast_msg(&sum_probs[1],K,DA_ALL_PES,0,DA_DOUBLE);
    
    cc = DA_send_msg(&log_likelihood,1,0,pe,DA_DOUBLE);
    cc = DA_broadcast_msg(&log_likelihood,1,DA_ALL_PES,0,DA_DOUBLE);
  }
  else 
  {
    for(p=1;p<numPE;p++)
    {
      cc = DA_recv_msg(&other_sum_probs[1],K,p,p,DA_DOUBLE);
      add_dvec(sum_probs, other_sum_probs, sum_probs, K);
    }
    cc = DA_broadcast_msg(&sum_probs[1],K,DA_ALL_PES,0,DA_DOUBLE);
    
    for(p=1;p<numPE;p++)
    {
      cc = DA_recv_msg(&other_log_likelihood,1,p,p,DA_DOUBLE);
      log_likelihood += other_log_likelihood;
    }
    cc = DA_broadcast_msg(&log_likelihood,1,DA_ALL_PES,0,DA_DOUBLE);
  }     
*/

  if (debug_output() && (pe == 0))
  {
    log_printf("sum_probs in mixture_likelihood:\n");
    print_drow(ut_log_fp, K, sum_probs);
  }
  
  /* free temporary storage */
  NR_free_dvector(other_sum_probs,1,K);

  return(log_likelihood);
}


/*******************************************************************************
ESTIMATE_MIXTURE_PARAMS_PP
Parallel version of estimate_mixture_params().
Given the matrix containing the posterior probability of each datum belonging
to each class in the mixture model, a dataset, and allocated space for storing
all the parameters to be computed, estimate the mixture model parameters from
the class probabilities.
Assumes the data are stored in columns, not rows.
When the function is finished, each processor has the global values for the
parameters.
K is the number of components in the mixture.
means is the  matrix of mean vectors, where each row is a mean.
covars is the array of covariance matrices.
weights is the vector of class weights.
data is the dataset.
probs is the matrix to contain all intermediate probabilities.
sum_probs is the vector to contain the posterior probability of each class
given the data.
numrows is the number of data.
numcols is the number of attributes.
min_weight is the minimum class weight; it determines when a class is consi-
dered to have collapsed.
num_empty_clusters is the number of classes that have collapsed.
empty_cluster_ok is the flag indicating whether to continue in the empty
cluster case.
pe is the current processing element's id.
numPE is the total number of processing elements.
AG
*******************************************************************************/
int estimate_mixture_params_pp(int K, double **means, double ***covars, 
                              double *weights, double **data, double **probs, 
                              double *sum_probs, int total_numrows, int numrows, 
                              int numcols, double min_weight, 
                              int *num_empty_clusters, boolean empty_cluster_ok,
                              int pe, int numPE)

{
  int      i,j,j2,k,p,cc;
  double    log_likelihood;
  double    *mean_k, **covar_k, *probs_k, *data_j, *data_j2;
  double    **othermeans, **other_covar_k;
  int      empty_clusters;

  /* make storage related to message passing */
  othermeans = NR_dmatrix(1,K,1,numcols);
  other_covar_k = NR_dmatrix(1,numcols,1,numcols);

  /* A. compute the class WEIGHTS, or priors */
  for (k=1; k<=K; k++)
    weights[k] = sum_probs[k] / total_numrows;
  
  if (debug_output() && (pe == 0))
  {
    log_printf("weights for all classes\n");
    print_drow(ut_log_fp, K, weights);
  }

  if (debug_output() && (pe == 0))
  {
    log_printf("%d: after computing weights\n",pe);
    log_printf("weights for all classes\n");
    print_drow(ut_log_fp, K, weights);
  }

  /* check for eliminated clusters */
  empty_clusters = 0;
  for (k=1; k<=K; k++)
    if (weights[k] <= min_weight)
      empty_clusters++;

  /* decide what to do if a cluster was eliminated, based on flag */
  if (empty_clusters > 0)
  {
    if (empty_cluster_ok == UT_FALSE)
    {
      /* set some values to be returned */
      *num_empty_clusters = empty_clusters;

      /* clean up allocated memory before exiting */
      NR_free_dmatrix(othermeans,1,K,1,numcols);
      NR_free_dmatrix(other_covar_k,1,numcols,1,numcols);

      return (UT_ERROR);
    }
    else
    {
      /* K -= empty_clusters; */ /* caller must also do this */
      /* also get rid of the corresponding means in the means matrix 
         and covar. matrices and weights */
      /*
      log_printf("This option does not work yet.\n");
      */

      /* set some values to be returned */
      *num_empty_clusters = empty_clusters;

      /* clean up allocated memory before exiting */
      NR_free_dmatrix(othermeans,1,K,1,numcols);
      NR_free_dmatrix(other_covar_k,1,numcols,1,numcols);

      return (UT_ERROR);  /* for now... when this works, just continue
                             with smaller K */
    }
  }
      
  if (debug_output() && (pe == 0))
    log_printf("%d: after checking for eliminated clusters\n",pe);

  /* B. compute the class MEAN vectors */
  /* can i just use mat_mult() here ???  I think so. */
  for (k=1; k<=K; k++)
  {
    mean_k  = means[k];
    probs_k = probs[k];
      
    /* first multiply the posteriors matrix by the data matrix */
    for (j=1; j<=numcols; j++)
    {
      data_j = data[j];

      mean_k[j] = 0.0;
      for (i=1; i<=numrows; i++)
        mean_k[j] += probs_k[i] * data_j[i];
    }

    /* then normalize the means by the sum of the posteriors */
    scalar_div_dvec(mean_k, numcols, sum_probs[k]);
  }

  if (debug_output() && (pe == 0))
    log_printf("%d: after computing means\n",pe);

  if (debug_output() && (pe == 0))
    log_printf("%d: before parallel comm. to collate means\n",pe);

  /* Do parallel communication to collate means */
  MPI_Allreduce(&means[1][1], &othermeans[1][1], K * numcols, MPI_DOUBLE,
                MPI_SUM, MPI_COMM_WORLD);
  copy_dmat(othermeans, means, K, numcols);

/*
  DA_barrier();
  if (pe != 0)
  {
    cc = DA_send_msg(&means[1][1],numcols*K,0,pe,DA_DOUBLE);
    
    if (debug_output() && (pe == 0))
      log_printf("%d: after sending local means\n",pe);

    cc = DA_broadcast_msg(&means[1][1],numcols*K,DA_ALL_PES,0,DA_DOUBLE);

    if (debug_output() && (pe == 0))
      log_printf("%d: after receiving global means\n",pe);

  }
  else 
  {
    for(p=1;p<numPE;p++)
    {
      cc = DA_recv_msg(&othermeans[1][1],numcols*K,p,p,DA_DOUBLE);
      add_dmat(means, othermeans, means, K, numcols);
    }

    if (debug_output() && (pe == 0))
      log_printf("%d: after receiving local means\n",pe);

    cc = DA_broadcast_msg(&means[1][1],numcols*K,DA_ALL_PES,0,DA_DOUBLE);

    if (debug_output() && (pe == 0))
      log_printf("%d: after broadcasting global means\n",pe);
  }
*/

  if (debug_output() && (pe == 0))
    log_printf("%d: after parallel comm. to collate means\n",pe);

  /* C. compute the class COVARIANCE matrices */
  for (k=1; k<=K; k++)
  {
    covar_k = covars[k];
    mean_k  = means[k];
    probs_k = probs[k];

    /* multiply the deviation vectors by the posteriors;
       also normalize by the sum of the posteriors */
    for (j=1; j<=numcols; j++)
    {
      data_j = data[j];
      for (j2=j; j2<=numcols; j2++)
      {
        data_j2 = data[j2];
        
        covar_k[j][j2] = 0.0;
        for (i=1; i<=numrows; i++)
          covar_k[j][j2] += (data_j[i] - mean_k[j]) * (data_j2[i] - mean_k[j2])
                            * probs_k[i];

        covar_k[j][j2] /= sum_probs[k];
      }
    }

    /* fill in the lower triangle of the matrix based on upper triangle */
    for (j=1; j<=numcols; j++)
      for (j2=1; j2<j; j2++)
        covar_k[j][j2] = covar_k[j2][j];

  }

  if (debug_output() && (pe == 0))
    log_printf("%d: after computing cov. matrices\n",pe);

  if (debug_output() && (pe == 0))
    log_printf("%d: before parallel comm. to collate cov. matrices\n",pe);

  /* Do parallel communication to collate covariance matrices */
  for (k = 1; k <= K; k++) {
    covar_k = covars[k];
    MPI_Allreduce(&covar_k[1][1], &other_covar_k[1][1], numcols * numcols,
                  MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
    copy_dmat(other_covar_k, covar_k, numcols, numcols);
  }

/*
  DA_barrier();
  if (pe != 0)
  {
    for (k=1; k<=K; k++)
    {
      covar_k = covars[k];
      cc = DA_send_msg(&covar_k[1][1],numcols*numcols,0,k,DA_DOUBLE);
    }

    for (k=1; k<=K; k++)
    {
      covar_k = covars[k];
      cc = DA_broadcast_msg(&covar_k[1][1],numcols*numcols,DA_ALL_PES,0,
		            DA_DOUBLE);
      DA_barrier();
    }
  }
  else 
  {
    for(p=1;p<numPE;p++)
    {
      for (k=1; k<=K; k++)
      {
        covar_k = covars[k];
        cc = DA_recv_msg(&other_covar_k[1][1],numcols*numcols,p,k,DA_DOUBLE);
        add_dmat(covar_k, other_covar_k, covar_k, numcols, numcols);
      }
    }

    if (debug_output() && (pe == 0))
      log_printf("%d: after receiving local cov. matrices\n",pe);

    for (k=1; k<=K; k++)
    {
      covar_k = covars[k];
      cc = DA_broadcast_msg(&covar_k[1][1],numcols*numcols,DA_ALL_PES,0,
                         DA_DOUBLE);
      DA_barrier();
    }

    if (debug_output() && (pe == 0))
      log_printf("%d: after broadcasting global cov. matrices\n",pe);

  }     
*/

  if (debug_output() && (pe == 0))
    log_printf("%d: after parallel comm. to collate cov. matrices\n",pe);

  /* free temporary storage space */
  NR_free_dmatrix(othermeans,1,K,1,numcols);
  NR_free_dmatrix(other_covar_k,1,numcols,1,numcols);

  /* set some values to be returned */
  *num_empty_clusters = empty_clusters;

  return (UT_OK);
}


/*******************************************************************************
RANDOM_MEANS_PP
Parallel version of random_means().
AG
*******************************************************************************/
int random_means_pp(means, num_means, num_cols, pe)

    double **means;
    int   num_means, num_cols;
    int   pe;
{
    int cc;

    if (debug_output() && (pe == 0))
      log_printf("%d: before message passing for choosing random means\n",pe);

    DA_barrier();
    if (pe != 0) /* This is not the controlling process */
    {	
      cc = DA_broadcast_msg(&means[1][1],num_means*num_cols,DA_ALL_PES,0,DA_DOUBLE);
    }
    else
    {
      /* choose initial values for means arbitrarily */
      random_means(means, num_means, num_cols);
      
      cc = DA_broadcast_msg(&means[1][1],num_means*num_cols,DA_ALL_PES,0,DA_DOUBLE);
    }

    if (debug_output() && (pe == 0))
    {
      log_printf("%d: means: \n",pe);
      print_dmatrix(ut_log_fp, num_means, num_cols, means);
    }

    return (UT_OK);
}


/*******************************************************************************
RANDOM_MEANS_FROM_DATA_PP
Parallel version of random_means_from_data().
AG
*******************************************************************************/
int random_means_from_data_pp(double **means, int num_means, int num_cols, 
		              double **data, int num_data, int pe, int numPE)
{
  int cc;
  int i, p;
  int num_local_means;
  int total_num_data;
  int startrow;
  int startmean;
  boolean *selected;

  selected = (boolean *) NR_cvector(1, num_data);

  split_data_pp(&startmean, &num_local_means, num_means, pe, numPE);
  
  if (num_local_means > 0)
    random_select_drows(data, num_data, num_cols, means, num_local_means, 
  		        selected);
  
  if (pe != 0) {
    if (num_local_means > 0)
      cc = DA_send_msg(&means[1][1], num_local_means * num_cols, 0, pe, 
  		       DA_DOUBLE);
    
    cc = DA_broadcast_msg(&means[1][1], num_means * num_cols, DA_ALL_PES, 0,
		          DA_DOUBLE);
  }
  else {
    for (p = 1; p < numPE; p++) {
      split_data_pp(&startmean, &num_local_means, num_means, p, numPE);
      
      if (num_local_means > 0)
        cc = DA_recv_msg(&means[startmean][1], num_local_means * num_cols, p, p,
  		         DA_DOUBLE);
    }
    
    cc = DA_broadcast_msg(&means[1][1], num_means * num_cols, DA_ALL_PES, 0,
		          DA_DOUBLE);
  }
  
  return(UT_OK);
}
@


1.14
log
@changed includes.
@
text
@d25 1
a25 1
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.13 1998/04/21 17:07:35 roden Exp agray $";
d31 3
d81 1
d83 3
d100 2
d123 2
a124 2
int k_means_pp(float **data, int numrows, int numcols, float **means, 
               float **initmeans, int *clustsize, int *class, int K,
d129 1
a129 1
    float          **oldmeans, **tempmeans, **othermeans;
d133 2
a134 2
    float          dist, mindist;
    boolean        stable, otherstable;
d137 1
a137 1
	if (verbose_output() && (pe == 0))
d142 1
a142 1
    othermeans = NR_matrix(1,K,1,numcols);
d148 1
d153 1
a153 1
	if (debug_output() && (pe == 0))
d159 3
d176 1
a176 1
        mindist = euclid_dist(data[i],oldmeans[1],numcols);
d189 1
a189 1
          if ((dist = euclid_dist(data[i],oldmeans[k],numcols)) < mindist)
d203 1
d220 7
a226 2
      barrier();
      if (pe != 0) /* This is not the controlling process */
d228 1
a228 4
        /* log_printf("%d: This is not the controlling process.\n",pe); */
     
        /* Send out the locally computed cluster sizes */
        cc = send_msg(&clustsize[1],K,0,pe,DA_INT);
d233 1
a233 3
        /* Now block until receive broadcast message from the controlling
           process giving the global cluster sizes */
        cc = recv_msg(&clustsize[1],K,0,0,DA_INT);
d238 1
a238 1
      else /* This is the controlling process */
a239 3
        /* log_printf("%d: This is the controlling process.\n",pe); */

        /* Receive the other cluster sizes */
d242 1
a242 1
          cc = recv_msg(&otherclustsize[1],K,p,p,DA_INT);
d250 1
a250 2
        /* Now broadcast the global cluster sizes */
        cc = broadcast_msg(&clustsize[1],K,DA_ALL_PES,0,DA_INT);
d255 1
d274 4
d282 1
a282 1
          K -= empty_clusters;  /* caller must also do this */
d284 1
d286 1
d292 4
d306 7
a312 2
        for (j=1; j<=numcols; j++)
          means[k][j] /= (float) clustsize[k];
d321 3
a323 4
      barrier();
      if (pe != 0) /* This is not the controlling process */
      {
        /* log_printf("%d: This is not the controlling process.\n",pe); */
d325 4
a328 2
        /* Send out the locally computed means */
        cc = send_msg(&means[1][1],numcols*K,0,pe,DA_FLOAT);
d330 8
a337 11
        /* Now block until receive broadcast message from the controlling 
           process giving the global means */
        cc = recv_msg(&means[1][1],numcols*K,0,0,DA_FLOAT);

        /* Send the stability measurement to the controlling processor, */
        /* using own PE number as the message tag */
        cc = send_msg(&stable,1,0,pe,DA_INT);
 
        /* Now block until receive broadcast message from controlling process
           giving the stability */
        cc = recv_msg(&stable,1,0,0,DA_INT);
d339 1
a339 1
      else /* This is the controlling process */
a340 5
        /* log_printf("%d: This is the controlling process.\n",pe); */

        /* Receive the other means */
        /* Note that all means have already been normalized by the global
           cluster sizes */
d343 1
a343 1
          cc = recv_msg(&othermeans[1][1],numcols*K,p,p,DA_FLOAT);
d350 2
a351 4
        /* Now broadcast the collated means */
        cc = broadcast_msg(&means[1][1],numcols*K,DA_ALL_PES,0,DA_FLOAT);
     
        /* Receive the other stability measurements */
d354 1
a354 1
          cc = recv_msg(&otherstable,1,p,p,DA_INT);
d360 1
a360 2
        /* Now broadcast the overall stability */
        cc = broadcast_msg(&stable,1,DA_ALL_PES,0,DA_INT);
d362 1
d370 1
d372 1
a372 1
        print_matrix(ut_log_fp, K, numcols, means);
d375 1
a375 1
	if (debug_output() && (pe == 0))
d378 1
a378 1
	if (debug_output() && (pe == 0))
d385 1
a385 1
	if (verbose_output() && (pe == 0))
d389 4
d396 1
d425 4
a428 4
float mixture_likelihood_pp(int K, float **means, float ***covars, 
                           float *weights, float **data, float **probs, 
                           float *prob_data, float *sum_probs, int numrows, 
                           int numcols, char *rows_or_cols, float min_diag,
d433 3
a435 3
  float    log_likelihood, other_log_likelihood;
  float   *mean_k, **covar_k, *probs_k;
  float   *other_sum_probs;
d437 1
a437 1
  float  a,b,c,d,e,f;
d440 1
a440 1
  other_sum_probs = NR_vector(1,K);
d443 2
a444 2
  set_vec(prob_data, numrows, 0.0);
  set_vec(sum_probs, K, 0.0);
d460 1
a460 1
    vector_gauss_eval(data, mean_k, covar_k, numcols, numrows, probs_k,
d466 1
a466 1
      print_row(ut_log_fp, 3, probs_k);
d473 1
a473 1
    scalar_mult_vec(probs_k, numrows, weights[k]);
d478 1
a478 1
    /* Faster than add_vec(numrows, probs_k, prob_data, prob_data); */
d484 1
a484 1
    print_row(ut_log_fp, 3, prob_data);
d496 1
a496 1
  /* Faster than div_vec_elt(numrows, probs_k, prob_data, probs_k); */
d504 1
a504 1
      print_row(ut_log_fp, 3, probs[k]);
d516 1
a516 1
    sum_probs[k] = sum_vec(probs[k], numrows);
d521 1
a521 1
    print_row(ut_log_fp, K, sum_probs);
d523 1
a523 1
    print_matrix(ut_log_fp,K,10,probs);
d534 1
a534 1
    log_likelihood += (float) log((float) prob_data[i]);
d541 12
a552 2
  barrier();
  if (pe != 0) /* This is not the controlling process */
d554 2
a555 1
    /* log_printf("%d: This is not the controlling process.\n",pe); */
d557 2
a558 14
    /* Send out the locally computed weights */
    cc = send_msg(&sum_probs[1],K,0,pe,DA_FLOAT);
    
    /* Now block until receive broadcast message from the controlling
       process giving the global sums of posteriors */
    cc = recv_msg(&sum_probs[1],K,0,0,DA_FLOAT);
    
    /* Send out the locally computed log-likelihood */
    cc = send_msg(&log_likelihood,1,0,pe,DA_FLOAT);
    
    /* Now block until receive broadcast message from the controlling
       process giving the global log-likelihood */
    cc = recv_msg(&log_likelihood,1,0,0,DA_FLOAT);

d560 1
a560 1
  else /* This is the controlling process */
a561 3
    /* log_printf("%d: This is the controlling process.\n",pe); */

    /* Receive the other sums of posteriors, do collation */
d564 2
a565 2
      cc = recv_msg(&other_sum_probs[1],K,p,p,DA_FLOAT);
      add_vec(sum_probs, other_sum_probs, sum_probs, K);
d567 1
a567 3

    /* Now broadcast the global sums of posteriors */
    cc = broadcast_msg(&sum_probs[1],K,DA_ALL_PES,0,DA_FLOAT);
a568 1
    /* Receive the other log-likelihoods, do collation */
d571 1
a571 1
      cc = recv_msg(&other_log_likelihood,1,p,p,DA_FLOAT);
d574 1
a574 4

    /* Now broadcast the global log-likelihood */
    cc = broadcast_msg(&log_likelihood,1,DA_ALL_PES,0,DA_FLOAT);

d576 1
d581 1
a581 1
    print_row(ut_log_fp, K, sum_probs);
d583 3
d590 1
d620 4
a623 4
int estimate_mixture_params_pp(int K, float **means, float ***covars, 
                              float *weights, float **data, float **probs, 
                              float *sum_probs, int total_numrows, int numrows, 
                              int numcols, float min_weight, 
d629 3
a631 3
  float    log_likelihood;
  float    *mean_k, **covar_k, *probs_k, *data_j, *data_j2;
  float    **othermeans, **other_covar_k;
d635 2
a636 2
  othermeans = NR_matrix(1,K,1,numcols);
  other_covar_k = NR_matrix(1,numcols,1,numcols);
d645 1
a645 1
    print_row(ut_log_fp, K, weights);
d652 1
a652 1
    print_row(ut_log_fp, K, weights);
d669 4
d677 1
a677 1
      K -= empty_clusters;  /* caller must also do this */
d680 1
d682 1
d687 4
d696 2
a697 2
	if (debug_output() && (pe == 0))
      log_printf("%d: after checking for eliminated clusters\n",pe);
d717 1
a717 1
    scalar_div_vec(mean_k, numcols, sum_probs[k]);
a722 7
  /*
  if (debug_output() && (pe == 0)) {
    log_printf("means for all classes\n");
    print_unnorm_matrix(ut_log_fp, K, numcols, means, range, minval);
  }
  */

d727 7
a733 2
  barrier();
  if (pe != 0) /* This is not the controlling process */
d735 1
a735 1
    /* log_printf("%d: This is not the controlling process.\n",pe); */
a736 3
    /* Send out the locally computed means */
    cc = send_msg(&means[1][1],numcols*K,0,pe,DA_FLOAT);
    
d740 1
a740 3
    /* Now block until receive broadcast message from the controlling 
       process giving the global means */
    cc = recv_msg(&means[1][1],numcols*K,0,0,DA_FLOAT);
d746 1
a746 1
  else /* This is the controlling process */
a747 5
    /* log_printf("%d: This is the controlling process.\n",pe); */
    
    /* Receive the other means */
    /* Note that all means have already been normalized by being multiplied 
       by the posteriors matrix */
d750 2
a751 2
      cc = recv_msg(&othermeans[1][1],numcols*K,p,p,DA_FLOAT);
      add_mat(means, othermeans, means, K, numcols);
d757 1
a757 2
    /* Now broadcast the collated means */
    cc = broadcast_msg(&means[1][1],numcols*K,DA_ALL_PES,0,DA_FLOAT);
a760 1

d762 1
d806 10
a815 2
  barrier();
  if (pe != 0) /* This is not the controlling process */
d817 1
a817 4
    /* log_printf("%d: This is not the controlling process.\n",pe); */
      
    /* Send out the locally computed covariance matrices */
    for (k=1; k<K; k++)
d820 1
a820 2
      cc = send_msg(&covar_k[1][1],numcols*numcols,0,k,DA_FLOAT);
      /* Was: cc = send_msg(&covar_k[1][1],numcols*numcols,0,K*pe+k,DA_FLOAT);*/
d823 1
a823 6
    if (debug_output() && (pe == 0))
      log_printf("%d: after sending local cov. matrices\n",pe);

    /* Now block until receive broadcast message from the controlling
       process giving the global covariance matrices */
    for (k=1; k<K; k++)
d826 3
a828 1
      cc = recv_msg(&covar_k[1][1],numcols*numcols,0,k,DA_FLOAT);
a829 4

    if (debug_output() && (pe == 0))
      log_printf("%d: after receiving global cov. matrices\n",pe);

d831 1
a831 1
  else /* This is the controlling process */
a832 3
    /* log_printf("%d: This is the controlling process.\n",pe); */
      
    /* Receive the other sets of covariance matrices, do collation */
d835 1
a835 1
      for (k=1; k<K; k++)
d838 2
a839 5
        cc = recv_msg(&other_covar_k[1][1],numcols*numcols,p,k,DA_FLOAT);
        /*
          cc = recv_msg(&other_covar_k[1][1],numcols*numcols,p,K*p+k,DA_FLOAT);
          */
        add_mat(covar_k, other_covar_k, covar_k, numcols, numcols);
d846 1
a846 2
    /* Now broadcast the global covariance matrices */
    for (k=1; k<K; k++)
d849 3
a851 2
      cc = broadcast_msg(&covar_k[1][1],numcols*numcols,DA_ALL_PES,k,
                         DA_FLOAT);
d858 1
d863 3
a865 6
  /*
  if (debug_output() && (pe == 0)) {
  log_printf("cov. matrix for class %d\n", k);
  print_unnorm_cov_matrix(ut_log_fp, numcols, numcols, covar_k, range);
  }
  */
d881 1
a881 1
    float **means;
d890 1
a890 1
    barrier();
d893 1
a893 1
      cc = recv_msg(&means[1][1],num_means*num_cols,0,0,DA_FLOAT);
d900 1
a900 1
      cc = broadcast_msg(&means[1][1],num_means*num_cols,DA_ALL_PES,0,DA_FLOAT);
d906 1
a906 1
      print_matrix(ut_log_fp, num_means, num_cols, means);
d918 42
a959 4
/*
int random_means_from_data_pp(means, num_means, num_cols, data, num_data, 
                              rows_or_cols, pe);
                              */
@


1.13
log
@Took out #include <pvm3.h> because I think it's not needed here, and it
doesn't conform to da_platform.h control over presence/absence of pvm.
@
text
@a18 4
PROGRAM EXAMPLE(S)
1. /proj/cooltools/sc_pp, AG.
2. /proj/cooltools/kmeans_pp, RG.

d25 1
a25 1
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.12 1998/03/09 00:38:36 granat Exp roden $";
d31 4
d92 1
a92 1
#include "da_signal.h"
@


1.12
log
@fixed boolean bug
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.11 1997/06/05 18:55:46 granat Exp granat $";
d35 3
a77 3

/* PVM library */
#include <pvm3.h>
@


1.11
log
@edited to conform to changes in da_linalg
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.10 1997/06/02 15:35:17 granat Exp granat $";
d35 3
d116 1
a116 1
               int *numiters, int *num_empty_clusters, bool empty_cluster_ok,
d125 1
a125 1
    bool           stable, otherstable;
d606 1
a606 1
                              int *num_empty_clusters, bool empty_cluster_ok, 
@


1.10
log
@changed to use new NR naming convention
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.9 1997/04/05 19:16:58 granat Exp granat $";
d35 3
d417 2
a418 2
  set_vec(numrows, prob_data, 0.0);
  set_vec(K, sum_probs, 0.0);
d447 1
a447 1
    scalar_mult_vec(numrows, probs_k, weights[k]);
d543 1
a543 1
      add_vec(K, sum_probs, other_sum_probs, sum_probs);
d686 1
a686 1
    scalar_div_vec(numcols, mean_k, sum_probs[k]);
d732 1
a732 1
      add_mat(K, numcols, means, othermeans, means);
d830 1
a830 1
        add_mat(numcols, numcols, covar_k, other_covar_k, covar_k);
@


1.9
log
@made adjustments to account for changes in da_linalg
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.8 1997/01/29 21:44:05 agray Exp granat $";
d35 3
a78 1
#include "nrutil.h"
d126 2
a127 2
    otherclustsize = ivector(1,K);
    othermeans = matrix(1,K,1,numcols);
d411 1
a411 1
  other_sum_probs = vector(1,K);
d611 2
a612 2
  othermeans = matrix(1,K,1,numcols);
  other_covar_k = matrix(1,numcols,1,numcols);
@


1.8
log
@new formatting, cleaning up debugging output using ut_output.
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.7 1996/10/31 02:06:43 agray Exp agray $";
d35 3
d485 1
a485 1
    sum_probs[k] = sum_vec(numrows, probs[k]);
@


1.7
log
@renamed from "da_clust_pp" to "da_cluster_pp";
changed .h and .c formats throughout library;
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_cluster_pp.c,v 1.6 1996/10/31 00:23:18 agray Exp agray $";
d31 2
d34 23
a56 19
 *$Log: da_cluster_pp.c,v $
 *Revision 1.6  1996/10/31 00:23:18  agray
 *changed naming from "mixmod" to "mixture"; other naming changes.
 *
 *Revision 1.5  1996/09/23 23:57:23  agray
 *updated utRandomPick() to use char's for selected array.
 *
 *Revision 1.4  1996/09/23 22:40:46  agray
 *minor changes.
 *
 *Revision 1.3  1996/07/19 17:53:03  agray
 *added random_means_pp(), started random_means_from_data_pp()
 *
 *Revision 1.2  1996/07/17 20:42:25  agray
 *cosmetic.
 *
 *Revision 1.1  1996/07/11 16:42:35  agray
 *Initial revision
 *
d69 2
a70 3
#include "ut_rand.h"
#include "ut_string.h"
#include "ut_debug.h"
d78 3
a81 2
#include "da_linalg.h"
#include "da_comm_pp.h"
d117 2
a118 2
if (pe==0)
printf("%d: beginning k_means_pp\n",pe);
d132 2
a133 2
if (pe==0)
printf("%d: beginning k_means_pp iterations\n",pe);
d155 1
a155 1
            printf("item %d to mean %d: %f\n",
d161 1
a161 1
            printf("item %d to mean %d: %f\n",
d188 2
a189 3
      if ((utDebugLevel > 0) && (pe == 0)) {
        printf("iter# %d\n", iters);
      }
d191 2
a192 2
if (pe==0)
printf("%d: before k_means_pp parallel comm. to collate clustsize\n",pe);
d198 1
a198 1
        /* printf("%d: This is not the controlling process.\n",pe); */
d202 3
a204 1
printf("%d: after sending local clustsize\n",pe);
d209 3
a211 1
printf("%d: after receiving global clustsize\n",pe);
d215 1
a215 1
        /* printf("%d: This is the controlling process.\n",pe); */
d221 3
a223 1
printf("%d: after receiving local clustsize\n",pe);
d230 3
a232 1
printf("%d: after sending global clustsize\n",pe);
d235 2
a236 2
if (pe==0)
printf("%d: after k_means_pp parallel comm. to collate clustsize\n",pe);
d258 1
a258 1
          printf("This option does not work yet.\n");
d269 2
a270 2
if (pe==0)
printf("%d: after checking for empty clusters in k_means_pp\n",pe);
d277 5
a281 4
if (pe==0)
printf("%d: after normalizing by clustsizes in k_means_pp\n",pe);
if (pe==0)
printf("%d: before doing parallel comm. to collate means in k_means_pp\n",pe);
d287 1
a287 1
        /* printf("%d: This is not the controlling process.\n",pe); */
d306 1
a306 1
        /* printf("%d: This is the controlling process.\n",pe); */
d336 2
a337 2
if (pe==0)
printf("%d: after doing parallel comm. to collate means in k_means_pp\n",pe);
d342 3
a344 3
      /* 
      print_matrix(stdout, K, numcols, means);
       */
d346 2
a347 1
    }
d349 2
a350 4
if (pe==0)
printf("%d: after convergence loop in k_means_pp()\n",pe);
if (pe==0)
printf("%d: empty_clusters = %d in k_means_pp()\n",pe,empty_clusters);
d356 3
a358 2
if (pe==0)
printf("%d: num_empty_clusters = %d in k_means_pp()\n",pe,*num_empty_clusters);
d429 6
a434 5
    if ((utDebugLevel > 0) && (pe == 0)) {
      printf("first three density values from vector_gauss_eval\n");
      print_row(stdout, 3, probs_k);
      printf("%f %f %f\n", probs_k[1],probs_k[2],probs_k[3]);
      printf("%g %g %g\n", probs_k[1],probs_k[2],probs_k[3]);
d444 1
a444 3
    /*
    add_vec(numrows, probs_k, prob_data, prob_data);
    */
d447 4
a450 3
  if ((utDebugLevel > 0) && (pe == 0)) {
    printf("first three density values from prob_data\n");
    print_row(stdout, 3, prob_data);
d462 1
a462 3
  /*
    div_vec_elt(numrows, probs_k, prob_data, probs_k);
    */
d464 9
a472 7
  if ((utDebugLevel > 0) && (pe == 0)) {
    for (k=1; k<=K; k++) {
      printf("first three posterior probabilities for class %d\n", k);
      printf("    in true class A\n");
      print_row(stdout, 3, probs[k]);
      printf("first three posterior probabilities for class %d\n", k);
      printf("    in true class B\n");
d474 2
a475 2
        printf("%g ",probs[k][i]);
      printf("\n");
d484 14
a497 12
  if ((utDebugLevel > 0) && (pe == 0)) {
    printf("sum of posteriors for all classes\n");
    print_row(stdout, K, sum_probs);
printf("first 10 cols of probs\n");
print_matrix(stdout,K,10,probs);
a=0; b=0;
for (i=1;i<=numrows;i++)
  a += probs[1][i];
for (i=1;i<=numrows;i++)
  b += probs[2][i];
printf("sum of 1st row = %g, sum of 2nd row = %g\n",a,b);
}
d502 2
a503 3
  if ((utDebugLevel > 0) && (pe == 0)) {
    printf("log-likelihood = %g\n", log_likelihood);
  }
d510 1
a510 1
    /* printf("%d: This is not the controlling process.\n",pe); */
d529 1
a529 1
    /* printf("%d: This is the controlling process.\n",pe); */
d553 5
a557 5
if (pe==0)
{
printf("sum_probs in mixture_likelihood:\n");
print_row(stdout, K, sum_probs);
}
d613 4
a616 3
  if ((utDebugLevel > 0) && (pe == 0)) {
    printf("weights for all classes\n");
    print_row(stdout, K, weights);
d619 6
a624 6
if (pe==0)
{
printf("%d: after computing weights\n",pe);
printf("weights for all classes\n");
print_row(stdout, K, weights);
}
d647 1
a647 1
      printf("This option does not work yet.\n");
d657 2
a658 2
if (pe==0)
printf("%d: after checking for eliminated clusters\n",pe);
d681 2
a682 2
if (pe==0)
printf("%d: after computing means\n",pe);
d685 3
a687 3
  if ((utDebugLevel > 0) && (pe == 0)) {
    printf("means for all classes\n");
    print_unnorm_matrix(stdout, K, numcols, means, range, minval);
d691 2
a692 2
if (pe==0)
printf("%d: before parallel comm. to collate means\n",pe);
d698 1
a698 1
    /* printf("%d: This is not the controlling process.\n",pe); */
d703 2
a704 1
printf("%d: after sending local means\n",pe);
d710 2
a711 1
printf("%d: after receiving global means\n",pe);
d716 1
a716 1
    /* printf("%d: This is the controlling process.\n",pe); */
d727 2
a728 2

printf("%d: after receiving local means\n",pe);
d733 2
a734 1
printf("%d: after broadcasting global means\n",pe);
d738 2
a739 2
if (pe==0)
printf("%d: after parallel comm. to collate means\n",pe);
d773 2
a774 2
if (pe==0)
printf("%d: after computing cov. matrices\n",pe);
d776 2
a777 2
if (pe==0)
printf("%d: before parallel comm. to collate cov. matrices\n",pe);
d783 1
a783 1
    /* printf("%d: This is not the controlling process.\n",pe); */
d790 1
a790 3
      /*
        cc = send_msg(&covar_k[1][1],numcols*numcols,0,K*pe+k,DA_FLOAT);
        */
d793 2
a794 1
printf("%d: after sending local cov. matrices\n",pe);
d804 2
a805 1
printf("%d: after receiving global cov. matrices\n",pe);
d810 1
a810 1
    /* printf("%d: This is the controlling process.\n",pe); */
d826 2
a827 1
printf("%d: after receiving local cov. matrices\n",pe);
d836 3
a838 1
printf("%d: after broadcasting global cov. matrices\n",pe);
d842 2
a843 2
if (pe==0)
printf("%d: after parallel comm. to collate cov. matrices\n",pe);
d846 3
a848 3
  if ((utDebugLevel > 0) && (pe == 0)) {
  printf("cov. matrix for class %d\n", k);
  print_unnorm_cov_matrix(stdout, numcols, numcols, covar_k, range);
d872 2
a873 2
    if (pe==0)
      printf("%d: before message passing for choosing random means\n",pe);
d888 1
a888 2

    if (pe==0)
d890 2
a891 2
      printf("%d: means: \n",pe);
      print_matrix(stdout, num_means, num_cols, means);
@


1.6
log
@changed naming from "mixmod" to "mixture"; other naming changes.
@
text
@d2 2
d5 2
a6 12
  Title:     da_clust_pp
  Author:    Alexander Gray
  Function:  Data analysis routines, Numerical Recipes style.
             This file contains clustering methods and related functions,
             for unsupervised learning of meaningful groupings in multivariate
             data.  
             This module contains parallelized versions of the functions in
             the da_clust module, and is intended to mirror it.  The PVM 
             parallel programming library is used.
  Reference: -
  How used:  First use - in ~/unsup/supercluster/sc.c.
  Notes:     - 
d8 19
d29 1
a29 1
static char rcsid[] = "$Id: da_clust_pp.c,v 1.5 1996/09/23 23:57:23 agray Exp agray $";
d32 4
a35 1
 *$Log: da_clust_pp.c,v $
d53 1
d58 1
d61 1
d67 1
d71 3
a73 2
#include "da_clust.h"
#include "da_dist.h"
d75 1
a75 65
#include "da_msg_pp.h"

#include "da_clust_pp.h"

/*******************************************************************************
 RANDOM_MEANS_PP
 Parallel version of random_means().
*******************************************************************************/
int random_means_pp(means, num_means, num_cols, pe)

    float **means;
    int   num_means, num_cols;
    int   pe;
{
    int cc;

    if (pe==0)
      printf("%d: before message passing for choosing random means\n",pe);

    barrier();
    if (pe != 0) /* This is not the controlling process */
    {	
      cc = recv_msg(&means[1][1],num_means*num_cols,0,0,DA_FLOAT);
    }
    else
    {
      /* choose initial values for means arbitrarily */
      random_means(means, num_means, num_cols);
      
      cc = broadcast_msg(&means[1][1],num_means*num_cols,DA_ALL_PES,0,DA_FLOAT);
    }


    if (pe==0)
    {
      printf("%d: means: \n",pe);
      print_matrix(stdout, num_means, num_cols, means);
    }

    return (UT_OK);
}


/*******************************************************************************
 RANDOM_MEANS_FROM_DATA_PP
 Parallel version of random_means_from_data().
*******************************************************************************/
int random_means_from_data_pp(means, num_means, num_cols, data, num_data, 
                              rows_or_cols, pe)

    float **means;
    int   num_means, num_cols;
    float **data;
    int   num_data;
    char  *rows_or_cols;
    int   pe;
{
    int   i,j,k, cc;
    char *selected;

    /****************** NOT PARALLELIZED YET ***********************/
    /***************************************************************/

    /* storage for indices for the data */
    selected = (char*) utMalloc(num_data * sizeof(char));
d77 2
a78 30
    /* randomly choose amongst the indices of the data */
    utRandomPick (num_means, num_data, selected);

    /* copy the chosen data to the means matrix */
    k = 1;
    for (i=1; i<=num_data; i++)
    {
      if (selected[i] == UT_TRUE)
      {
        for (j=1; j<=num_cols; j++)
        {
          if (utEqStr(rows_or_cols, "rows"))
            means[k][j] = data[i][j];
          else if (utEqStr(rows_or_cols, "cols"))
            means[k][j] = data[j][i];
          else
          {
            utError("Bad option given to pick_random_means()\n");
            return (UT_ERROR);
          }
        }
        k++;
      }
    }

    /* clean up */
    utFree(selected);
    
    return (UT_OK);
}
d82 14
a95 12
 K_MEANS_PP
 Parallel version of k_means().
 Perform the k-means clustering algorithm on a dataset of continuous values,
 returning the matrix of k mean vectors and a class labelling for each of the
 data.  The input matrices and vectors are all assumed to have been allocated
 before calling this function.  The argument initmeans should contain the
 mean vectors which will start off the k-means iterations.  The data is assumed
 to be stored in row vector form (i.e. each row is a datum).
 When the function is finished, each of the processors has the final means.
 Each processor will end up with its own chunk of the labels file.
 NOTE: In practice, it is a good idea to normalize the columns of the data to
 the same scale before performing k-means (see the function normalize_data()).
d350 5
a354 5
 MIXTURE_LIKELIHOOD_PP
 Parallel version of mixture_likelihood().
 Given mixture model parameters, a dataset, and allocated space for storing
 all the intermediate probabilities, compute the log-likelihood of the model 
 given the data.
d356 20
a375 19
 When the function is finished, each processor has the global log-likelihood 
 value.
 K is the number of components in the mixture.
 means is the  matrix of mean vectors, where each row is a mean.
 covars is the array of covariance matrices.
 weights is the vector of class weights.
 data is the dataset.
 probs is the matrix to contain all intermediate probabilities.
 prob_data is the vector to contain the posterior probability of each datum
 given the model.
 sum_probs is the vector to contain the posterior probability of each class
 given the data.
 numrows is the number of data.
 numcols is the number of attributes.
 rows_or_cols is the string specifying whether data are stored as rows or as
 columns.
 min_diag is a small number for perturbing ill-conditioned covariance matrices.
 pe is the current processing element's id.
 numPE is the total number of processing elements.
d548 27
a574 26
 ESTIMATE_MIXTURE_PARAMS_PP
 Parallel version of estimate_mixture_params().
 Given the matrix containing the posterior probability of each datum belonging
 to each class in the mixture model, a dataset, and allocated space for storing
 all the parameters to be computed, estimate the mixture model parameters from
 the class probabilities.
 Assumes the data are stored in columns, not rows.
 When the function is finished, each processor has the global values for the
 parameters.
 K is the number of components in the mixture.
 means is the  matrix of mean vectors, where each row is a mean.
 covars is the array of covariance matrices.
 weights is the vector of class weights.
 data is the dataset.
 probs is the matrix to contain all intermediate probabilities.
 sum_probs is the vector to contain the posterior probability of each class
 given the data.
 numrows is the number of data.
 numcols is the number of attributes.
 min_weight is the minimum class weight; it determines when a class is consi-
 dered to have collapsed.
 num_empty_clusters is the number of classes that have collapsed.
 empty_cluster_ok is the flag indicating whether to continue in the empty
 cluster case.
 pe is the current processing element's id.
 numPE is the total number of processing elements.
d835 51
@


1.5
log
@updated utRandomPick() to use char's for selected array.
@
text
@d18 1
a18 1
static char rcsid[] = "$Id: da_clust_pp.c,v 1.4 1996/09/23 22:40:46 agray Exp agray $";
d22 3
d155 2
a156 2
 KMEANS_PP
 Parallel version of kmeans().
d168 4
a171 4
int kmeans_pp(float **data, int numrows, int numcols, float **means, 
              float **initmeans, int *clustsize, int *class, int K,
              int *numiters, int *num_empty_clusters, bool empty_cluster_ok,
              int pe, int numPE)
d183 1
a183 1
printf("%d: beginning kmeans_pp\n",pe);
d198 1
a198 1
printf("%d: beginning kmeans_pp iterations\n",pe);
d258 1
a258 1
printf("%d: before kmeans_pp parallel comm. to collate clustsize\n",pe);
d294 1
a294 1
printf("%d: after kmeans_pp parallel comm. to collate clustsize\n",pe);
d328 1
a328 1
printf("%d: after checking for empty clusters in kmeans_pp\n",pe);
d336 1
a336 1
printf("%d: after normalizing by clustsizes in kmeans_pp\n",pe);
d338 1
a338 1
printf("%d: before doing parallel comm. to collate means in kmeans_pp\n",pe);
d394 1
a394 1
printf("%d: after doing parallel comm. to collate means in kmeans_pp\n",pe);
d406 1
a406 1
printf("%d: after convergence loop in kmeans_pp()\n",pe);
d408 1
a408 1
printf("%d: empty_clusters = %d in kmeans_pp()\n",pe,empty_clusters);
d415 1
a415 1
printf("%d: num_empty_clusters = %d in kmeans_pp()\n",pe,*num_empty_clusters);
d421 2
a422 2
 MIXMOD_LIKELIHOOD_PP
 Parallel version of mixmod_likelihood().
d447 1
a447 1
float mixmod_likelihood_pp(int K, float **means, float ***covars, 
d610 1
a610 1
printf("sum_probs in mixmod_likelihood:\n");
d618 2
a619 2
 ESTIMATE_MIXMOD_PARAMS_PP
 Parallel version of estimate_mixmod_params().
d645 1
a645 1
int estimate_mixmod_params_pp(int K, float **means, float ***covars, 
@


1.4
log
@minor changes.
@
text
@d18 1
a18 1
static char rcsid[] = "$Id: da_clust_pp.c,v 1.3 1996/07/19 17:53:03 agray Exp agray $";
d22 3
d50 1
a52 1
#include "da_clust.h"
d69 2
a70 2
if (pe==0)
printf("%d: before message passing for choosing random means\n",pe);
d86 5
a90 5
if (pe==0)
  {
    printf("%d: means: \n",pe);
    print_matrix(stdout, num_means, num_cols, means);
  }
d111 1
a111 1
    short *selected;
d117 1
a117 1
    selected = (short*) utMalloc(num_data * sizeof(short));
@


1.3
log
@added random_means_pp(), started random_means_from_data_pp()
@
text
@d18 1
a18 1
static char rcsid[] = "$Id: da_clust_pp.c,v 1.2 1996/07/17 20:42:25 agray Exp agray $";
d22 3
d64 1
d72 1
a72 1
      cc = recv_msg(&means[1][1],num_means*numcols,0,0,DA_FLOAT);
d77 1
a77 1
      random_means(means, num_means, numcols);
d79 1
a79 1
      cc = broadcast_msg(&means[1][1],num_means*numcols,DA_ALL_PES,0,DA_FLOAT);
d86 1
a86 1
    print_matrix(stdout, num_means, numcols, means);
d107 1
a107 1
    int   i,j,k;
d476 1
a476 1
    vector_prob_gauss(data, mean_k, covar_k, numcols, numrows, probs_k,
d480 1
a480 1
      printf("first three density values from vector_prob_gauss\n");
@


1.2
log
@cosmetic.
@
text
@d18 1
a18 1
static char rcsid[] = "$Id: da_clust_pp.c,v 1.1 1996/07/11 16:42:35 agray Exp agray $";
d22 3
d50 93
@


1.1
log
@Initial revision
@
text
@a13 3

  Compile:   make
  Example:   -
d18 1
a18 1
static char rcsid[] = "$Id$";
d21 4
a24 1
 *$Log$
@
