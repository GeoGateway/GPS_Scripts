\documentclass{slides}
\begin{document}
DA Library talk
===============

"Building a Framework for Code Reuse"

0. Intro
--------

These are my opinions based on my experience in programming, and my observations
	of several good programmers (Joe Roden, Dennis Decoste, Ramesh Subramonian,
	Rob Farber, Paul Hilfinger, Kurt Partridge, Gordon Gray) and lots and lots
	of good, bad, and medium code.

Abstract:
	Code reuse should not need to be explained.  Assuming that you have an
environment which demands lots of code, over time, which ends up needing to 
solve problems and sub-problems which crop up over and over in some form,
code reuse saves time and mental energy, and reduces bugs and cost over time.
(though obvious, i will demonstrate here and there why this is true)
This is true on a one-programmer scale, and especially true on a multi-
programmer scale.
(i'll talking about code sharing at the end)
	I will talk about what I perceive to be the ISSUES in building a framework 
for code reuse.  Then I'll present one set of solutions to these issues which
comprises a library I've been building and using for all of my C software
development since January.  
(this is my Data Analysis library)
This set of solutions is based on common-sense 
principles of software engineering applied to the particular needs of the MLS 
group.  
(I'll talk about what those needs are)
	My intention in this talk is to illustrate, using a real example, the 
process of designing a framework for code reuse which can be used to create a 
group or multi-group resource.  
The principles I will outline will encompass 
both C/C++ and Matlab code, and should be illustrative for anyone contemplating
building a large reusable code infrastructure.
(my library is being used already as a multi-programmer resource by 3 people;
after seeing what i have to say, you may either wish to use my library and
its framework directly, or perhaps use it as a starting point for doing your
own thing.  or use it as an anti-example.)


I. Software Development is an Art
---------------------------------

Doing it properly requires careful thought, direct experience, and good
computer science intuition.

0. Easy:  
	- knowing the syntax and semantics, how to compile
		e.g. to define a structure, say "struct <name> { ... };"

(absolutely anyone can do this)

1. Takes experience:  
	- finding useful idioms
		e.g. to use a structure like a basic type, say "typedef struct <name>
			{ ... } <newname>;"
	- safeguarding against common pitfalls, learning common causes of bugs
		e.g. to avoid seg. faults, have cases that check whether a pointer is
			NULL
	- learning solutions to common problems
		e.g. to make a two-dimensional array, say "<array> = (<type>**) 
			malloc(<maxsize> * sizeof(<type>*)); then say "for (i..."
	- having a base of coded solutions, for copy-and-modify
		e.g. any time you want to process command-line arguments, just copy
			the part of main() in the last program you wrote that does that,
			and modify it according to your current needs
	- coding clearly
		e.g. indenting, source code commenting, simple flow of control
	- coding consistently
		e.g. consistent naming of functions, formatting, levels of abstraction
	- coding generally
		e.g. instead of making a function to create a particular array which
			has a particular size, make it take a size parameter
	- using good tools
		e.g. gdb, purify, emacs
	- making code efficient
		e.g. macros, eliminating unnecessary pointer dereferences, buffering
			and contiguity, doing operations in-situ to save memory
	- realizing an algorithm in code naturally
		e.g. keep variables consistent with notation of the reference, keep the
			conceptual steps identifiable

	Requires attention to the environment you are in:
	- learning the needs of the environment you are in
		e.g. time constraints, language constraints, machine constraints, 
			maintenance needs, speed needs
	- observing the common programming problems
		e.g. particular functions, how they are used, what executables are 
			needed, what program options are usually needed

2. Takes experience and intuition:
	- how much to trade space vs. time for your problems and environment
		e.g. can we normally afford to read everything into RAM?  can we 
			normally afford to read directly from disk?
	- how much to trade development time vs. code quality for your problems
		e.g. what are the deadlines like?  how much time is there to design,
			test, add features, ...?
	- how much to trade abstraction vs. transparency in your code base
		e.g. do you make read_dataset() a function or not?  do you typedef
			float** to matrix or not?
	- how much to trade flexibility vs. consistency in your code base
		e.g. do you use a function naming convention or not?  do you allow
			TRUE and FALSE to exist or not?
	- how much to trade proprietariness vs. external dependence in your code
		base (aka customization vs. standardization)
		e.g. do you depend on a standard library or not?  do you adopt someone
			else's naming or formatting conventions or not?
	- how much to trade optimization vs. clarity in your code base
		e.g. do you call the read_dataset() function or just do it in line?
			do you store three different conceptual values in the same variable
				and do sequential computations or do you use three different
				variables?
	- how much to trade optimization vs. portability in your code base
		e.g. do you use in-line assembly code for inner loops or not?  do you
			use machine-specific routines or not?

3. --> This leads to the ability to decide:
	- what mechanism for code reuse to create
		e.g. 4GL/code dumper, library, visual programming language
	- what conceptual language you want to create
		e.g. filter-based, connection-based, interactive, object-based
	- what the levels of abstraction are
		e.g. how much does each function do, how much is functionality hidden
			at each level
	- how to organize and index your code
		e.g. size and scope of modules, where to draw library boundaries
	- what functionality is in the library
		e.g. definition of scope of the library, what's definitely not in the
			library
	- what conventions to set up and enforce
		e.g. naming, command-line argument processing, error codes, error 
			reporting, scope of functionality of functions and programs
	- what language to use
		e.g. C, C++, Matlab, Java, mixtures of languages
	- how to manage the building of the code base
		e.g. all at once, incremental


II. The choices I made
----------------------

Environment:
	lots of quick prototypes
	variety of problems
	all related to data analysis and manipulation
	code users are the data analysis experts (people in the group)
	data mining - big datasets
	matlab is already used a lot 

Overriding principle #1: simplicity for efficiency
	keep things as simple as possible
		easier to learn, to maintain, to reason about
	80/20 rule - only the first 80% is worth the effort, due to uncertainty
		about future needs (aka "don't overoptimize")
		corrolary: you can usually do the first 80% by doing simple things
	rapid prototyping ("move then consider") is the most efficient way to 
		explore a solution space
		seeing as you go is also a corrolary: starting with something simple
			points the direction for the rest of what you need; then you're done

Overriding principle #2: the past predicts the present
	what you last did has a lot to do with what you want to do now
		i.e., copy-and-modify is the most efficient way to write new code
	this means that effort should be put into indexing and organizing old code,
		making old code understandable and trusted for reuse, and using a 
		consistent style and conventions so that you can use old snippets 
		of code verbatim
	however, you don't want to limited by the past, so you keep the conventions
		to a minimum and in general maintain as much flexibility as possible

Overriding principle #3: avoid the big monsters
	monsters are things that waste your time in huge chunks all at once
	they are:
		bugs and incompatibilities in the underlying foundation, like the 

3. Highest-level decisions:
	- what mechanism for code reuse to create
		a library
		this is programmer-oriented, rather than user-oriented
		allows maximum flexibility for creating a variety of solutions
		we don't have the solution space focused enough to warrant an overarch-
			ing interface
		even if we want a code dumper, we'll need a library first
	- what conceptual language you want to create
		functions on matrices and vectors
		matlab-like
		this dovetails nicely with the wide use of matlab in the group for
			prototyping functions
		historical note
			got inspiration from Mike Burl, who had a handful of functions
				that did linear algebra operations, e.g. mat_vec_mult(), which 
				sometimes called NR functions
		a matlab-like C library has these advantages over matlab:
			faster
			has the possibility of complex structures
			you can use only the data types you need
			no memory limit; can also play with disk as needed
			can write parallel code
			more portable
			free
			the style of programming encourages memory efficiency, not the 
				reverse
		it has these disadvantages compared to matlab:
			takes longer to write something in C
				(however, I claim that this approach closes that gap signifi-
				cantly)
			not as many pre-written functions as in matlab
				(again...)
			matlab has graphics, an interpreter - this is what makes it great
				for prototyping
		note that matlab and c already communicate both ways
			c can use matlab code via a matlab2c dumper
				this is okay but a theorem about translators is that they do
					not preserve understandability very well and in this case
					there is no reason to think it makes things more efficient
				so i wouldn't recommend its use most of the time; it only saves
					you interpretation time
			matlab can use c code through .mex files
				this is where the matlab-like model used in my c code can make
					this interfacing trivial
		so by linking C and matlab with a common conceptual language, we can
			conceive of a prototype -> fast implementation pipeline
		we also achieve a general union of mental models between the two media
	- what functionality is in the library
		layers:
				CoolTools
			~~~~~~~~~~~~~~~~~
			       DA
			~~~~~~~~~~~~~~~~~
			  NR, PVM   util
		util contains very low-level operations which are very broadly useful
			e.g. open_file(), log(), log_printf(), get_args()
		NR contains low-level numerical operations
			e.g. ludcmp(), svd()
		PVM contains portable, low-level parallel programming operations
			e.g. pvm_send(), pvm_recv()
		DA contains low-level data analysis operations all the way up to high-
			level
			e.g. read_matrix(), invert_matrix(), mult_mat(), pca(), 
				compute_hmm_likelihood(), est_hmm_params(), learn_hmm()
		CoolTools are often just program shells which handle input and output,
			and call one high-level function in the DA lib.
	- what the levels of abstraction are
		so there are strong and useful conceptual separations at the library
			levels, and other separations at the module level within DA and util
		the guiding principle is to have the programmer, at any given place,
			see what he needs to see but no more
		so abstraction is spread out at all levels, at an equal proportion of
			abstraction to transparency
		this is most easily learned by emulating existing parts of the code
	- how to organize and index your code
		right now, there are these modules:
			da_data.c		da_data_pp.c
			da_dist.c		da_dist_pp.c
			da_unsup.c		da_unsup_pp.c
							da_msg_pp.c
			da_linalg.c
			da_rand.c
			da_prob.c
		there is a convention that equivalent parallel functions mirror the
			serial versions, which are always written first; similarly for the
			modules
		the idea is to have these module boundaries evolve on an as-needed
			basis, since the whole library can grow in arbitrary directions,
			depending on what code needs writing
	- what conventions to set up and enforce
		there is a small and simple set of conventions for naming various
			things and commenting
		other than that, there are recommended, existing mechanisms for 
			things like command-line argument processing, error codes, and
			error reporting, based on the util library
		so the constraints on programming individuality are designed to be just
			enough to maintain minimal consistency, but not more than that
	- what language to use
		C
		chose this because it is the most portable language on earth, everyone
			knows it, and it has no problems with efficiency or expressiveness,
			vast infrastructure
		the only time you need to go beyond C is to buy some extra capability
		C++ - extra capabilities may or may not add a lot for this type of code,
			infrastructure is significant but still not ubiquitous - demands
			some investigation though
		Java - extra capability, taking a big risk as far as portability;
			can probably get the extra capability without the risk by just using
			it for interfaces and calling C code underneath
		Matlab - previously discussed; seems complementary
	- how to manage the building of the code base
		whenever you write a program, you abstract out what are general
			functions, and put them into the library - you try to make your
			program depend as little as appropriate on non-library code
		this way, two things happen:
			the library grows by almost the amount of new code you had to write,
				so all that work can be reused later
			the conscious thought you put into writing code for the library is
				no more than what you had to do anyway, except for putting it
				into the right places
		over time, the library grows to cover everything that has been done in
			the past, yet takes very little conscious effort to expand, since
			it is done on an as-needed basis

2. Code-level decisions
	- how much to trade space vs. time for your problems and environment
		fact: disk access is about 1 million times as slow as memory access
		so computationally intensive algorithms (which all of ours tend to be)
			never want to touch disk unless it is absolutely necessary
		so my whole code base is designed with RAM maximization, including
			parallelization, in mind
		all memory allocation is done at the top level and passed in, so that
			all RAM usage is explicit - this coding style forces the user to
			be memory conscious
		the allocation and free'ing routines also allow tracking of RAM usage
			at any given point
		routines for reading in data in a buffered style implement the quick 
			piping of data to RAM
		in general, i will lean toward using more space to reduce time since i
			have these ways of increasing/maximizing RAM
	- how much to trade development time vs. code quality for your problems
		fairly short turnaround times are required, allowing enough time to
			get functionality going and validate
		features tend to be of a standard type, so templatization is especially
			powerful here
		an evolutionary approach to building the library means that you can put
			in quick versions of functions in the library to get code working, 
			then underlying functions get improved later then all software
			depending on that part of library improve; this is a way to improve
			code later without having to change it at the high level - it
			relies on good use of abstraction
	- how much to trade abstraction vs. transparency in your code base
		don't hide data structures
			thm: data structures are the conceptual foundation of the whole
				program, so making these simple makes the program simple, and
				making these complex makes the program complex
			hiding what the data really is (like a float** typedef'ed to a
				matrix) is an annoying obstacle to anyone who wants to start
				doing something with that data structure, like debugging on it
			exception - when there is an absolutely natural 'object', or a
				large group of structures that are related, and it becomes 
				cumbersome or unclear not to define a more complex structure
		hide functions wherever possible
			the simplest way to understand a function is to know its input, 
				output, and conceptually what it does - everything else can be
				hidden as long as the name is descriptive
		these two ideas make a program into a conceptual box-and-arrow flow
			diagram
	- how much to trade flexibility vs. consistency in your code base
		define just a few constants like error codes and true/false, a 
			naming convention for similar functions, and enforcing liberal
			commenting - that's it, besides some prewritten mechanisms that are
			recommended for use
		this is probably the minimum for the consistency i want - the indivi-
			dual programmer is otherwise allowed to be an individual
		this is very important for keeping multiple users happy and to allow
			adaptation of styles for different tasks
	- how much to trade proprietariness vs. external dependence in your code
		base (aka customization vs. standardization)
		we leverage a large base of standard code - NR lib.
			this is done mainly because that would be time-consuming and error-
				prone for me to write myself; for most other things, this is not
				the case; but where it is, relying on external code can make 
				sense
		NR dependence is only okay because:
			it is almost free ($50)
			it is a ubiquitous standard
			it has great documentation compared to similar packages (the book)
			the massive user base ensures the pointing out and fixing of bugs
			it is highly portable
			it actually exists in multiple languages
			it also uses a very minimal set of conventions, so it imposes little
			we can build upon it by using its own data structures, which are 
				actually pretty good (contiguous allocation, simple indexing,
				pointer indexing)
		in general these underlying NR functions can be used as is and don't
			have to change; it is the higher level concepts and algorithms that
			you want proprietary so you can change them
		all the same goes for PVM; we may actually move to the other one
	- how much to trade optimization vs. clarity in your code base
		optimizations which start to obfuscate the meaning of the code, or its
			translation to the original algorithm, are avoided unless absolutely
			needed, and in this case these parts of the code are thoroughly
			documented and isolated if machine-specific
		clarity is far more important in the long run
		transparent algorithm implementation is what one needs in an algorithm-
			development house
		optimization is only necessary for highly specific situations, usually
		we normally just want to prototype something, so optimization should
			only be done on an as-needed basis
		optimization is a never-ending process, so doing it as needed also tells
			you when to stop
	- how much to trade optimization vs. portability in your code base
		again, optimization loses out - portability is more important
		the need to port doesn't seem to happen often, but when it does, you
			may be in big trouble if you haven't designed for it
		examples - cray, hp convex, pc workstations are on the horizon


1. Simple stuff:
	- finding useful idioms
		these are used by convention throughout the library; obscure idioms are
			avoided
	- safeguarding against common pitfalls, learning common causes of bugs
		done by convention throughout the library
	- learning solutions to common problems
		these are implemented as low-level routines which reside in the bottom
			layer of abstraction
	- having a base of coded solutions, for copy-and-modify
		this is done by having all the pieces of old solutions visible, from
			util fns. up to CoolTools		
	- coding clearly
		done by convention throughout the library
	- coding consistently
		done by convention throughout the library
	- coding generally
		done by convention throughout the library
	- using good tools
		up to the user
	- making code efficient
		simple and obvious things are done by convention throughout the library
	- realizing an algorithm in code naturally
		keep variables consistent with notation of the reference, keep the
			conceptual steps identifiable
		this is one reason I allow lots of flexibility in naming intra-function
			entities (variables)

	The environment:
	- meeting the needs of the environment you are in
		allows quicker construction of working code overall, due to
			using pre-written, pre-tested chunks of code of various sizes
			facilitating copy-and-modify at all levels, including main(), the
				Makefile, the command-line arguments
			lots of examples to draw upon for quickly forming a mental model
		C and matlab are used here
		workstations are pretty fast - don't need to super-optimize for time, 
			but need to be concerned about memory due to massive datasets
		we have parallel capability, so our setup should allow natural parallel-
			ization
		we need to write code that can be understood between users and 
			modified by others
	- facilitating the solution of the common programming problems
		this is handled by doing everything in terms of the library; then all
			past code is available


III. Sharing Code - Multiple Programmers

There are several issues which arise when trying to share code:

	- how are additions and modifications to the code base managed?
		e.g. not at all, by democracy, by republic, by dictatorship
	- how is the overall organization of the code base evolved and maintained?
		e.g. not at all, by democracy, by republic, by dictatorship
	- how are changes to existing code made known to all users?
		e.g. not at all, change log, memos sent
	- how is a single code base modified by multiple users asynchronously?
		e.g. free-for-all, memos sent and agreements made, version control
	- how is knowledge about the code base represented?
		e.g. what's in the library, how to modify it, how to use it, what the
			current state is, bugs

My set of solutions:

	- how are additions and modifications to the code base managed?
		by a library manager (conscientious dictator)
		this is necessary to maintain integrity and consistency of the code base
		this reduces discussion to a minimum in most cases; only issues with
			any substance are brought up for discussion by the library manager
		this gives some quality assurance mechanism for new code
		assigns responsibility to someone for making sure the library is kept
			in good shape
	- how is the overall organization of the code base evolved and maintained?
		by the library manager
		someone has to be constantly monitoring the overall evolution of the
			library, or it will become a mess quickly
	- how are changes to existing code made known to all users?
		RCS logs in each source file
		e-mail list - changes which affect users will be broadcast
	- how is a single code base modified by multiple users asynchronously?
		RCS
	- how is knowledge about the code base represented?
		significant commenting in all source code, including descriptions for
			each function
		web page FAQ - includes how to use, how to modify, pointers to docs
			below
		index.txt - this is automatically created with a make-index script
		help script - just grabs one thing from the index.txt file
		conventions.txt
		design.txt
		todo.txt
		lots of examples - all the CoolTools
\end{document}
