DA Library Design Rationale
---------------------------

(This file will be the basis of a LaTeX document describing the Library.)


* Purpose:

	framework for code reuse


* Goals:

	- #1 goal - simplicity (of appearance and conventions)
		this has profound implications for lots of other factors (see below)

	- programmer orientation (it's a library)
		this offers the most flexibility in a situation where 1) we develop lots
			of quick prototypes, for a variety of problems, that 2) are used by
			ml experts (ourselves)
		thus we don't need an overarching interface
		since our problems are different every time, it's hard to do a code
			dumper, which requires a limited domain to make effective
		see "Other options" below for the alternatives

	- uniformity/consistency
		this is what gives you all the advantages of a single framework
		this facilitates what is perhaps the most powerful programming
			technique - copy and modify

	- useful and natural abstraction
		various levels of abstraction are used, so that code reuse can happen
			at all levels
		wherever it is natural to have a routine, it is created; no artificial
			abstraction layers exist
		in general, the library includes the highest-level function that can
			created for a given task, like learn_hmm() for example; this reduces
			the building of standalone programs, quite often, to calls to a 
			single function
			
	- flexibility across different tasks
		you get most of the flexibility from the fact that it is just a library
		having a small set of 'rules', or being simple, also gives flexibility

	- expressive flow of control
		since it is a library, any flow of control that C/C++ allows can be used
			to connect the pieces

	- understandability
		a high level of source code documentation is essential for code reuse
		good organization helps keep complexity and confusion under control

	- reasonable performance (without sacrificing legibility)
		the underlying code is NR, so it is reasonably efficient
		major performance tweaks are made throughout, but not to the point that
			it makes the algorithms unclear
		assumes strategy of maxing out RAM; therefore memory must be allocated
			in a cautious way - malloc only at the highest level, and pass in
			chunks of blank memory to functions, so that memory can be more
			clearly controlled and is all done up front; also malloc'ing small
			amounts in low-level functions, which may be called several times,
			is a slowdown when compared to just passing in the memory; when
			memory gets thin, move to parallelism
			
	- use of standard software
		practical problems regarding finding/fixing bugs and sharing expertise
			are lessened by having a wide and deep infrastructure of users
		it uses Numerical Recipes as a base, which is free and widely used
		the only other thing you need is C/C++ 


* Other options

	other ways to achieve a good level of code reuse are:
		- a code dumper (4GL) generated from a high-level specification, like:
			- a visual programming language, allowing blocks to be connected 
				with lines (e.g. LabView)
			- a file containing a specification in a high level language 
				(e.g. Dennis' neural net dumper written in perl)
		- a code dumper generated from a medium-level language, like:
			- a smart macro preprocessor front-end to C++ which allows either
				embedded code or a different sort of syntax to be used in a
				C++ program
				(e.g. Wray Buntine's PNS bayes net dumper, which allows 
					convenient reference to probabilities)
		- a high-level interface which simply does all of the operations you
			want to do directly; then you keep adding sub-programs to this suite
			and its interface 
			(e.g. xclass)
			- a high-level interface with a nifty programming language allows
				much more flexibility and is a significant improvement over this
				(e.g. matlab)
	an example of something close to the DA lib. is MLC++

* Design:

	layers:

		DA 
		~~~~~~
		NR
		~~~~~~
		util

	structures - matrix (float, double, int, char), vector, pointers to them
		these are the only types that are ever passed or returned by functions
			in the library (besides the basic types)
		except for a few cases, where specialized models require tons of 
			associated parameters (e.g. hmm); otherwise special structures are
			avoided
		this matches the matlab paradigm, making this library somewhat comple-
			mentary to matlab; e.g. converting matlab functions to C DA lib.
			functions is fairly natural
		thm: structures are the key to what makes a programming method clear or
			unclear - this is why tcl is very simple to learn, and perl is 
			difficult; it is the hardest thing about reading someone else's
			code - understanding the structures and what in them is important
		thm: abstraction is bad when it hides too much of what is happening; a
			programmer wants to know as much as he needs, but not more


* Matlab complementarity

	philosophy/mental model - vector and matrix operations

	better than matlab in these ways:
		has the possibility of complex structures
		faster
		not efficient with memory
		no memory limit; can play with disk as needed
		easier to write parallel code
		more portable
		free
		the style of programming encourages memory efficiency, not the reverse

	worse than matlab in these ways:
		takes longer to write something ...?
		not as many functions ...?
		matlab has graphics, an interpreter
		
	equal to matlab in these ways:
