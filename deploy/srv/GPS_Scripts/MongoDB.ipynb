{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All paths as in properties.py but with changed UNIX style paths for the code to execute across platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "V={}\n",
    "WORK_DIR = '/media/hru/Data/_Active_Projects/PythonRDAHMM/test'\n",
    "V['cron_path']=os.path.join(WORK_DIR, \"RDAHMM\",\"CRON_Download/\")  \n",
    "V['download_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Download/\")  \n",
    "V['script_path']=os.path.join(WORK_DIR,\"PythonRDAHMM/\") \n",
    "V['data_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Data/\")\n",
    "# temp_path is the temporary working directory for ingesting raw data\n",
    "V['temp_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"TEMP/\")\n",
    "V['model_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Model/\")\n",
    "V['eval_path']=os.path.join(WORK_DIR,\"daily/single/\")\n",
    "V['train_epoch']=\"2013-12-31\"\n",
    "V['rdahmm_bin']=os.path.join(WORK_DIR,\"RDAHMM\", \"rdahmm3\",\"bin\", \"rdahmm\")\n",
    "V['rdahmm_model_parm']=\"-data <inputFile> -T <dataCount> -D <dimensionCount> -N 5 -output_type gauss -anneal -annealfactor 1.1 -betamin 0.1 -regularize -omega 0 0 1 1.0e-6 -ntries 10 -seed 1234\"\n",
    "V['rdahmm_eval_parm']=\"-data <proBaseName>.all.input -T <dataCount> -D <dimensionCount> -N 5 -output_type gauss -A <modelBaseName>.A -B <modelBaseName>.B -pi <modelBaseName>.pi -minvalfile <modelBaseName>.minval -maxvalfile <modelBaseName>.maxval -rangefile <modelBaseName>.range -eval\"\n",
    "V['dygraphsJs']=os.path.join(WORK_DIR,\"PythonRDAHMM\",\"dygraphsJsCreator.pearl\")\n",
    "\n",
    "def properties(key):\n",
    "    return V[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the json file for Presentation layer\n",
    "This is the existing code obtained from PythonRDAHMM/create_summary_jsons.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, string, re, json\n",
    "from datetime import date, datetime, timedelta, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# command line input argument\n",
    "# dataSet = \"rawNeuTimeSeries.MEASURES_Combination\"\n",
    "dataSet = \"UNR_SPLICE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to set <b>endDate = '2016-10-19'</b> as the database was downloaded from gf9 that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some useful global constants\n",
    "today = datetime.today()\n",
    "serverName = \"gf9.ucs.indiana.edu\"\n",
    "updateTime = str(today.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "beginDate = \"1994-01-01\"\n",
    "# endDate = str(today.strftime(\"%Y-%m-%d\"))\n",
    "endDate = '2016-10-19'\n",
    "centerLng = \"-119.7713889\"\n",
    "centerLat = \"36.7477778\"\n",
    "stateChangeNumTxtFile = \"stateChangeNums.txt\"\n",
    "stateChangeNumJsInput = \"stateChangeNums.txt.jsi\"\n",
    "allStationInputName = \"all_stations.all.input\"\n",
    "filters = \"Fill_Missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used to separate parts of the station name\n",
    "SEPARATOR_CHARACTER=\"_\"\n",
    "NO_DATA_TIME_STAMP=\"22:22:22\"\n",
    "FINAL_PATH=properties('eval_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationId(stationList, stationData):\n",
    "    #Get the station name.\n",
    "    stationName=stationList.split(SEPARATOR_CHARACTER)[2];\n",
    "\n",
    "    stationData['id'] = stationName\n",
    "    stationData['pro_dir'] = \"daily_project_\" + stationName + \"_\" + endDate\n",
    "    stationData['AFile'] = \"daily_project_\" + stationName + \".A\"\n",
    "    stationData['BFile'] = \"daily_project_\" + stationName + \".B\"\n",
    "    stationData['InputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input\"\n",
    "    stationData['RawInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.raw\"\n",
    "    stationData['SwfInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".plotswf.input\"\n",
    "    stationData['DygraphsInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".dygraphs.js\"\n",
    "    stationData['LFile'] = \"daily_project_\" + stationName + \".L\"\n",
    "    stationData['XPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.X.png\"\n",
    "    stationData['YPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Y.png\"\n",
    "    stationData['ZPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Z.png\"\n",
    "    stationData['XTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.X_tiny.png\"\n",
    "    stationData['YTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Y_tiny.png\"\n",
    "    stationData['ZTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Z_tiny.png\"\n",
    "    stationData['PiFile'] = \"daily_project_\" + stationName + \".pi\"\n",
    "    stationData['QFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.Q\"\n",
    "    stationData['MaxValFile'] = \"daily_project_\" + stationName + \".maxval\"\n",
    "    stationData['MinValFile'] = \"daily_project_\" + stationName + \".minval\"\n",
    "    stationData['RangeFile'] = \"daily_project_\" + stationName + \".range\"\n",
    "    stationData['ModelFiles'] = \"daily_project_\" + stationName + \".zip\"\n",
    "    stationData['RefFile'] = \"daily_project_\" + stationName + \".input.ref\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationStartDate(stationDir, stationData):\n",
    "    startFileName = stationDir + \"daily_project_\" + stationData['id'] + \".input.starttime\"\n",
    "    if (os.path.isfile(startFileName)):\n",
    "        with open(startFileName,\"r\") as startFile:\n",
    "            startDate = startFile.readline().rstrip()\n",
    "        startFile.close()\n",
    "    else:\n",
    "\t\tstartDate = \"1994-01-01\"\n",
    "    stationData['start_date'] = startDate\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationRefLatLonHgt(stationDir, stationData):\n",
    "    refFileName = stationDir + stationData['RefFile']\n",
    "    refLat=\"\"\n",
    "    refLon=\"\"\n",
    "    refHgt=\"\"\n",
    "    if (os.path.isfile(refFileName)):\n",
    "        with open(refFileName,\"r\") as refFile:\n",
    "            refParts=refFile.readline().split(\" \")\n",
    "            refLat=refParts[0]\n",
    "            refLon=refParts[1]\n",
    "            refHgt=refParts[2].rstrip() # Have to chomp off the final \\n\n",
    "        refFile.close()\n",
    "    else:\n",
    "        refLat=\"1.0\"\n",
    "        refLon=\"2.0\"\n",
    "        refHgt=\"-1.0\"\n",
    "\n",
    "    stationData['lat'] = refLat      \n",
    "    stationData['long'] = refLon      \n",
    "    stationData['height'] = refHgt\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStatusChanges(stationDir, stationData):\n",
    "    # Open the .all.Q and the .all.raw files.  We get the state from the first and\n",
    "    # the data from the second. \n",
    "    # TODO: for now, we assume these files always exist\n",
    "    qFileName = stationDir + stationData['QFile']\n",
    "    rawFileName = stationDir + stationData['RawInputFile']\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Bail out if the required files don't exist   \n",
    "    if((not os.path.isfile(qFileName)) or (not os.path.isfile(rawFileName))): \n",
    "        return \n",
    "    qFile = open(qFileName,\"r\")\n",
    "    rawFile = open(rawFileName,\"r\")\n",
    "\n",
    "    stateChanges = []\n",
    "    changeCount = 0\n",
    "    # Now step through the Q file looking for state changes\n",
    "    # If we find a state change, get the date from the raw file\n",
    "    # We will save these to the string stateChangeArray since we\n",
    "    # need to record in latest-first order\n",
    "    qline1 = qFile.readline()\n",
    "    rline1 = rawFile.readline()        \n",
    "    while True:\n",
    "        eventData = {}\n",
    "        qline2 = qFile.readline()\n",
    "        rline2 = rawFile.readline()\n",
    "        if not qline2: break\n",
    "        \n",
    "        # See if qline1 and qline2 are the same.  If so, extract the dates from rline1 and rline2\n",
    "        # The line splits below are specific to the raw file line format.\n",
    "        if (qline1.rstrip() != qline2.rstrip()):\n",
    "            eventdate = rline2.split(\" \")[1] \n",
    "            eventdate = eventdate.split(\"T\")[0]\n",
    "            oldstate = qline1.rstrip()\n",
    "            newstate = qline2.rstrip()\n",
    "            eventData['date'] = eventdate\n",
    "            eventData['from'] = oldstate\n",
    "            eventData['to'] = newstate \n",
    "            stateChanges.append(eventData)\n",
    "            changeCount += 1\n",
    "\n",
    "        # Make the previous \"next\" lines the \"first\" lines for the next comparison\n",
    "        qline1=qline2\n",
    "        rline1=rline2\n",
    "\n",
    "    stationData['status_changes'] = stateChanges\n",
    "    stationData['change_count'] = changeCount\n",
    "\n",
    "    # Clean up\n",
    "    qFile.close\n",
    "    rawFile.close\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setTimesNoData(stationDir, stationData):\n",
    "    rawFileName = stationDir + stationData['RawInputFile']\n",
    "\n",
    "    # Required file doesn't exist so bail out\n",
    "    if(not os.path.isfile(rawFileName)): return\n",
    "    rawFile = open(rawFileName, \"r\")\n",
    "    \n",
    "    noDataRanges = []\n",
    "    noDataCount = 0\n",
    "    noDataEvent = {}\n",
    "\n",
    "    # We need to set a no-data range from beginDate (for the epoch, 1994-01-01) to the day before\n",
    "    # our first data point for this station.  If the station has data before 1994-01-01, then \n",
    "    # ignore.\n",
    "    firstDataDateParts=rawFile.readline().split(\" \")[1].split(\"T\")[0].split(\"-\");\n",
    "\n",
    "    beginEpoch=date(1994,1,1)\n",
    "\n",
    "    #Convert this into a data object\n",
    "    dayMinusOne=date(int(firstDataDateParts[0]),int(firstDataDateParts[1]),int(firstDataDateParts[2]))\n",
    "    dayMinusOne-=timedelta(days=1)\n",
    "    if(dayMinusOne > beginEpoch): \n",
    "        dayMinusOneString=dayMinusOne.isoformat()\n",
    "        noDataEvent['to'] = dayMinusOneString\n",
    "        noDataEvent['from'] = beginDate\n",
    "        noDataCount += 1\n",
    "\n",
    "    #Reset the \"raw\" file to the beginning\n",
    "    rawFile.seek(0)\n",
    "\n",
    "    # Step through the file to find the starting and ending dates with no data.\n",
    "    # By convention, this occurs when the line has a timestamp T22:22:22.  Also, by\n",
    "    # convention, we will record the latest to earliest dates with no data.\n",
    "\n",
    "    while True:\n",
    "        noDataEvent = {}\n",
    "        nodata=False\n",
    "        rline1=rawFile.readline()\n",
    "        if not rline1: break\n",
    "\n",
    "        # Get the date and timestamp, following format conventions\n",
    "        fulleventdate1=rline1.split(\" \")[1]\n",
    "        eventdate1=fulleventdate1.split(\"T\")[0]\n",
    "        timestamp1=fulleventdate1.split(\"T\")[1]\n",
    "\n",
    "        # See if we have detected a no-data line\n",
    "        if(timestamp1==NO_DATA_TIME_STAMP):\n",
    "            nodata=True\n",
    "            #Keep eventdate1 in case this is an isolated no-data line.\n",
    "            eventdate_keep=eventdate1\n",
    "\n",
    "            # We have a no-data line, so step ahead until the \n",
    "            # no-data line ends.\n",
    "            while(nodata):\n",
    "                rline2=rawFile.readline()\n",
    "                if not rline2: break\n",
    "                fulleventdate2=rline2.split(\" \")[1]\n",
    "                eventdate2=fulleventdate2.split(\"T\")[0]\n",
    "                timestamp2=fulleventdate2.split(\"T\")[1]\n",
    "                if(timestamp2!=NO_DATA_TIME_STAMP):\n",
    "                    # Data exists for the second time stamp, so break out\n",
    "                    # The last no-data line was the previous line\n",
    "                    nodata=False\n",
    "                    break\n",
    "                else:\n",
    "                    # No data for this line either, so keep this timestamp\n",
    "                    # and start the while(nodata) loop again\n",
    "                    eventdate_keep=eventdate2\n",
    "\n",
    "            # We now know the range of no-data values, so insert this range, latest first\n",
    "\t    noDataEvent['to'] = eventdate_keep\n",
    "\t    noDataEvent['from'] = eventdate1\n",
    "\t    noDataRanges.append(noDataEvent)\n",
    "\t    noDataCount += 1\n",
    "            \n",
    "    # Finally, prepend the data-not-yet-available date range, from the last day of data\n",
    "    # until today's date.\n",
    "    today=date.today()\n",
    "    formattedToday=today.isoformat() \n",
    "    \n",
    "    #Reread the last event\n",
    "    rawFile.seek(0)\n",
    "    lastRawLine=rawFile.readlines()[-1]\n",
    "    lastRawDate=lastRawLine.split(\" \")[1].split(\"T\")[0]\n",
    "    lastDataDateParts=lastRawDate.split(\"-\")  # This is the last date\n",
    "    #Create a new date object out of the string we get from the file.\n",
    "    lastDataDatePlus1=date(int(lastDataDateParts[0]),int(lastDataDateParts[1]),int(lastDataDateParts[2]))\n",
    "    #Now increment this date one day.\n",
    "    lastDataDatePlus1+=timedelta(days=1)    \n",
    "    #Now convert to a string\n",
    "    lastDataDataP1String=lastDataDatePlus1.isoformat()\n",
    "\n",
    "    noDataEvent = {}\n",
    "    noDataEvent['to'] = formattedToday\n",
    "    noDataEvent['from'] = lastDataDataP1String\n",
    "    noDataRanges.append(noDataEvent)\n",
    "    noDataCount += 1\n",
    "    \n",
    "    stationData['time_nodata'] = noDataRanges\n",
    "    stationData['nodata_count'] = noDataCount\n",
    "    rawFile.close\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "projectDir = FINAL_PATH +dataSet\n",
    "if(os.path.isdir(projectDir)):\n",
    "# Open the JSON file that will contain the results\n",
    "    outputPath = FINAL_PATH+dataSet + \"_FILL.json\"\n",
    "    summaryData = {}\n",
    "        \n",
    "    summaryData['update_time'] = updateTime\n",
    "    summaryData['data_source'] = dataSet\n",
    "    summaryData['begin_date'] = beginDate\n",
    "    summaryData['end_date'] = endDate\n",
    "    summaryData['center_longitude'] = centerLng\n",
    "    summaryData['center_latitude'] = centerLat\n",
    "    summaryData['server_url'] = \"http://\" + serverName + \"/daily_rdahmmexec/daily/\" + dataSet\n",
    "    summaryData['stateChangeNumTxtFile'] = stateChangeNumTxtFile\n",
    "    summaryData['stateChangeNumJsInput'] = stateChangeNumJsInput\n",
    "    summaryData['allStationInputName'] = allStationInputName\n",
    "    summaryData['Filters'] = filters\n",
    "    summaryData['video_url'] = \"\"\n",
    "\n",
    "    stations = []\n",
    "    stationCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputPath\n",
    "\n",
    "import os, errno\n",
    "\n",
    "def silentremove(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
    "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
    "            raise # re-raise exception if a different error occured\n",
    "            \n",
    "silentremove(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Filters': 'Fill_Missing',\n",
       " 'allStationInputName': 'all_stations.all.input',\n",
       " 'begin_date': '1994-01-01',\n",
       " 'center_latitude': '36.7477778',\n",
       " 'center_longitude': '-119.7713889',\n",
       " 'data_source': 'UNR_SPLICE',\n",
       " 'end_date': '2016-10-19',\n",
       " 'server_url': 'http://gf9.ucs.indiana.edu/daily_rdahmmexec/daily/UNR_SPLICE',\n",
       " 'stateChangeNumJsInput': 'stateChangeNums.txt.jsi',\n",
       " 'stateChangeNumTxtFile': 'stateChangeNums.txt',\n",
       " 'update_time': '2016-12-05T22:22:42',\n",
       " 'video_url': ''}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for stationList in os.listdir(projectDir):\n",
    "    stationPath = projectDir + \"/\" + stationList + \"/\"\n",
    "    if (os.path.isdir(stationPath)):\n",
    "        stationData = {}\n",
    "\n",
    "        setStationId(stationList, stationData)\n",
    "        setStationStartDate(stationPath, stationData)\n",
    "        setStationRefLatLonHgt(stationPath, stationData)\n",
    "        setStatusChanges(stationPath, stationData)\n",
    "        setTimesNoData(stationPath, stationData)\n",
    "\n",
    "        stations.append(stationData)\n",
    "        stationCount += 1 \n",
    "\n",
    "summaryData['stations'] = stations\n",
    "summaryData['station_count'] = stationCount\n",
    "\n",
    "# with open(outputPath, 'w') as jsonfile:\n",
    "#     jsonfile.write(json.dumps(summaryData, sort_keys=True, indent=2))\n",
    "# jsonfile.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate client side processing to store state changes in last days\n",
    "This is python code translated from JavaScript code from https://github.com/GeoGateway/geogateway-portal/blob/master/html/js/gps-tools.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global params\n",
    "gpsStationState=[\"green\",\"red\",\"yellow\",\"lightblue\",\"blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_string_to_date(string):\n",
    "#     return datetime.strptime(string, '%Y-%m-%d').now().date()\n",
    "    return datetime.strptime(string, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkDateForData(selectedDate,noDataDates):\n",
    "    dataOnDate=True;\n",
    "    #   Selected date is after the last no-data date.\n",
    "    if selectedDate > convert_string_to_date(noDataDates[len(noDataDates) -1]['to']):\n",
    "        dataOnDate=True;\n",
    "\n",
    "    # Otherwise, check each no-data interval to see if the date falls within.\n",
    "    else:\n",
    "        for elem in noDataDates:\n",
    "            startDate = convert_string_to_date(elem['from'])\n",
    "            endDate = convert_string_to_date(elem['to'])\n",
    "\n",
    "            if (startDate <= selectedDate) & (endDate >= selectedDate):\n",
    "                dataOnDate=False\n",
    "                break\n",
    "\n",
    "    return dataOnDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Would be nice to throw an exception here \n",
    "def getPrecedingStateChange(selectedDate,statusChanges):\n",
    "    \n",
    "    stateLastDate = selectedDate\n",
    "    latestPossibleDate=convert_string_to_date(statusChanges[len(statusChanges)-1]['date'])\n",
    "    earliestPossibleDate=convert_string_to_date(statusChanges[0]['date'])\n",
    "\n",
    "#     This should actually throw an erorr since there is no earlier state change.\n",
    "    if(selectedDate <= earliestPossibleDate):\n",
    "#         stateLastDate=earliestPossibleDate;\n",
    "        stateLastDate=selectedDate\n",
    "    \n",
    "    elif (selectedDate >= latestPossibleDate):\n",
    "        stateLastDate=latestPossibleDate\n",
    "        \n",
    "    else:\n",
    "        for i, e in reversed(list(enumerate(statusChanges))):\n",
    "            stateChangeDate1 = convert_string_to_date(statusChanges[i-1]['date'])\n",
    "            stateChangeDate2 = convert_string_to_date(statusChanges[i]['date'])    \n",
    "#             The last state change date to find is the one\n",
    "#             on or before the curren date.\n",
    "            if (selectedDate >= stateChangeDate1) & (selectedDate < stateChangeDate2):\n",
    "                stateLastDate=stateChangeDate1\n",
    "#                 Dates are in order, so we can stop\n",
    "                break;\n",
    "        \n",
    "#         for(var i=statusChanges.length-1; i>0; i--) {\n",
    "#             var stateChangeDate1=new Date(statusChanges[i-1].date);\n",
    "#             var stateChangeDate2=new Date(statusChanges[i].date);\n",
    "#             //The last state change date to find is the one\n",
    "#             //on or before the curren date.\n",
    "#             if(selectedDate >= stateChangeDate1 \n",
    "#                && selectedDate < stateChangeDate2) {\n",
    "#                 stateLastDate=stateChangeDate1;\n",
    "#                 //Dates are in order, so we can stop\n",
    "#                 break;\n",
    "#             }\n",
    "#         }\n",
    "    return stateLastDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getStationState(date,gpsStation):\n",
    "    #     theState=gpsStationState[0]  #This is the default.\n",
    "    theState=0\n",
    "    today=date\n",
    "    lastMonth=today - timedelta(30)\n",
    "    dayBefore=today - timedelta(1)\n",
    "\n",
    "    statusChanges=gpsStation['status_changes']\n",
    "    noDataDates=gpsStation['time_nodata']\n",
    "\n",
    "    earliestDataDate = convert_string_to_date(gpsStation['start_date'])\n",
    "    dataOnDate = checkDateForData(date,noDataDates)\n",
    "\n",
    "    #     Hopefully this big if-else construction correctly captures all the case.\n",
    "    #     It can be simplified later.\n",
    "\n",
    "    #     Provided date is before any data available for that station, so state is light blue\n",
    "    if (today < earliestDataDate):\n",
    "    #         theState=gpsStationState[3]  #light blue\n",
    "        theState=3\n",
    "    #     print (\"c1\")\n",
    "\n",
    "\n",
    "    #     Date falls within data range, there are no status changes, and data is available on date\n",
    "    elif (today >= earliestDataDate) & (len(statusChanges)==0) & (dataOnDate==True):\n",
    "    #         theState=gpsStationState[0]  # green\n",
    "        theState=0\n",
    "    #     print (\"c2\")\n",
    "\n",
    "    #     Date falls within data range, there are no state changes, and no data on selected date.\n",
    "    elif (today > earliestDataDate) & (len(statusChanges)==0) &(dataOnDate==False):\n",
    "    #         theState=gpsStationState[3]  # light blue\n",
    "        theState=3\n",
    "    #     print (\"c3\")\n",
    "\n",
    "\n",
    "    #     We have data on the date, but it proceeds the earliest state change\n",
    "    elif (today < convert_string_to_date(statusChanges[0]['date'])):\n",
    "    #         theState=gpsStationState[0]  # green\n",
    "        theState=0\n",
    "    #     print (\"c4\")\n",
    "\n",
    "    #     See if the date falls within 1 day or 1 month of a state change.\n",
    "    elif (len(statusChanges) > 0):\n",
    "        #     print (\"c5\")\n",
    "        #    Get nearest preceding state change date\n",
    "        stateLastDate=getPrecedingStateChange(date,statusChanges)\n",
    "\n",
    "    #         See if state change was yesterday.\n",
    "        if(stateLastDate > dayBefore):\n",
    "            #             theState=gpsStationState[1]  # red\n",
    "            theState=1\n",
    "\n",
    "    #         See if the station has changed state in between the\n",
    "    #         selected date and 30 days prior to the selected date.\n",
    "        elif (stateLastDate >= lastMonth) & (stateLastDate <= today):\n",
    "            #       See if we have no data within 24 hours of the selected date.            \n",
    "            if (dataOnDate == False):\n",
    "                #       Data is missing within last 24 hours and state has changed within last 30 days.\n",
    "                #                 theState=gpsStationState[4]  # blue\n",
    "                theState=4\n",
    "            else:\n",
    "                #       We have data on the date and state has changed within a 30 day window.\n",
    "                #                 theState=gpsStationState[2] # yellow\n",
    "                theState=2\n",
    "        #         No data is available on selected date for this station, and station is\n",
    "        #         not within either the 1 day or 30 day window.\n",
    "        elif(dataOnDate==False):\n",
    "            #             theState=gpsStationState[3]  # light blue\n",
    "            theState=3\n",
    "\n",
    "    return theState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE TO MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'GPS_UNR_SPLICE', u'GPS_mXn_UNR_SPLICE', u'local', u'poetry_london']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPS_UNR_SPLICE'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet = 'UNR_SPLICE'\n",
    "database_name='GPS_'+dataSet\n",
    "database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def daterange(start_date, end_date):\n",
    "# #     http://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python?answertab=votes#tab-top\n",
    "#     for n in range(int ((end_date - start_date).days)):\n",
    "#         yield start_date + timedelta(n)\n",
    "\n",
    "# start_date = datetime.strptime(beginDate, '%Y-%m-%d')\n",
    "# end_date = datetime.strptime(endDate, '%Y-%m-%d')\n",
    "\n",
    "# for date in daterange(start_date, end_date):\n",
    "#     for station in stations:\n",
    "#         break\n",
    "#         #print date.strftime('%Y-%m-%d') + \" : \" + str(getStationState(date, station))\n",
    "# #     print type(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop existing database and create new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.drop_database(database_name)\n",
    "db =client[database_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 3 collections\n",
    "# for network meta data\n",
    "collections_meta_network = db.collections_meta_network\n",
    "# for station meta data\n",
    "collections_meta_stations = db.collections_meta_stations\n",
    "# for stations\n",
    "collections_time_series_stations = db.collections_time_series_stations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network-meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x7fccc4225f00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_network= {}\n",
    "meta_network['update_time'] = summaryData['update_time']\n",
    "meta_network['data_source'] = summaryData['data_source']\n",
    "meta_network['begin_date'] = summaryData['begin_date']\n",
    "meta_network['end_date'] = summaryData['end_date']\n",
    "meta_network['center_longitude'] = summaryData['center_longitude']\n",
    "meta_network['center_latitude'] = summaryData['center_latitude']\n",
    "meta_network['server_url'] = summaryData['server_url']\n",
    "meta_network['stateChangeNumTxtFile'] = summaryData['stateChangeNumTxtFile']\n",
    "meta_network['stateChangeNumJsInput'] = summaryData['stateChangeNumJsInput']\n",
    "meta_network['allStationInputName'] = summaryData['allStationInputName']\n",
    "meta_network['Filters'] = summaryData['Filters']\n",
    "meta_network['video_url'] = summaryData['video_url']\n",
    "meta_network['station_count'] = summaryData['station_count']\n",
    "\n",
    "collections_meta_network.insert_one(meta_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stations and stations-meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_legacy_data(station):\n",
    "    \n",
    "    document= {}\n",
    "    document['_id'] = station['id']\n",
    "    document['pro_dir'] = station['pro_dir']\n",
    "    document['AFile'] = station['AFile']\n",
    "    document['BFile'] = station['BFile']\n",
    "    document['InputFile'] = station['InputFile']\n",
    "    document['RawInputFile'] = station['RawInputFile']\n",
    "    document['SwfInputFile'] = station['SwfInputFile']\n",
    "    document['DygraphsInputFile'] = station['DygraphsInputFile']\n",
    "    document['LFile'] = station['LFile']\n",
    "    document['XPngFile'] = station['XPngFile']\n",
    "    document['YPngFile'] = station['YPngFile']\n",
    "    document['ZPngFile'] = station['ZPngFile']\n",
    "    document['XTinyPngFile'] = station['XTinyPngFile']\n",
    "    document['YTinyPngFile'] = station['YTinyPngFile']\n",
    "    document['ZTinyPngFile'] = station['ZTinyPngFile']\n",
    "    document['PiFile'] = station['PiFile']\n",
    "    document['QFile'] = station['QFile']\n",
    "    document['MaxValFile'] = station['MaxValFile']\n",
    "    document['MinValFile'] = station['MinValFile']\n",
    "    document['RangeFile'] = station['RangeFile']\n",
    "    document['ModelFiles'] = station['ModelFiles']\n",
    "    document['RefFile'] = station['RefFile']\n",
    "    document['start_date'] = station['start_date']\n",
    "    document['lat'] = station['lat']\n",
    "    document['long'] = station['long']\n",
    "    document['height'] = station['height']\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date=datetime.strptime(beginDate, '%Y-%m-%d')\n",
    "end_date=datetime.strptime(endDate, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/21231789/how-to-get-all-days-in-current-month\n",
    "from calendar import monthrange\n",
    "\n",
    "# temp_station_list= []\n",
    "for station in stations:\n",
    "#     GeoJson format: maybe considered in future\n",
    "#     loc = {'type' : \"Point\", \n",
    "#            'coordinates' : [float(station['long']), float(station['lat'])]\n",
    "#           }\n",
    "    loc = [float(station['long']), float(station['lat'])]   \n",
    "    document={'station_id' : station['id'], 'loc' : loc }\n",
    "    \n",
    "    data_for_all_years = {}\n",
    "    \n",
    "    for year in range(start_date.year,end_date.year+1):\n",
    "        data_for_a_year = {}\n",
    "        \n",
    "        for month in range(1,13):\n",
    "            no_of_days_in_month = monthrange(year, month)[1]+1\n",
    "            days=range(1, no_of_days_in_month)\n",
    "#             state=[None]*(no_of_days_in_month-1)          \n",
    "            data_for_a_month={}\n",
    "            \n",
    "            for day in days:\n",
    "                date_in_time_series=datetime(year, month, day)\n",
    "                if (date_in_time_series > today):\n",
    "                    break                \n",
    "#                 state[str(day-1)]=str(getStationState(date_in_time_series, station))\n",
    "                data_for_a_month[str(day)] = str(getStationState(date_in_time_series, station))\n",
    "#             data_for_a_year[str(month)] = dict(zip(days, state))\n",
    "\n",
    "            if (date_in_time_series > today):\n",
    "                break\n",
    "            data_for_a_year[str(month)] = data_for_a_month\n",
    "            \n",
    "        if (date_in_time_series > today):\n",
    "            break            \n",
    "        data_for_all_years[str(year)] = data_for_a_year\n",
    "        \n",
    "    document['status'] = data_for_all_years\n",
    "#     set_legacy_data(document, station)\n",
    "    \n",
    "    # Add station time series\n",
    "    collections_time_series_stations.insert_one(document)\n",
    "    \n",
    "    # Add station metadata\n",
    "    collections_meta_stations.insert_one(get_legacy_data(station))\n",
    "#     temp_station_list.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFile': 'daily_project_ESM1.A',\n",
       " 'BFile': 'daily_project_ESM1.B',\n",
       " 'DygraphsInputFile': 'daily_project_ESM1_2016-10-19.dygraphs.js',\n",
       " 'InputFile': 'daily_project_ESM1_2016-10-19.all.input',\n",
       " 'LFile': 'daily_project_ESM1.L',\n",
       " 'MaxValFile': 'daily_project_ESM1.maxval',\n",
       " 'MinValFile': 'daily_project_ESM1.minval',\n",
       " 'ModelFiles': 'daily_project_ESM1.zip',\n",
       " 'PiFile': 'daily_project_ESM1.pi',\n",
       " 'QFile': 'daily_project_ESM1_2016-10-19.all.Q',\n",
       " 'RangeFile': 'daily_project_ESM1.range',\n",
       " 'RawInputFile': 'daily_project_ESM1_2016-10-19.all.raw',\n",
       " 'RefFile': 'daily_project_ESM1.input.ref',\n",
       " 'SwfInputFile': 'daily_project_ESM1_2016-10-19.plotswf.input',\n",
       " 'XPngFile': 'daily_project_ESM1_2016-10-19.all.input.X.png',\n",
       " 'XTinyPngFile': 'daily_project_ESM1_2016-10-19.all.input.X_tiny.png',\n",
       " 'YPngFile': 'daily_project_ESM1_2016-10-19.all.input.Y.png',\n",
       " 'YTinyPngFile': 'daily_project_ESM1_2016-10-19.all.input.Y_tiny.png',\n",
       " 'ZPngFile': 'daily_project_ESM1_2016-10-19.all.input.Z.png',\n",
       " 'ZTinyPngFile': 'daily_project_ESM1_2016-10-19.all.input.Z_tiny.png',\n",
       " '_id': 'ESM1',\n",
       " 'height': '19.8537558',\n",
       " 'lat': '47.803480434',\n",
       " 'long': '-122.569173848',\n",
       " 'pro_dir': 'daily_project_ESM1_2016-10-19',\n",
       " 'start_date': '2006-08-07'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_legacy_data(station)\n",
    "# document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 11, 19, 0, 0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_in_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temp_mySummary['stations'] = temp_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# outputPath = properties('eval_path') + 'my_UNR_SPLICE_FILL.json'\n",
    "# with open(outputPath, 'w') as jsonfile:\n",
    "#     jsonfile.write(json.dumps(temp_mySummary, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add station data\n",
    "# collections_stations.insert_many(temp_station_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'lon_2d'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index based on latitude, longitude\n",
    "# https://docs.mongodb.com/v3.2/tutorial/build-a-2d-index/\n",
    "# db.<collection>.createIndex( {<location field> : \"<index type>\"} ,\n",
    "#                              { bits : <bit precision> } )\n",
    "\n",
    "db.collections_time_series_stations.create_index( [(\"lon\", pymongo.GEO2D)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'update_time': u'2016-11-18T20:23:26', u'data_source': u'UNR_SPLICE', u'begin_date': u'1994-01-01', u'video_url': u'', u'end_date': u'2016-10-19', u'allStationInputName': u'all_stations.all.input', u'stateChangeNumTxtFile': u'stateChangeNums.txt', u'center_latitude': u'36.7477778', u'center_longitude': u'-119.7713889', 'network_station_count': 5, u'server_url': u'http://gf9.ucs.indiana.edu/daily_rdahmmexec/daily/UNR_SPLICE', u'Filters': u'Fill_Missing', u'stateChangeNumJsInput': u'stateChangeNums.txt.jsi'}\n"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "cursor_meta_network = db.collections_meta_network.find()\n",
    "\n",
    "for i in cursor_meta_network:\n",
    "    i.pop('_id', None)\n",
    "    i['network_station_count'] = i['station_count']\n",
    "    i.pop('station_count', None)\n",
    "    output = i\n",
    "    \n",
    "    \n",
    "print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2016-10-19'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "\tfor i in db.collections_meta_network.find():\n",
    "\t    end_date = i['end_date']\n",
    "except:\n",
    "\tend_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'update_time': u'2016-11-18T20:23:26', u'data_source': u'UNR_SPLICE', u'begin_date': u'1994-01-01', u'video_url': u'', u'end_date': u'2016-10-19', u'allStationInputName': u'all_stations.all.input', u'stateChangeNumTxtFile': u'stateChangeNums.txt', u'center_latitude': u'36.7477778', u'center_longitude': u'-119.7713889', u'server_url': u'http://gf9.ucs.indiana.edu/daily_rdahmmexec/daily/UNR_SPLICE', u'Filters': u'Fill_Missing', u'stateChangeNumJsInput': u'stateChangeNums.txt.jsi', u'_id': ObjectId('582fa98e8bf03635c0a72319'), u'station_count': 5}\n"
     ]
    }
   ],
   "source": [
    "for i in db.collections_meta_network.find():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for 1 station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "station_id_to_find = 'ESM1'\n",
    "\n",
    "output = {}\n",
    "cursor_meta_stations = db.collections_meta_stations.find( \\\n",
    "                                        { '_id': station_id_to_find })\n",
    "\n",
    "print cursor_meta_stations.count()\n",
    "\n",
    "for i in cursor_meta_stations:\n",
    "    output=i \n",
    "    \n",
    "dygraph_file_path = os.path.join(properties('eval_path'),dataSet, \\\n",
    "                    'daily_project_'+ station_id_to_find+'_'+ end_date)\n",
    "\n",
    "for file in os.listdir(dygraph_file_path):\n",
    "    if file.endswith(\".js\"):\n",
    "        with open(os.path.join(dygraph_file_path, file), 'r') as js_file:\n",
    "            dygraphs=js_file.readlines()\n",
    "#         with open(\"Output.js\", \"w\") as text_file:\n",
    "#             text_file.write(\"{}\".format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.cursor.Cursor at 0x7fccc41b36d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor_meta_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Hack to get around frontend bug \n",
    " JavaScript at fronend is unable to process the long string for dygraph. It somehow drops some of the text. So dividing the text into parts and sending them to frontend, then joining these strings to form a single string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['function data_east() {\\nreturn \"\" +\"Data, State 1, State 2, State 3, State 4, State 5\\\\n\" +\\n\"2012-05-01,,,,878.679,\\\\n 2012-05-02,,,,880.106,\\\\n 2012-05-03,,,,879.209,\\\\n 2012-05-04,,,,879.965,\\\\n 2012-05-05,,,,877.72,\\\\n 2012-05-06,,,,878.799,\\\\n 2012-05-07,,,,878.887,\\\\n 2012-05-08,,,,877.939,\\\\n 2012-05-09,,,,877.2,\\\\n 2012-05-10,,,,880.904,\\\\n 2012-05-11,,,,879.845,\\\\n 2012-05-12,,,,879.144,\\\\n 2012-05-13,,,,876.518,\\\\n 2012-05-14,,,,879.723,\\\\n 2012-05-15,,,,880.709,\\\\n 2012-05-16,,,,879.56,\\\\n 2012-05-17,,,,879.738,\\\\n 2012-05-18,,,,879.823,\\\\n 2012-05-20,,,,881.584,\\\\n 2012-05-21,,,,879.25,\\\\n 2012-05-22,,,,881.016,\\\\n 2012-05-23,,,,877.879,\\\\n 2012-05-24,,,,879.476,\\\\n 2012-05-25,,,,877.718,\\\\n 2012-05-26,,,,877.177,\\\\n 2012-05-27,,,,879.349,\\\\n 2012-05-28,,,,880.003,\\\\n 2012-05-29,,,,877.251,\\\\n 2012-05-30,,,,878.248,\\\\n 2012-05-31,,,,878.429,\\\\n 2012-06-01,,,,878.607,\\\\n 2012-06-02,,,,877.559,\\\\n 2012-06-03,,,,877.88,\\\\n 2012-06-04,,,,880.88,\\\\n 2012-06-05,,,,878.877,\\\\n 2012-06-06,,,,876.429,\\\\n 2012-06-07,,,,877.314,\\\\n 2012-06-08,,,,877.485,\\\\n 2012-06-09,,,,877.57,\\\\n 2012-06-10,,,,878.817,\\\\n 2012-06-11,,,,877.997,\\\\n 2012-06-12,,,,877.688,\\\\n 2012-06-13,,,,878.631,\\\\n 2012-06-15,,,,875.071,\\\\n 2012-06-16,,,,879.27,\\\\n 2012-06-17,,,,874.43,\\\\n 2012-06-18,,,,874.825,\\\\n 2012-06-19,,,,878.074,\\\\n 2012-06-20,,,,877.699,\\\\n 2012-06-21,,,,877.397,\\\\n 2012-06-22,,,,878.36,\\\\n 2012-06-23,,,,880.307,\\\\n 2012-06-24,,,,880.016,\\\\n 2012-06-25,,,,877.434,\\\\n 2012-06-26,,,,878.848,\\\\n 2012-06-27,,,,875.767,\\\\n 2012-06-28,,,,878.124,\\\\n 2012-06-29,,,,876.716,\\\\n 2012-06-30,,,,876.832,\\\\n 2012-07-01,,,,876.649,\\\\n 2012-07-02,,,,875.486,\\\\n 2012-07-06,,,,876.104,\\\\n 2012-07-07,,,,876.516,\\\\n 2012-07-08,,,,876.676,\\\\n 2012-07-09,,,,880.724,\\\\n 2012-07-10,,,,878.218,\\\\n 2012-07-11,,,,876.045,\\\\n 2012-07-12,,,,877.607,\\\\n 2012-07-13,,,,880.349,\\\\n 2012-07-14,,,,872.914,\\\\n 2012-07-15,,,,876.83,\\\\n 2012-07-16,,,,877.961,\\\\n 2012-07-21,,,,880.049,\\\\n 2012-07-22,,,,876.844,\\\\n \"',\n",
       " '\\nfunction data_east_disp() {\\nreturn \"\" +\"Data, State 1, State 2, State 3, State 4, State 5\\\\n\" +\\n\"2012-05-01,,,,878.679,\\\\n 2012-05-02,,,,880.106,\\\\n 2012-05-03,,,,879.209,\\\\n 2012-05-04,,,,879.965,\\\\n 2012-05-05,,,,877.72,\\\\n 2012-05-06,,,,878.799,\\\\n 2012-05-07,,,,878.887,\\\\n 2012-05-08,,,,877.939,\\\\n 2012-05-09,,,,877.2,\\\\n 2012-05-10,,,,880.904,\\\\n 2012-05-11,,,,879.845,\\\\n 2012-05-12,,,,879.144,\\\\n 2012-05-13,,,,876.518,\\\\n 2012-05-14,,,,879.723,\\\\n 2012-05-15,,,,880.709,\\\\n 2012-05-16,,,,879.56,\\\\n 2012-05-17,,,,879.738,\\\\n 2012-05-18,,,,879.823,\\\\n 2012-05-20,,,,881.584,\\\\n 2012-05-21,,,,879.25,\\\\n 2012-05-22,,,,881.016,\\\\n 2012-05-23,,,,877.879,\\\\n 2012-05-24,,,,879.476,\\\\n 2012-05-25,,,,877.718,\\\\n 2012-05-26,,,,877.177,\\\\n 2012-05-27,,,,879.349,\\\\n 2012-05-28,,,,880.003,\\\\n 2012-05-29,,,,877.251,\\\\n 2012-05-30,,,,878.248,\\\\n 2012-05-31,,,,878.429,\\\\n 2012-06-01,,,,878.607,\\\\n 2012-06-02,,,,877.559,\\\\n 2012-06-03,,,,877.88,\\\\n 2012-06-04,,,,880.88,\\\\n 2012-06-05,,,,878.877,\\\\n 2012-06-06,,,,876.429,\\\\n 2012-06-07,,,,877.314,\\\\n 2012-06-08,,,,877.485,\\\\n 2012-06-09,,,,877.57,\\\\n 2012-06-10,,,,878.817,\\\\n 2012-06-11,,,,877.997,\\\\n 2012-06-12,,,,877.688,\\\\n 2012-06-13,,,,878.631,\\\\n 2012-06-15,,,,875.071,\\\\n 2012-06-16,,,,879.27,\\\\n 2012-06-17,,,,874.43,\\\\n 2012-06-18,,,,874.825,\\\\n 2012-06-19,,,,878.074,\\\\n 2012-06-20,,,,877.699,\\\\n 2012-06-21,,,,877.397,\\\\n 2012-06-22,,,,878.36,\\\\n 2012-06-23,,,,880.307,\\\\n 2012-06-24,,,,880.016,\\\\n 2012-06-25,,,,877.434,\\\\n 2012-06-26,,,,878.848,\\\\n 2012-06-27,,,,875.767,\\\\n 2012-06-28,,,,878.124,\\\\n 2012-06-29,,,,876.716,\\\\n 2012-06-30,,,,876.832,\\\\n 2012-07-01,,,,876.649,\\\\n 2012-07-02,,,,875.486,\\\\n 2012-07-06,,,,876.104,\\\\n 2012-07-07,,,,876.516,\\\\n 2012-07-08,,,,876.676,\\\\n 2012-07-09,,,,880.724,\\\\n 2012-07-10,,,,878.218,\\\\n 2012-07-11,,,,876.045,\\\\n 2012-07-12,,,,877.607,\\\\n 2012-07-13,,,,880.349,\\\\n 2012-07-14,,,,872.914,\\\\n 2012-07-15,,,,876.83,\\\\n 2012-07-16,,,,877.961,\\\\n 2012-07-21,,,,880.049,\\\\n 2012-07-22,,,,876.844,\\\\n \"',\n",
       " '\\nfunction data_north() {\\nreturn \"\" +\"Data, State 1, State 2, State 3, State 4, State 5\\\\n\" +\\n\"2012-05-01,,,,984.991,\\\\n 2012-05-02,,,,986.158,\\\\n 2012-05-03,,,,986.225,\\\\n 2012-05-04,,,,985.961,\\\\n 2012-05-05,,,,987.185,\\\\n 2012-05-06,,,,986.786,\\\\n 2012-05-07,,,,985.578,\\\\n 2012-05-08,,,,988.813,\\\\n 2012-05-09,,,,987.98,\\\\n 2012-05-10,,,,988.195,\\\\n 2012-05-11,,,,987.653,\\\\n 2012-05-12,,,,986.722,\\\\n 2012-05-13,,,,991.988,\\\\n 2012-05-14,,,,988.591,\\\\n 2012-05-15,,,,990.998,\\\\n 2012-05-16,,,,987.761,\\\\n 2012-05-17,,,,988.466,\\\\n 2012-05-18,,,,988.609,\\\\n 2012-05-20,,,,988.664,\\\\n 2012-05-21,,,,987.799,\\\\n 2012-05-22,,,,984.171,\\\\n 2012-05-23,,,,988.884,\\\\n 2012-05-24,,,,985.738,\\\\n 2012-05-25,,,,985.529,\\\\n 2012-05-26,,,,985.548,\\\\n 2012-05-27,,,,988.078,\\\\n 2012-05-28,,,,988.788,\\\\n 2012-05-29,,,,987.819,\\\\n 2012-05-30,,,,985.943,\\\\n 2012-05-31,,,,989.62,\\\\n 2012-06-01,,,,989.255,\\\\n 2012-06-02,,,,987.104,\\\\n 2012-06-03,,,,988.208,\\\\n 2012-06-04,,,,987.599,\\\\n 2012-06-05,,,,989.376,\\\\n 2012-06-06,,,,989.398,\\\\n 2012-06-07,,,,986.505,\\\\n 2012-06-08,,,,985.042,\\\\n 2012-06-09,,,,987.807,\\\\n 2012-06-10,,,,986.154,\\\\n 2012-06-11,,,,989.284,\\\\n 2012-06-12,,,,988.095,\\\\n 2012-06-13,,,,986.486,\\\\n 2012-06-15,,,,986.188,\\\\n 2012-06-16,,,,988.114,\\\\n 2012-06-17,,,,984.549,\\\\n 2012-06-18,,,,985.293,\\\\n 2012-06-19,,,,985.462,\\\\n 2012-06-20,,,,987.49,\\\\n 2012-06-21,,,,985.275,\\\\n 2012-06-22,,,,985.785,\\\\n 2012-06-23,,,,990,\\\\n 2012-06-24,,,,990.033,\\\\n 2012-06-25,,,,985.794,\\\\n 2012-06-26,,,,986.656,\\\\n 2012-06-27,,,,987.085,\\\\n 2012-06-28,,,,989.018,\\\\n 2012-06-29,,,,986.536,\\\\n 2012-06-30,,,,987.358,\\\\n 2012-07-01,,,,985.066,\\\\n 2012-07-02,,,,986.789,\\\\n 2012-07-06,,,,988.988,\\\\n 2012-07-07,,,,987.769,\\\\n 2012-07-08,,,,986.583,\\\\n 2012-07-09,,,,984.33,\\\\n 2012-07-10,,,,985.214,\\\\n 2012-07-11,,,,983.884,\\\\n 2012-07-12,,,,983.434,\\\\n 2012-07-13,,,,984.097,\\\\n 2012-07-14,,,,984.536,\\\\n 2012-07-15,,,,984.75,\\\\n 2012-07-16,,,,986.675,\\\\n 2012-07-21,,,,983.05,\\\\n 2012-07-22,,,,984.144,\\\\n \"',\n",
       " '\\nfunction data_north_disp() {\\nreturn \"\" +\"Data, State 1, State 2, State 3, State 4, State 5\\\\n\" +\\n\"2012-05-01,,,,984.991,\\\\n 2012-05-02,,,,986.158,\\\\n 2012-05-03,,,,986.225,\\\\n 2012-05-04,,,,985.961,\\\\n 2012-05-05,,,,987.185,\\\\n 2012-05-06,,,,986.786,\\\\n 2012-05-07,,,,985.578,\\\\n 2012-05-08,,,,988.813,\\\\n 2012-05-09,,,,987.98,\\\\n 2012-05-10,,,,988.195,\\\\n 2012-05-11,,,,987.653,\\\\n 2012-05-12,,,,986.722,\\\\n 2012-05-13,,,,991.988,\\\\n 2012-05-14,,,,988.591,\\\\n 2012-05-15,,,,990.998,\\\\n 2012-05-16,,,,987.761,\\\\n 2012-05-17,,,,988.466,\\\\n 2012-05-18,,,,988.609,\\\\n 2012-05-20,,,,988.664,\\\\n 2012-05-21,,,,987.799,\\\\n 2012-05-22,,,,984.171,\\\\n 2012-05-23,,,,988.884,\\\\n 2012-05-24,,,,985.738,\\\\n 2012-05-25,,,,985.529,\\\\n 2012-05-26,,,,985.548,\\\\n 2012-05-27,,,,988.078,\\\\n 2012-05-28,,,,988.788,\\\\n 2012-05-29,,,,987.819,\\\\n 2012-05-30,,,,985.943,\\\\n 2012-05-31,,,,989.62,\\\\n 2012-06-01,,,,989.255,\\\\n 2012-06-02,,,,987.104,\\\\n 2012-06-03,,,,988.208,\\\\n 2012-06-04,,,,987.599,\\\\n 2012-06-05,,,,989.376,\\\\n 2012-06-06,,,,989.398,\\\\n 2012-06-07,,,,986.505,\\\\n 2012-06-08,,,,985.042,\\\\n 2012-06-09,,,,987.807,\\\\n 2012-06-10,,,,986.154,\\\\n 2012-06-11,,,,989.284,\\\\n 2012-06-12,,,,988.095,\\\\n 2012-06-13,,,,986.486,\\\\n 2012-06-15,,,,986.188,\\\\n 2012-06-16,,,,988.114,\\\\n 2012-06-17,,,,984.549,\\\\n 2012-06-18,,,,985.293,\\\\n 2012-06-19,,,,985.462,\\\\n 2012-06-20,,,,987.49,\\\\n 2012-06-21,,,,985.275,\\\\n 2012-06-22,,,,985.785,\\\\n 2012-06-23,,,,990,\\\\n 2012-06-24,,,,990.033,\\\\n 2012-06-25,,,,985.794,\\\\n 2012-06-26,,,,986.656,\\\\n 2012-06-27,,,,987.085,\\\\n 2012-06-28,,,,989.018,\\\\n 2012-06-29,,,,986.536,\\\\n 2012-06-30,,,,987.358,\\\\n 2012-07-01,,,,985.066,\\\\n 2012-07-02,,,,986.789,\\\\n 2012-07-06,,,,988.988,\\\\n 2012-07-07,,,,987.769,\\\\n 2012-07-08,,,,986.583,\\\\n 2012-07-09,,,,984.33,\\\\n 2012-07-10,,,,985.214,\\\\n 2012-07-11,,,,983.884,\\\\n 2012-07-12,,,,983.434,\\\\n 2012-07-13,,,,984.097,\\\\n 2012-07-14,,,,984.536,\\\\n 2012-07-15,,,,984.75,\\\\n 2012-07-16,,,,986.675,\\\\n 2012-07-21,,,,983.05,\\\\n 2012-07-22,,,,984.144,\\\\n \"',\n",
       " '\\nfunction data_up() {\\nreturn \"\" +\"Data, State 1, State 2, State 3, State 4, State 5\\\\n\" +\\n\"2012-05-01,,,,767.858,\\\\n 2012-05-02,,,,764.923,\\\\n 2012-05-03,,,,769.308,\\\\n 2012-05-04,,,,766.977,\\\\n 2012-05-05,,,,765.882,\\\\n 2012-05-06,,,,762.023,\\\\n 2012-05-07,,,,770.115,\\\\n 2012-05-08,,,,772.125,\\\\n 2012-05-09,,,,769.671,\\\\n 2012-05-10,,,,763.894,\\\\n 2012-05-11,,,,762.774,\\\\n 2012-05-12,,,,765.308,\\\\n 2012-05-13,,,,776.957,\\\\n 2012-05-14,,,,769.502,\\\\n 2012-05-15,,,,761.454,\\\\n 2012-05-16,,,,768.44,\\\\n 2012-05-17,,,,771.208,\\\\n 2012-05-18,,,,768.984,\\\\n 2012-05-20,,,,776.004,\\\\n 2012-05-21,,,,773.976,\\\\n 2012-05-22,,,,767.469,\\\\n 2012-05-23,,,,762.533,\\\\n 2012-05-24,,,,772.639,\\\\n 2012-05-25,,,,765.641,\\\\n 2012-05-26,,,,762.195,\\\\n 2012-05-27,,,,762.834,\\\\n 2012-05-28,,,,769.153,\\\\n 2012-05-29,,,,761.614,\\\\n 2012-05-30,,,,773.227,\\\\n 2012-05-31,,,,775.92,\\\\n 2012-06-01,,,,767.637,\\\\n 2012-06-02,,,,772.246,\\\\n 2012-06-03,,,,764.129,\\\\n 2012-06-04,,,,768.689,\\\\n 2012-06-05,,,,771.02,\\\\n 2012-06-06,,,,764.449,\\\\n 2012-06-07,,,,764.665,\\\\n 2012-06-08,,,,763.768,\\\\n 2012-06-09,,,,767.163,\\\\n 2012-06-10,,,,763.756,\\\\n 2012-06-11,,,,765.461,\\\\n 2012-06-12,,,,768.864,\\\\n 2012-06-13,,,,771.424,\\\\n 2012-06-15,,,,770.966,\\\\n 2012-06-16,,,,767.62,\\\\n 2012-06-17,,,,777.472,\\\\n 2012-06-18,,,,772.088,\\\\n 2012-06-19,,,,769.663,\\\\n 2012-06-20,,,,767.148,\\\\n 2012-06-21,,,,768.062,\\\\n 2012-06-22,,,,772.4,\\\\n 2012-06-23,,,,769.466,\\\\n 2012-06-24,,,,763.258,\\\\n 2012-06-25,,,,765.459,\\\\n 2012-06-26,,,,771.283,\\\\n 2012-06-27,,,,763.122,\\\\n 2012-06-28,,,,760.976,\\\\n 2012-06-29,,,,770.053,\\\\n 2012-06-30,,,,775.634,\\\\n 2012-07-01,,,,774.195,\\\\n 2012-07-02,,,,767.178,\\\\n 2012-07-06,,,,763.804,\\\\n 2012-07-07,,,,760.169,\\\\n 2012-07-08,,,,760.316,\\\\n 2012-07-09,,,,780.663,\\\\n 2012-07-10,,,,771.709,\\\\n 2012-07-11,,,,768.942,\\\\n 2012-07-12,,,,766.211,\\\\n 2012-07-13,,,,767.696,\\\\n 2012-07-14,,,,782.584,\\\\n 2012-07-15,,,,777.721,\\\\n 2012-07-16,,,,765.779,\\\\n 2012-07-21,,,,773.39,\\\\n 2012-07-22,,,,766.178,\\\\n \"',\n",
       " '\\nfunction data_up_disp() {\\nreturn \"\" +\"Data, State 1, State 2, State 3, State 4, State 5\\\\n\" +\\n\"2012-05-01,,,,767.858,\\\\n 2012-05-02,,,,764.923,\\\\n 2012-05-03,,,,769.308,\\\\n 2012-05-04,,,,766.977,\\\\n 2012-05-05,,,,765.882,\\\\n 2012-05-06,,,,762.023,\\\\n 2012-05-07,,,,770.115,\\\\n 2012-05-08,,,,772.125,\\\\n 2012-05-09,,,,769.671,\\\\n 2012-05-10,,,,763.894,\\\\n 2012-05-11,,,,762.774,\\\\n 2012-05-12,,,,765.308,\\\\n 2012-05-13,,,,776.957,\\\\n 2012-05-14,,,,769.502,\\\\n 2012-05-15,,,,761.454,\\\\n 2012-05-16,,,,768.44,\\\\n 2012-05-17,,,,771.208,\\\\n 2012-05-18,,,,768.984,\\\\n 2012-05-20,,,,776.004,\\\\n 2012-05-21,,,,773.976,\\\\n 2012-05-22,,,,767.469,\\\\n 2012-05-23,,,,762.533,\\\\n 2012-05-24,,,,772.639,\\\\n 2012-05-25,,,,765.641,\\\\n 2012-05-26,,,,762.195,\\\\n 2012-05-27,,,,762.834,\\\\n 2012-05-28,,,,769.153,\\\\n 2012-05-29,,,,761.614,\\\\n 2012-05-30,,,,773.227,\\\\n 2012-05-31,,,,775.92,\\\\n 2012-06-01,,,,767.637,\\\\n 2012-06-02,,,,772.246,\\\\n 2012-06-03,,,,764.129,\\\\n 2012-06-04,,,,768.689,\\\\n 2012-06-05,,,,771.02,\\\\n 2012-06-06,,,,764.449,\\\\n 2012-06-07,,,,764.665,\\\\n 2012-06-08,,,,763.768,\\\\n 2012-06-09,,,,767.163,\\\\n 2012-06-10,,,,763.756,\\\\n 2012-06-11,,,,765.461,\\\\n 2012-06-12,,,,768.864,\\\\n 2012-06-13,,,,771.424,\\\\n 2012-06-15,,,,770.966,\\\\n 2012-06-16,,,,767.62,\\\\n 2012-06-17,,,,777.472,\\\\n 2012-06-18,,,,772.088,\\\\n 2012-06-19,,,,769.663,\\\\n 2012-06-20,,,,767.148,\\\\n 2012-06-21,,,,768.062,\\\\n 2012-06-22,,,,772.4,\\\\n 2012-06-23,,,,769.466,\\\\n 2012-06-24,,,,763.258,\\\\n 2012-06-25,,,,765.459,\\\\n 2012-06-26,,,,771.283,\\\\n 2012-06-27,,,,763.122,\\\\n 2012-06-28,,,,760.976,\\\\n 2012-06-29,,,,770.053,\\\\n 2012-06-30,,,,775.634,\\\\n 2012-07-01,,,,774.195,\\\\n 2012-07-02,,,,767.178,\\\\n 2012-07-06,,,,763.804,\\\\n 2012-07-07,,,,760.169,\\\\n 2012-07-08,,,,760.316,\\\\n 2012-07-09,,,,780.663,\\\\n 2012-07-10,,,,771.709,\\\\n 2012-07-11,,,,768.942,\\\\n 2012-07-12,,,,766.211,\\\\n 2012-07-13,,,,767.696,\\\\n 2012-07-14,,,,782.584,\\\\n 2012-07-15,,,,777.721,\\\\n 2012-07-16,,,,765.779,\\\\n 2012-07-21,,,,773.39,\\\\n 2012-07-22,,,,766.178,\\\\n \"',\n",
       " '\\n']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_list = dygraphs.split('}')\n",
    "output['d_0'] = dy_list[0]\n",
    "output['d_1'] = dy_list[1]\n",
    "output['d_2'] = dy_list[2]\n",
    "output['d_3'] = dy_list[3]\n",
    "output['d_4'] = dy_list[4]\n",
    "output['d_5'] = dy_list[5]\n",
    "\n",
    "dy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get status of stations within bounding box for particular date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_min = -89.998914447\n",
    "lat_max = 83.64323665\n",
    "lon_min = -359.998835383\n",
    "lon_max = -0.037155605\n",
    "\n",
    "year = '2013'\n",
    "month = '12'\n",
    "day = '31'\n",
    "\n",
    "# cursor_stations = collections_stations.find( {\"lat\" : {\"$gte\" : lat_min, \"$lte\" : lat_max}, \\\n",
    "#                             \"long\" : {\"$gte\" : long_min, \"$lte\" : long_max}})\n",
    "\n",
    "# db.<collection>.find( { loc: [ <x> , <y> ] } )\n",
    "\n",
    "# cursor_stations = collections_stations.find({\"loc\" : [long_min, lat_min]})\n",
    "\n",
    "# db.places.find( { loc :\n",
    "#                   { $geoWithin :\n",
    "#                      { $box : [ [ 0 , 0 ] ,\n",
    "#                                 [ 100 , 100 ] ]\n",
    "#                  } } } )\n",
    "\n",
    "status_to_find = 'status.' + year + '.' + month + '.' + day\n",
    "cursor_stations = db.collections_time_series_stations.find({ \"loc\" : \\\n",
    "                                                  { \"$geoWithin\" : \n",
    "                                                    { \"$box\" : [ [lon_min, lat_min],\\\n",
    "                                                               [lon_max, lat_max]] \\\n",
    "                                                    } }, \\\n",
    "                                                 status_to_find : {'$exists': 1} \\\n",
    "                                                }, {'station_id': 1, 'loc': 1, status_to_find:1})\n",
    "# http://stackoverflow.com/questions/20662691/how-to-search-through-a-mongodb-collection-for-dictionary-keys-nested-in-array\n",
    "# http://stackoverflow.com/questions/5301795/mongodb-get-specific-part-of-document\n",
    "\n",
    "list_station = []\n",
    "if cursor_stations:\n",
    "    for station in cursor_stations:\n",
    "#         station.pop('_id', None)\n",
    "#         list_station.append(station)\n",
    "        try:\n",
    "            res = {'station_id' : station['station_id'],\n",
    "                   'status' : station['status'][year][month][day],\n",
    "                   'lat' : station['loc'][1],\n",
    "                   'lon' : station['loc'][0]\n",
    "                  }\n",
    "            list_station.append(res)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lat': -12.466640097,\n",
       "  'lon': -229.156012745,\n",
       "  'station_id': u'00NA',\n",
       "  'status': u'3'},\n",
       " {'lat': -12.478223997,\n",
       "  'lon': -229.017953041,\n",
       "  'station_id': u'01NA',\n",
       "  'status': u'3'},\n",
       " {'lat': 30.40742467,\n",
       "  'lon': -91.180261768,\n",
       "  'station_id': u'1LSU',\n",
       "  'status': u'0'},\n",
       " {'lat': 31.75080049,\n",
       "  'lon': -93.097603682,\n",
       "  'station_id': u'1NSU',\n",
       "  'status': u'0'},\n",
       " {'lat': 47.803480434,\n",
       "  'lon': -122.569173848,\n",
       "  'station_id': u'ESM1',\n",
       "  'status': u'3'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONGODB mXn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSet = 'UNR_SPLICE'\n",
    "database_name='GPS_mXn_'+dataSet\n",
    "database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create database\n",
    "client.drop_database(database_name)\n",
    "db =client[database_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 collections\n",
    "# for stations\n",
    "collections_stations = db.collections_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_date=datetime.strptime(beginDate, '%Y-%m-%d')\n",
    "end_date=datetime.strptime(endDate, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "today = datetime.today()\n",
    "for station in stations:\n",
    "#     GeoJson format: maybe considered in future\n",
    "#     loc = {'type' : \"Point\", \n",
    "#            'coordinates' : [float(station['long']), float(station['lat'])]\n",
    "#           }\n",
    "    loc = [float(station['long']), float(station['lat'])]   \n",
    "    document={'station_id' : station['id'], 'loc' : loc }\n",
    "        \n",
    "    for year in range(start_date.year,end_date.year+1):\n",
    "        \n",
    "        for month in range(1,13):\n",
    "            no_of_days_in_month = monthrange(year, month)[1]+1\n",
    "            days=range(1, no_of_days_in_month)\n",
    "\n",
    "            for day in days:\n",
    "                date_in_time_series=datetime(year, month, day)\n",
    "                if (date_in_time_series > today):\n",
    "                    break\n",
    "                \n",
    "                doc = document.copy()\n",
    "                doc['status'] = str(getStationState(date_in_time_series, station))             \n",
    "                doc['date'] = date_in_time_series\n",
    "                \n",
    "                # Add station data\n",
    "                data.append(doc)\n",
    "                collections_stations.insert_one(doc)\n",
    "            \n",
    "            if (date_in_time_series > today):\n",
    "                break\n",
    "                \n",
    "        if (date_in_time_series > today):\n",
    "            break            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_in_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.collections_stations.create_index([(\"loc\", pymongo.GEO2D), \\\n",
    "                                        (\"date\", pymongo.ASCENDING)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive data from mXn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get status of stations within bounding box for particular date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_min = -89.998914447\n",
    "lat_max = 83.64323665\n",
    "lon_min = -359.998835383\n",
    "lon_max = -0.037155605\n",
    "\n",
    "year = '2013'\n",
    "month = '12'\n",
    "day = '12'\n",
    "\n",
    "date_to_find_in_str = year + '-' + month + '-' + day\n",
    "date_to_find = datetime.strptime(date_to_find_in_str,'%Y-%m-%d')\n",
    "\n",
    "cursor_stations = db.collections_stations.find({ \"loc\" : \\\n",
    "                                                  { \"$geoWithin\" : \n",
    "                                                    { \"$box\" : [ [lon_min, lat_min],\\\n",
    "                                                               [lon_max, lat_max]] \\\n",
    "                                                    } }, \\\n",
    "                                                 \"date\" : {'$eq': date_to_find} \\\n",
    "                                                }, {'station_id': 1, 'status':1})\n",
    "\n",
    "list_station = []\n",
    "if cursor_stations:\n",
    "    for record in cursor_stations:\n",
    "#         print record\n",
    "        record.pop('_id', None)\n",
    "        list_station.append(record)\n",
    "#         try:\n",
    "#             res = {'id' : station['id'],\n",
    "#                    'status' : station['status'][year][month][day]\n",
    "#                   }\n",
    "#             list_station.append(record)\n",
    "#         except:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# properties('eval_path')\n",
    "\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path to json\n",
    "dataset = 'UNR_SPLICE'\n",
    "json_file_name = dataset + '_FILL.json'\n",
    "json_file = properties('eval_path') + json_file_name \n",
    "json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the json\n",
    "import json\n",
    "\n",
    "with open(json_file) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    \n",
    "# http://stackoverflow.com/questions/5316720/python-how-to-convert-a-list-of-dictionaries-values-into-int-float-from-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert lat & long to float\n",
    "# [dict([a, int(x)] for a, x in b.iteritems()) for b in list]\n",
    "for dictionary in data['stations']:\n",
    "    dictionary['lat'] = float(dictionary['lat'])\n",
    "    dictionary['long'] = float(dictionary['long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "database_name=dataset+'_timeseries_database'\n",
    "database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete existing database\n",
    "client.drop_database(database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new database\n",
    "db = client.gps_timeseries_database\n",
    "db =client[database_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 collections\n",
    "# for meta\n",
    "collections_meta = db.collections_meta\n",
    "# for stations\n",
    "collections_stations = db.collections_stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta= {}\n",
    "meta['update_time'] = data['update_time']\n",
    "meta['data_source'] = data['data_source']\n",
    "meta['begin_date'] = data['begin_date']\n",
    "meta['end_date'] = data['end_date']\n",
    "meta['center_longitude'] = data['center_longitude']\n",
    "meta['center_latitude'] = data['center_latitude']\n",
    "meta['server_url'] = data['server_url']\n",
    "meta['stateChangeNumTxtFile'] = data['stateChangeNumTxtFile']\n",
    "meta['stateChangeNumJsInput'] = data['stateChangeNumJsInput']\n",
    "meta['allStationInputName'] = data['allStationInputName']\n",
    "meta['Filters'] = data['Filters']\n",
    "meta['video_url'] = data['video_url']\n",
    "meta['station_count'] = data['station_count']\n",
    "collections_meta.insert_one(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add station data\n",
    "collections_stations.insert_many(data['stations'])\n",
    "\n",
    "# Create index based on latitude, longitude\n",
    "db.collections_stations.create_index( [(\"lat\", pymongo.ASCENDING), \\\n",
    "                                     (\"lon\", pymongo.ASCENDING)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query to get network information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "cursor_meta = collections_meta.find()\n",
    "\n",
    "for i in cursor_meta:\n",
    "    i.pop('_id', None)\n",
    "    output = i;\n",
    "    \n",
    "    \n",
    "print output\n",
    "#     i.pop('_id', None)\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get status of stations within bounding box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# {\"age\" : {\"$gte\" : 18, \"$lte\" : 30}}\n",
    "# cursor = collections_stations.find( {\"lat\" : {\"$gte\" : 30, \"$lte\" : 60}, \\\n",
    "#                                      \"long\" : {\"$gte\" : -170, \"$lte\" : -80}})\n",
    "\n",
    "lat_min = 30\n",
    "lat_max = 60\n",
    "long_min = -170\n",
    "long_max = -80\n",
    "cursor_stations = collections_stations.find( {\"lat\" : {\"$gte\" : lat_min, \"$lte\" : lat_max}, \\\n",
    "                            \"long\" : {\"$gte\" : long_min, \"$lte\" : long_max}})\n",
    "\n",
    "if cursor_stations:\n",
    "    list_station = []\n",
    "    for i in cursor_stations:\n",
    "        i.pop('_id', None)\n",
    "        list_station.append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(list_station)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_res.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print str(df_stations[\"lat\"].min()) + \" : \" + str(df_stations[\"lat\"].max())\n",
    "print str(df_stations[\"long\"].min()) + \" : \" + str(df_stations[\"long\"].max())\n",
    "\n",
    "\n",
    "# -89.998914447 : 83.64323665\n",
    "# -359.998835383 : -0.037155605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputPath = os.path.join(os.getcwd(), \"test_output.json\")\n",
    "outputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(outputPath, 'w') as jsonfile:\n",
    "    jsonfile.write(json.dumps(output, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfile.close()\n",
    "client.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# client.drop_database(database_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
