{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All paths as in properties.py but with changed UNIX style paths for the code to execute across platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "V={}\n",
    "WORK_DIR = '/media/hru/Data/_Active_Projects/PythonRDAHMM/test'\n",
    "V['cron_path']=os.path.join(WORK_DIR, \"RDAHMM\",\"CRON_Download/\")  \n",
    "V['download_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Download/\")  \n",
    "V['script_path']=os.path.join(WORK_DIR,\"PythonRDAHMM/\") \n",
    "V['data_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Data/\")\n",
    "# temp_path is the temporary working directory for ingesting raw data\n",
    "V['temp_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"TEMP/\")\n",
    "V['model_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Model/\")\n",
    "V['eval_path']=os.path.join(WORK_DIR,\"daily/single/\")\n",
    "V['train_epoch']=\"2013-12-31\"\n",
    "V['rdahmm_bin']=os.path.join(WORK_DIR,\"RDAHMM\", \"rdahmm3\",\"bin\", \"rdahmm\")\n",
    "V['rdahmm_model_parm']=\"-data <inputFile> -T <dataCount> -D <dimensionCount> -N 5 -output_type gauss -anneal -annealfactor 1.1 -betamin 0.1 -regularize -omega 0 0 1 1.0e-6 -ntries 10 -seed 1234\"\n",
    "V['rdahmm_eval_parm']=\"-data <proBaseName>.all.input -T <dataCount> -D <dimensionCount> -N 5 -output_type gauss -A <modelBaseName>.A -B <modelBaseName>.B -pi <modelBaseName>.pi -minvalfile <modelBaseName>.minval -maxvalfile <modelBaseName>.maxval -rangefile <modelBaseName>.range -eval\"\n",
    "V['dygraphsJs']=os.path.join(WORK_DIR,\"PythonRDAHMM\",\"dygraphsJsCreator.pearl\")\n",
    "\n",
    "def properties(key):\n",
    "    return V[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the json file for Presentation layer\n",
    "This is the existing code obtained from PythonRDAHMM/create_summary_jsons.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, string, re, json\n",
    "from datetime import date, datetime, timedelta, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# command line input argument\n",
    "# dataSet = \"rawNeuTimeSeries.MEASURES_Combination\"\n",
    "dataSet = \"UNR_SPLICE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to set <b>endDate = '2016-10-19'</b> as the database was downloaded from gf9 that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some useful global constants\n",
    "today = datetime.today()\n",
    "serverName = \"gf9.ucs.indiana.edu\"\n",
    "updateTime = str(today.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "beginDate = \"1994-01-01\"\n",
    "# endDate = str(today.strftime(\"%Y-%m-%d\"))\n",
    "endDate = '2016-10-19'\n",
    "centerLng = \"-119.7713889\"\n",
    "centerLat = \"36.7477778\"\n",
    "stateChangeNumTxtFile = \"stateChangeNums.txt\"\n",
    "stateChangeNumJsInput = \"stateChangeNums.txt.jsi\"\n",
    "allStationInputName = \"all_stations.all.input\"\n",
    "filters = \"Fill_Missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used to separate parts of the station name\n",
    "SEPARATOR_CHARACTER=\"_\"\n",
    "NO_DATA_TIME_STAMP=\"22:22:22\"\n",
    "FINAL_PATH=properties('eval_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationId(stationList, stationData):\n",
    "    #Get the station name.\n",
    "    stationName=stationList.split(SEPARATOR_CHARACTER)[2];\n",
    "\n",
    "    stationData['id'] = stationName\n",
    "    stationData['pro_dir'] = \"daily_project_\" + stationName + \"_\" + endDate\n",
    "    stationData['AFile'] = \"daily_project_\" + stationName + \".A\"\n",
    "    stationData['BFile'] = \"daily_project_\" + stationName + \".B\"\n",
    "    stationData['InputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input\"\n",
    "    stationData['RawInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.raw\"\n",
    "    stationData['SwfInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".plotswf.input\"\n",
    "    stationData['DygraphsInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".dygraphs.js\"\n",
    "    stationData['LFile'] = \"daily_project_\" + stationName + \".L\"\n",
    "    stationData['XPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.X.png\"\n",
    "    stationData['YPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Y.png\"\n",
    "    stationData['ZPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Z.png\"\n",
    "    stationData['XTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.X_tiny.png\"\n",
    "    stationData['YTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Y_tiny.png\"\n",
    "    stationData['ZTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Z_tiny.png\"\n",
    "    stationData['PiFile'] = \"daily_project_\" + stationName + \".pi\"\n",
    "    stationData['QFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.Q\"\n",
    "    stationData['MaxValFile'] = \"daily_project_\" + stationName + \".maxval\"\n",
    "    stationData['MinValFile'] = \"daily_project_\" + stationName + \".minval\"\n",
    "    stationData['RangeFile'] = \"daily_project_\" + stationName + \".range\"\n",
    "    stationData['ModelFiles'] = \"daily_project_\" + stationName + \".zip\"\n",
    "    stationData['RefFile'] = \"daily_project_\" + stationName + \".input.ref\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationStartDate(stationDir, stationData):\n",
    "    startFileName = stationDir + \"daily_project_\" + stationData['id'] + \".input.starttime\"\n",
    "    if (os.path.isfile(startFileName)):\n",
    "        with open(startFileName,\"r\") as startFile:\n",
    "            startDate = startFile.readline().rstrip()\n",
    "        startFile.close()\n",
    "    else:\n",
    "\t\tstartDate = \"1994-01-01\"\n",
    "    stationData['start_date'] = startDate\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationRefLatLonHgt(stationDir, stationData):\n",
    "    refFileName = stationDir + stationData['RefFile']\n",
    "    refLat=\"\"\n",
    "    refLon=\"\"\n",
    "    refHgt=\"\"\n",
    "    if (os.path.isfile(refFileName)):\n",
    "        with open(refFileName,\"r\") as refFile:\n",
    "            refParts=refFile.readline().split(\" \")\n",
    "            refLat=refParts[0]\n",
    "            refLon=refParts[1]\n",
    "            refHgt=refParts[2].rstrip() # Have to chomp off the final \\n\n",
    "        refFile.close()\n",
    "    else:\n",
    "        refLat=\"1.0\"\n",
    "        refLon=\"2.0\"\n",
    "        refHgt=\"-1.0\"\n",
    "\n",
    "    stationData['lat'] = refLat      \n",
    "    stationData['long'] = refLon      \n",
    "    stationData['height'] = refHgt\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStatusChanges(stationDir, stationData):\n",
    "    # Open the .all.Q and the .all.raw files.  We get the state from the first and\n",
    "    # the data from the second. \n",
    "    # TODO: for now, we assume these files always exist\n",
    "    qFileName = stationDir + stationData['QFile']\n",
    "    rawFileName = stationDir + stationData['RawInputFile']\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Bail out if the required files don't exist   \n",
    "    if((not os.path.isfile(qFileName)) or (not os.path.isfile(rawFileName))): \n",
    "        return \n",
    "    qFile = open(qFileName,\"r\")\n",
    "    rawFile = open(rawFileName,\"r\")\n",
    "\n",
    "    stateChanges = []\n",
    "    changeCount = 0\n",
    "    # Now step through the Q file looking for state changes\n",
    "    # If we find a state change, get the date from the raw file\n",
    "    # We will save these to the string stateChangeArray since we\n",
    "    # need to record in latest-first order\n",
    "    qline1 = qFile.readline()\n",
    "    rline1 = rawFile.readline()        \n",
    "    while True:\n",
    "        eventData = {}\n",
    "        qline2 = qFile.readline()\n",
    "        rline2 = rawFile.readline()\n",
    "        if not qline2: break\n",
    "        \n",
    "        # See if qline1 and qline2 are the same.  If so, extract the dates from rline1 and rline2\n",
    "        # The line splits below are specific to the raw file line format.\n",
    "        if (qline1.rstrip() != qline2.rstrip()):\n",
    "            eventdate = rline2.split(\" \")[1] \n",
    "            eventdate = eventdate.split(\"T\")[0]\n",
    "            oldstate = qline1.rstrip()\n",
    "            newstate = qline2.rstrip()\n",
    "            eventData['date'] = eventdate\n",
    "            eventData['from'] = oldstate\n",
    "            eventData['to'] = newstate \n",
    "            stateChanges.append(eventData)\n",
    "            changeCount += 1\n",
    "\n",
    "        # Make the previous \"next\" lines the \"first\" lines for the next comparison\n",
    "        qline1=qline2\n",
    "        rline1=rline2\n",
    "\n",
    "    stationData['status_changes'] = stateChanges\n",
    "    stationData['change_count'] = changeCount\n",
    "\n",
    "    # Clean up\n",
    "    qFile.close\n",
    "    rawFile.close\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setTimesNoData(stationDir, stationData):\n",
    "    rawFileName = stationDir + stationData['RawInputFile']\n",
    "\n",
    "    # Required file doesn't exist so bail out\n",
    "    if(not os.path.isfile(rawFileName)): return\n",
    "    rawFile = open(rawFileName, \"r\")\n",
    "    \n",
    "    noDataRanges = []\n",
    "    noDataCount = 0\n",
    "    noDataEvent = {}\n",
    "\n",
    "    # We need to set a no-data range from beginDate (for the epoch, 1994-01-01) to the day before\n",
    "    # our first data point for this station.  If the station has data before 1994-01-01, then \n",
    "    # ignore.\n",
    "    firstDataDateParts=rawFile.readline().split(\" \")[1].split(\"T\")[0].split(\"-\");\n",
    "\n",
    "    beginEpoch=date(1994,1,1)\n",
    "\n",
    "    #Convert this into a data object\n",
    "    dayMinusOne=date(int(firstDataDateParts[0]),int(firstDataDateParts[1]),int(firstDataDateParts[2]))\n",
    "    dayMinusOne-=timedelta(days=1)\n",
    "    if(dayMinusOne > beginEpoch): \n",
    "        dayMinusOneString=dayMinusOne.isoformat()\n",
    "        noDataEvent['to'] = dayMinusOneString\n",
    "        noDataEvent['from'] = beginDate\n",
    "        noDataCount += 1\n",
    "\n",
    "    #Reset the \"raw\" file to the beginning\n",
    "    rawFile.seek(0)\n",
    "\n",
    "    # Step through the file to find the starting and ending dates with no data.\n",
    "    # By convention, this occurs when the line has a timestamp T22:22:22.  Also, by\n",
    "    # convention, we will record the latest to earliest dates with no data.\n",
    "\n",
    "    while True:\n",
    "        noDataEvent = {}\n",
    "        nodata=False\n",
    "        rline1=rawFile.readline()\n",
    "        if not rline1: break\n",
    "\n",
    "        # Get the date and timestamp, following format conventions\n",
    "        fulleventdate1=rline1.split(\" \")[1]\n",
    "        eventdate1=fulleventdate1.split(\"T\")[0]\n",
    "        timestamp1=fulleventdate1.split(\"T\")[1]\n",
    "\n",
    "        # See if we have detected a no-data line\n",
    "        if(timestamp1==NO_DATA_TIME_STAMP):\n",
    "            nodata=True\n",
    "            #Keep eventdate1 in case this is an isolated no-data line.\n",
    "            eventdate_keep=eventdate1\n",
    "\n",
    "            # We have a no-data line, so step ahead until the \n",
    "            # no-data line ends.\n",
    "            while(nodata):\n",
    "                rline2=rawFile.readline()\n",
    "                if not rline2: break\n",
    "                fulleventdate2=rline2.split(\" \")[1]\n",
    "                eventdate2=fulleventdate2.split(\"T\")[0]\n",
    "                timestamp2=fulleventdate2.split(\"T\")[1]\n",
    "                if(timestamp2!=NO_DATA_TIME_STAMP):\n",
    "                    # Data exists for the second time stamp, so break out\n",
    "                    # The last no-data line was the previous line\n",
    "                    nodata=False\n",
    "                    break\n",
    "                else:\n",
    "                    # No data for this line either, so keep this timestamp\n",
    "                    # and start the while(nodata) loop again\n",
    "                    eventdate_keep=eventdate2\n",
    "\n",
    "            # We now know the range of no-data values, so insert this range, latest first\n",
    "\t    noDataEvent['to'] = eventdate_keep\n",
    "\t    noDataEvent['from'] = eventdate1\n",
    "\t    noDataRanges.append(noDataEvent)\n",
    "\t    noDataCount += 1\n",
    "            \n",
    "    # Finally, prepend the data-not-yet-available date range, from the last day of data\n",
    "    # until today's date.\n",
    "    today=date.today()\n",
    "    formattedToday=today.isoformat() \n",
    "    \n",
    "    #Reread the last event\n",
    "    rawFile.seek(0)\n",
    "    lastRawLine=rawFile.readlines()[-1]\n",
    "    lastRawDate=lastRawLine.split(\" \")[1].split(\"T\")[0]\n",
    "    lastDataDateParts=lastRawDate.split(\"-\")  # This is the last date\n",
    "    #Create a new date object out of the string we get from the file.\n",
    "    lastDataDatePlus1=date(int(lastDataDateParts[0]),int(lastDataDateParts[1]),int(lastDataDateParts[2]))\n",
    "    #Now increment this date one day.\n",
    "    lastDataDatePlus1+=timedelta(days=1)    \n",
    "    #Now convert to a string\n",
    "    lastDataDataP1String=lastDataDatePlus1.isoformat()\n",
    "\n",
    "    noDataEvent = {}\n",
    "    noDataEvent['to'] = formattedToday\n",
    "    noDataEvent['from'] = lastDataDataP1String\n",
    "    noDataRanges.append(noDataEvent)\n",
    "    noDataCount += 1\n",
    "    \n",
    "    stationData['time_nodata'] = noDataRanges\n",
    "    stationData['nodata_count'] = noDataCount\n",
    "    rawFile.close\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "projectDir = FINAL_PATH +dataSet\n",
    "if(os.path.isdir(projectDir)):\n",
    "# Open the JSON file that will contain the results\n",
    "    outputPath = FINAL_PATH+dataSet + \"_FILL.json\"\n",
    "    summaryData = {}\n",
    "        \n",
    "    summaryData['update_time'] = updateTime\n",
    "    summaryData['data_source'] = dataSet\n",
    "    summaryData['begin_date'] = beginDate\n",
    "    summaryData['end_date'] = endDate\n",
    "    summaryData['center_longitude'] = centerLng\n",
    "    summaryData['center_latitude'] = centerLat\n",
    "    summaryData['server_url'] = \"http://\" + serverName + \"/daily_rdahmmexec/daily/\" + dataSet\n",
    "    summaryData['stateChangeNumTxtFile'] = stateChangeNumTxtFile\n",
    "    summaryData['stateChangeNumJsInput'] = stateChangeNumJsInput\n",
    "    summaryData['allStationInputName'] = allStationInputName\n",
    "    summaryData['Filters'] = filters\n",
    "    summaryData['video_url'] = \"\"\n",
    "\n",
    "    stations = []\n",
    "    stationCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, errno\n",
    "\n",
    "def silentremove(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
    "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
    "            raise # re-raise exception if a different error occured\n",
    "            \n",
    "silentremove(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Filters': 'Fill_Missing',\n",
       " 'allStationInputName': 'all_stations.all.input',\n",
       " 'begin_date': '1994-01-01',\n",
       " 'center_latitude': '36.7477778',\n",
       " 'center_longitude': '-119.7713889',\n",
       " 'data_source': 'UNR_SPLICE',\n",
       " 'end_date': '2016-10-19',\n",
       " 'server_url': 'http://gf9.ucs.indiana.edu/daily_rdahmmexec/daily/UNR_SPLICE',\n",
       " 'stateChangeNumJsInput': 'stateChangeNums.txt.jsi',\n",
       " 'stateChangeNumTxtFile': 'stateChangeNums.txt',\n",
       " 'update_time': '2016-12-27T19:12:50',\n",
       " 'video_url': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for stationList in os.listdir(projectDir):\n",
    "    stationPath = projectDir + \"/\" + stationList + \"/\"\n",
    "    if (os.path.isdir(stationPath)):\n",
    "        stationData = {}\n",
    "\n",
    "        setStationId(stationList, stationData)\n",
    "        setStationStartDate(stationPath, stationData)\n",
    "        setStationRefLatLonHgt(stationPath, stationData)\n",
    "        setStatusChanges(stationPath, stationData)\n",
    "        setTimesNoData(stationPath, stationData)\n",
    "\n",
    "        stations.append(stationData)\n",
    "        stationCount += 1 \n",
    "\n",
    "summaryData['stations'] = stations\n",
    "summaryData['station_count'] = stationCount\n",
    "\n",
    "# with open(outputPath, 'w') as jsonfile:\n",
    "#     jsonfile.write(json.dumps(summaryData, sort_keys=True, indent=2))\n",
    "# jsonfile.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate client side processing to store state changes in last days\n",
    "This is python code translated from JavaScript code from https://github.com/GeoGateway/geogateway-portal/blob/master/html/js/gps-tools.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global params\n",
    "gpsStationState=[\"green\",\"red\",\"yellow\",\"lightblue\",\"blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_string_to_date(string):\n",
    "#     return datetime.strptime(string, '%Y-%m-%d').now().date()\n",
    "    return datetime.strptime(string, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkDateForData(selectedDate,noDataDates):\n",
    "    dataOnDate=True;\n",
    "    #   Selected date is after the last no-data date.\n",
    "    if selectedDate > convert_string_to_date(noDataDates[len(noDataDates) -1]['to']):\n",
    "        dataOnDate=True;\n",
    "\n",
    "    # Otherwise, check each no-data interval to see if the date falls within.\n",
    "    else:\n",
    "        for elem in noDataDates:\n",
    "            startDate = convert_string_to_date(elem['from'])\n",
    "            endDate = convert_string_to_date(elem['to'])\n",
    "\n",
    "            if (startDate <= selectedDate) & (endDate >= selectedDate):\n",
    "                dataOnDate=False\n",
    "                break\n",
    "\n",
    "    return dataOnDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Would be nice to throw an exception here \n",
    "def getPrecedingStateChange(selectedDate,statusChanges):\n",
    "    \n",
    "    stateLastDate = selectedDate\n",
    "    latestPossibleDate=convert_string_to_date(statusChanges[len(statusChanges)-1]['date'])\n",
    "    earliestPossibleDate=convert_string_to_date(statusChanges[0]['date'])\n",
    "\n",
    "#     This should actually throw an erorr since there is no earlier state change.\n",
    "    if(selectedDate <= earliestPossibleDate):\n",
    "#         stateLastDate=earliestPossibleDate;\n",
    "        stateLastDate=selectedDate\n",
    "    \n",
    "    elif (selectedDate >= latestPossibleDate):\n",
    "        stateLastDate=latestPossibleDate\n",
    "        \n",
    "    else:\n",
    "        for i, e in reversed(list(enumerate(statusChanges))):\n",
    "            stateChangeDate1 = convert_string_to_date(statusChanges[i-1]['date'])\n",
    "            stateChangeDate2 = convert_string_to_date(statusChanges[i]['date'])    \n",
    "#             The last state change date to find is the one\n",
    "#             on or before the curren date.\n",
    "            if (selectedDate >= stateChangeDate1) & (selectedDate < stateChangeDate2):\n",
    "                stateLastDate=stateChangeDate1\n",
    "#                 Dates are in order, so we can stop\n",
    "                break;\n",
    "        \n",
    "#         for(var i=statusChanges.length-1; i>0; i--) {\n",
    "#             var stateChangeDate1=new Date(statusChanges[i-1].date);\n",
    "#             var stateChangeDate2=new Date(statusChanges[i].date);\n",
    "#             //The last state change date to find is the one\n",
    "#             //on or before the curren date.\n",
    "#             if(selectedDate >= stateChangeDate1 \n",
    "#                && selectedDate < stateChangeDate2) {\n",
    "#                 stateLastDate=stateChangeDate1;\n",
    "#                 //Dates are in order, so we can stop\n",
    "#                 break;\n",
    "#             }\n",
    "#         }\n",
    "    return stateLastDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getStationState(date,gpsStation):\n",
    "    #     theState=gpsStationState[0]  #This is the default.\n",
    "    theState=0\n",
    "    today=date\n",
    "    lastMonth=today - timedelta(30)\n",
    "    dayBefore=today - timedelta(1)\n",
    "\n",
    "    statusChanges=gpsStation['status_changes']\n",
    "    noDataDates=gpsStation['time_nodata']\n",
    "\n",
    "    earliestDataDate = convert_string_to_date(gpsStation['start_date'])\n",
    "    dataOnDate = checkDateForData(date,noDataDates)\n",
    "\n",
    "    #     Hopefully this big if-else construction correctly captures all the case.\n",
    "    #     It can be simplified later.\n",
    "\n",
    "    #     Provided date is before any data available for that station, so state is light blue\n",
    "    if (today < earliestDataDate):\n",
    "    #         theState=gpsStationState[3]  #light blue\n",
    "        theState=3\n",
    "    #     print (\"c1\")\n",
    "\n",
    "\n",
    "    #     Date falls within data range, there are no status changes, and data is available on date\n",
    "    elif (today >= earliestDataDate) & (len(statusChanges)==0) & (dataOnDate==True):\n",
    "    #         theState=gpsStationState[0]  # green\n",
    "        theState=0\n",
    "    #     print (\"c2\")\n",
    "\n",
    "    #     Date falls within data range, there are no state changes, and no data on selected date.\n",
    "    elif (today > earliestDataDate) & (len(statusChanges)==0) &(dataOnDate==False):\n",
    "    #         theState=gpsStationState[3]  # light blue\n",
    "        theState=3\n",
    "    #     print (\"c3\")\n",
    "\n",
    "\n",
    "    #     We have data on the date, but it proceeds the earliest state change\n",
    "    elif (today < convert_string_to_date(statusChanges[0]['date'])):\n",
    "    #         theState=gpsStationState[0]  # green\n",
    "        theState=0\n",
    "    #     print (\"c4\")\n",
    "\n",
    "    #     See if the date falls within 1 day or 1 month of a state change.\n",
    "    elif (len(statusChanges) > 0):\n",
    "        #     print (\"c5\")\n",
    "        #    Get nearest preceding state change date\n",
    "        stateLastDate=getPrecedingStateChange(date,statusChanges)\n",
    "\n",
    "    #         See if state change was yesterday.\n",
    "        if(stateLastDate > dayBefore):\n",
    "            #             theState=gpsStationState[1]  # red\n",
    "            theState=1\n",
    "\n",
    "    #         See if the station has changed state in between the\n",
    "    #         selected date and 30 days prior to the selected date.\n",
    "        elif (stateLastDate >= lastMonth) & (stateLastDate <= today):\n",
    "            #       See if we have no data within 24 hours of the selected date.            \n",
    "            if (dataOnDate == False):\n",
    "                #       Data is missing within last 24 hours and state has changed within last 30 days.\n",
    "                #                 theState=gpsStationState[4]  # blue\n",
    "                theState=4\n",
    "            else:\n",
    "                #       We have data on the date and state has changed within a 30 day window.\n",
    "                #                 theState=gpsStationState[2] # yellow\n",
    "                theState=2\n",
    "        #         No data is available on selected date for this station, and station is\n",
    "        #         not within either the 1 day or 30 day window.\n",
    "        elif(dataOnDate==False):\n",
    "            #             theState=gpsStationState[3]  # light blue\n",
    "            theState=3\n",
    "\n",
    "    return theState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE TO MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'GPS_mXn_UNR_SPLICE', u'local', u'poetry_london']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPS_UNR_SPLICE'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet = 'UNR_SPLICE'\n",
    "database_name='GPS_'+dataSet\n",
    "database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def daterange(start_date, end_date):\n",
    "# #     http://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python?answertab=votes#tab-top\n",
    "#     for n in range(int ((end_date - start_date).days)):\n",
    "#         yield start_date + timedelta(n)\n",
    "\n",
    "# start_date = datetime.strptime(beginDate, '%Y-%m-%d')\n",
    "# end_date = datetime.strptime(endDate, '%Y-%m-%d')\n",
    "\n",
    "# for date in daterange(start_date, end_date):\n",
    "#     for station in stations:\n",
    "#         break\n",
    "#         #print date.strftime('%Y-%m-%d') + \" : \" + str(getStationState(date, station))\n",
    "# #     print type(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop existing database and create new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.drop_database(database_name)\n",
    "db =client[database_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 3 collections\n",
    "# for network meta data\n",
    "collections_meta_network = db.collections_meta_network\n",
    "# for station meta data\n",
    "collections_meta_stations = db.collections_meta_stations\n",
    "# for stations\n",
    "collections_time_series_stations = db.collections_time_series_stations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network-meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x7f1924057dc0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_network= {}\n",
    "meta_network['update_time'] = summaryData['update_time']\n",
    "meta_network['data_source'] = summaryData['data_source']\n",
    "meta_network['begin_date'] = summaryData['begin_date']\n",
    "meta_network['end_date'] = summaryData['end_date']\n",
    "meta_network['center_longitude'] = summaryData['center_longitude']\n",
    "meta_network['center_latitude'] = summaryData['center_latitude']\n",
    "meta_network['server_url'] = summaryData['server_url']\n",
    "meta_network['stateChangeNumTxtFile'] = summaryData['stateChangeNumTxtFile']\n",
    "meta_network['stateChangeNumJsInput'] = summaryData['stateChangeNumJsInput']\n",
    "meta_network['allStationInputName'] = summaryData['allStationInputName']\n",
    "meta_network['Filters'] = summaryData['Filters']\n",
    "meta_network['video_url'] = summaryData['video_url']\n",
    "meta_network['station_count'] = summaryData['station_count']\n",
    "\n",
    "collections_meta_network.insert_one(meta_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stations and stations-meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_legacy_data(station):\n",
    "    \n",
    "    document= {}\n",
    "    document['_id'] = station['id']\n",
    "    document['pro_dir'] = station['pro_dir']\n",
    "    document['AFile'] = station['AFile']\n",
    "    document['BFile'] = station['BFile']\n",
    "    document['InputFile'] = station['InputFile']\n",
    "    document['RawInputFile'] = station['RawInputFile']\n",
    "    document['SwfInputFile'] = station['SwfInputFile']\n",
    "    document['DygraphsInputFile'] = station['DygraphsInputFile']\n",
    "    document['LFile'] = station['LFile']\n",
    "    document['XPngFile'] = station['XPngFile']\n",
    "    document['YPngFile'] = station['YPngFile']\n",
    "    document['ZPngFile'] = station['ZPngFile']\n",
    "    document['XTinyPngFile'] = station['XTinyPngFile']\n",
    "    document['YTinyPngFile'] = station['YTinyPngFile']\n",
    "    document['ZTinyPngFile'] = station['ZTinyPngFile']\n",
    "    document['PiFile'] = station['PiFile']\n",
    "    document['QFile'] = station['QFile']\n",
    "    document['MaxValFile'] = station['MaxValFile']\n",
    "    document['MinValFile'] = station['MinValFile']\n",
    "    document['RangeFile'] = station['RangeFile']\n",
    "    document['ModelFiles'] = station['ModelFiles']\n",
    "    document['RefFile'] = station['RefFile']\n",
    "    document['start_date'] = station['start_date']\n",
    "    document['lat'] = station['lat']\n",
    "    document['long'] = station['long']\n",
    "    document['height'] = station['height']\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date=datetime.strptime(beginDate, '%Y-%m-%d')\n",
    "end_date=datetime.strptime(endDate, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/21231789/how-to-get-all-days-in-current-month\n",
    "from calendar import monthrange\n",
    "\n",
    "# temp_station_list= []\n",
    "for station in stations:\n",
    "#     GeoJson format: maybe considered in future\n",
    "#     loc = {'type' : \"Point\", \n",
    "#            'coordinates' : [float(station['long']), float(station['lat'])]\n",
    "#           }\n",
    "    loc = [float(station['long']), float(station['lat'])]   \n",
    "    document={'station_id' : station['id'], 'loc' : loc }\n",
    "    \n",
    "    data_for_all_years = {}\n",
    "    \n",
    "    for year in range(start_date.year,end_date.year+1):\n",
    "        data_for_a_year = {}\n",
    "        \n",
    "        for month in range(1,13):\n",
    "            no_of_days_in_month = monthrange(year, month)[1]+1\n",
    "            days=range(1, no_of_days_in_month)\n",
    "#             state=[None]*(no_of_days_in_month-1)          \n",
    "            data_for_a_month={}\n",
    "            \n",
    "            for day in days:\n",
    "                date_in_time_series=datetime(year, month, day)\n",
    "                \n",
    "                if (date_in_time_series >= end_date):\n",
    "                    data_for_a_year[str(month)] = data_for_a_month\n",
    "                    data_for_all_years[str(year)] = data_for_a_year\n",
    "                    break                \n",
    "#                 state[str(day-1)]=str(getStationState(date_in_time_series, station))\n",
    "                data_for_a_month[str(day)] = str(getStationState(date_in_time_series, station))\n",
    "#             data_for_a_year[str(month)] = dict(zip(days, state))\n",
    "\n",
    "            if (date_in_time_series >= end_date):\n",
    "                break\n",
    "            data_for_a_year[str(month)] = data_for_a_month\n",
    "            \n",
    "        if (date_in_time_series >= end_date):\n",
    "            break            \n",
    "        data_for_all_years[str(year)] = data_for_a_year\n",
    "        \n",
    "    document['status'] = data_for_all_years\n",
    "#     set_legacy_data(document, station)\n",
    "    \n",
    "    # Add station time series\n",
    "    collections_time_series_stations.insert_one(document)\n",
    "    \n",
    "    # Add station metadata\n",
    "    collections_meta_stations.insert_one(get_legacy_data(station))\n",
    "#     temp_station_list.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFile': 'daily_project_ESM1.A',\n",
       " 'BFile': 'daily_project_ESM1.B',\n",
       " 'DygraphsInputFile': 'daily_project_ESM1_2016-10-19.dygraphs.js',\n",
       " 'InputFile': 'daily_project_ESM1_2016-10-19.all.input',\n",
       " 'LFile': 'daily_project_ESM1.L',\n",
       " 'MaxValFile': 'daily_project_ESM1.maxval',\n",
       " 'MinValFile': 'daily_project_ESM1.minval',\n",
       " 'ModelFiles': 'daily_project_ESM1.zip',\n",
       " 'PiFile': 'daily_project_ESM1.pi',\n",
       " 'QFile': 'daily_project_ESM1_2016-10-19.all.Q',\n",
       " 'RangeFile': 'daily_project_ESM1.range',\n",
       " 'RawInputFile': 'daily_project_ESM1_2016-10-19.all.raw',\n",
       " 'RefFile': 'daily_project_ESM1.input.ref',\n",
       " 'SwfInputFile': 'daily_project_ESM1_2016-10-19.plotswf.input',\n",
       " 'XPngFile': 'daily_project_ESM1_2016-10-19.all.input.X.png',\n",
       " 'XTinyPngFile': 'daily_project_ESM1_2016-10-19.all.input.X_tiny.png',\n",
       " 'YPngFile': 'daily_project_ESM1_2016-10-19.all.input.Y.png',\n",
       " 'YTinyPngFile': 'daily_project_ESM1_2016-10-19.all.input.Y_tiny.png',\n",
       " 'ZPngFile': 'daily_project_ESM1_2016-10-19.all.input.Z.png',\n",
       " 'ZTinyPngFile': 'daily_project_ESM1_2016-10-19.all.input.Z_tiny.png',\n",
       " '_id': 'ESM1',\n",
       " 'height': '19.8537558',\n",
       " 'lat': '47.803480434',\n",
       " 'long': '-122.569173848',\n",
       " 'pro_dir': 'daily_project_ESM1_2016-10-19',\n",
       " 'start_date': '2006-08-07'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_legacy_data(station)\n",
    "# document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 10, 20, 0, 0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_in_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temp_mySummary['stations'] = temp_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# outputPath = properties('eval_path') + 'my_UNR_SPLICE_FILL.json'\n",
    "# with open(outputPath, 'w') as jsonfile:\n",
    "#     jsonfile.write(json.dumps(temp_mySummary, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add station data\n",
    "# collections_stations.insert_many(temp_station_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'lon_2d'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index based on latitude, longitude\n",
    "# https://docs.mongodb.com/v3.2/tutorial/build-a-2d-index/\n",
    "# db.<collection>.createIndex( {<location field> : \"<index type>\"} ,\n",
    "#                              { bits : <bit precision> } )\n",
    "\n",
    "db.collections_time_series_stations.create_index( [(\"lon\", pymongo.GEO2D)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'update_time': u'2016-12-27T19:12:50', u'data_source': u'UNR_SPLICE', u'begin_date': u'1994-01-01', u'video_url': u'', u'end_date': u'2016-10-19', u'allStationInputName': u'all_stations.all.input', u'stateChangeNumTxtFile': u'stateChangeNums.txt', u'center_latitude': u'36.7477778', u'center_longitude': u'-119.7713889', 'network_station_count': 5, u'server_url': u'http://gf9.ucs.indiana.edu/daily_rdahmmexec/daily/UNR_SPLICE', u'Filters': u'Fill_Missing', u'stateChangeNumJsInput': u'stateChangeNums.txt.jsi'}\n"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "cursor_meta_network = db.collections_meta_network.find()\n",
    "\n",
    "for i in cursor_meta_network:\n",
    "    i.pop('_id', None)\n",
    "    i['network_station_count'] = i['station_count']\n",
    "    i.pop('station_count', None)\n",
    "    output = i\n",
    "    \n",
    "    \n",
    "print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2016-10-19'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "\tfor i in db.collections_meta_network.find():\n",
    "\t    end_date = i['end_date']\n",
    "except:\n",
    "\tend_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'update_time': u'2016-12-27T19:12:50', u'data_source': u'UNR_SPLICE', u'begin_date': u'1994-01-01', u'video_url': u'', u'end_date': u'2016-10-19', u'allStationInputName': u'all_stations.all.input', u'stateChangeNumTxtFile': u'stateChangeNums.txt', u'center_latitude': u'36.7477778', u'center_longitude': u'-119.7713889', u'server_url': u'http://gf9.ucs.indiana.edu/daily_rdahmmexec/daily/UNR_SPLICE', u'Filters': u'Fill_Missing', u'stateChangeNumJsInput': u'stateChangeNums.txt.jsi', u'_id': ObjectId('586303848bf0362de64900fe'), u'station_count': 5}\n"
     ]
    }
   ],
   "source": [
    "for i in db.collections_meta_network.find():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for 1 station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "station_id_to_find = 'ESM1'\n",
    "\n",
    "output = {}\n",
    "cursor_meta_stations = db.collections_meta_stations.find( \\\n",
    "                                        { '_id': station_id_to_find })\n",
    "\n",
    "print cursor_meta_stations.count()\n",
    "\n",
    "for i in cursor_meta_stations:\n",
    "    output=i \n",
    "    \n",
    "dygraph_file_path = os.path.join(properties('eval_path'),dataSet, \\\n",
    "                    'daily_project_'+ station_id_to_find+'_'+ end_date)\n",
    "\n",
    "for file in os.listdir(dygraph_file_path):\n",
    "    if file.endswith(\".js\"):\n",
    "        with open(os.path.join(dygraph_file_path, file), 'r') as js_file:\n",
    "            dygraphs=js_file.readlines()\n",
    "#         with open(\"Output.js\", \"w\") as text_file:\n",
    "#             text_file.write(\"{}\".format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.cursor.Cursor at 0x7f190eda12d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor_meta_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Hack to get around frontend bug \n",
    " JavaScript at fronend is unable to process the long string for dygraph. It somehow drops some of the text. So dividing the text into parts and sending them to frontend, then joining these strings to form a single string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-6c70e4b23e75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdy_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdygraphs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd_0'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdy_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdy_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd_2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdy_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'd_3'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdy_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "dy_list = dygraphs.split('}')\n",
    "output['d_0'] = dy_list[0]\n",
    "output['d_1'] = dy_list[1]\n",
    "output['d_2'] = dy_list[2]\n",
    "output['d_3'] = dy_list[3]\n",
    "output['d_4'] = dy_list[4]\n",
    "output['d_5'] = dy_list[5]\n",
    "\n",
    "dy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get status of stations within bounding box for particular date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_min = -89.998914447\n",
    "lat_max = 83.64323665\n",
    "lon_min = -359.998835383\n",
    "lon_max = -0.037155605\n",
    "\n",
    "year = '2016'\n",
    "month = '10'\n",
    "day = '2'\n",
    "\n",
    "# cursor_stations = collections_stations.find( {\"lat\" : {\"$gte\" : lat_min, \"$lte\" : lat_max}, \\\n",
    "#                             \"long\" : {\"$gte\" : long_min, \"$lte\" : long_max}})\n",
    "\n",
    "# db.<collection>.find( { loc: [ <x> , <y> ] } )\n",
    "\n",
    "# cursor_stations = collections_stations.find({\"loc\" : [long_min, lat_min]})\n",
    "\n",
    "# db.places.find( { loc :\n",
    "#                   { $geoWithin :\n",
    "#                      { $box : [ [ 0 , 0 ] ,\n",
    "#                                 [ 100 , 100 ] ]\n",
    "#                  } } } )\n",
    "\n",
    "status_to_find = 'status.' + year + '.' + month + '.' + day\n",
    "cursor_stations = db.collections_time_series_stations.find({ \"loc\" : \\\n",
    "                                                  { \"$geoWithin\" : \n",
    "                                                    { \"$box\" : [ [lon_min, lat_min],\\\n",
    "                                                               [lon_max, lat_max]] \\\n",
    "                                                    } }, \\\n",
    "                                                 status_to_find : {'$exists': 1} \\\n",
    "                                                }, {'station_id': 1, 'loc': 1, status_to_find:1})\n",
    "# http://stackoverflow.com/questions/20662691/how-to-search-through-a-mongodb-collection-for-dictionary-keys-nested-in-array\n",
    "# http://stackoverflow.com/questions/5301795/mongodb-get-specific-part-of-document\n",
    "\n",
    "list_station = []\n",
    "if cursor_stations:\n",
    "    for station in cursor_stations:\n",
    "#         station.pop('_id', None)\n",
    "#         list_station.append(station)\n",
    "        try:\n",
    "            res = {'station_id' : station['station_id'],\n",
    "                   'status' : station['status'][year][month][day],\n",
    "                   'lat' : station['loc'][1],\n",
    "                   'lon' : station['loc'][0]\n",
    "                  }\n",
    "            list_station.append(res)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lat': -12.466640097,\n",
       "  'lon': -229.156012745,\n",
       "  'station_id': u'00NA',\n",
       "  'status': u'3'},\n",
       " {'lat': -12.478223997,\n",
       "  'lon': -229.017953041,\n",
       "  'station_id': u'01NA',\n",
       "  'status': u'3'},\n",
       " {'lat': 30.40742467,\n",
       "  'lon': -91.180261768,\n",
       "  'station_id': u'1LSU',\n",
       "  'status': u'3'},\n",
       " {'lat': 31.75080049,\n",
       "  'lon': -93.097603682,\n",
       "  'station_id': u'1NSU',\n",
       "  'status': u'0'},\n",
       " {'lat': 47.803480434,\n",
       "  'lon': -122.569173848,\n",
       "  'station_id': u'ESM1',\n",
       "  'status': u'3'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONGODB mXn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSet = 'UNR_SPLICE'\n",
    "database_name='GPS_mXn_'+dataSet\n",
    "database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create database\n",
    "client.drop_database(database_name)\n",
    "db =client[database_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 collections\n",
    "# for stations\n",
    "collections_stations = db.collections_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_date=datetime.strptime(beginDate, '%Y-%m-%d')\n",
    "end_date=datetime.strptime(endDate, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "today = datetime.today()\n",
    "for station in stations:\n",
    "#     GeoJson format: maybe considered in future\n",
    "#     loc = {'type' : \"Point\", \n",
    "#            'coordinates' : [float(station['long']), float(station['lat'])]\n",
    "#           }\n",
    "    loc = [float(station['long']), float(station['lat'])]   \n",
    "    document={'station_id' : station['id'], 'loc' : loc }\n",
    "        \n",
    "    for year in range(start_date.year,end_date.year+1):\n",
    "        \n",
    "        for month in range(1,13):\n",
    "            no_of_days_in_month = monthrange(year, month)[1]+1\n",
    "            days=range(1, no_of_days_in_month)\n",
    "\n",
    "            for day in days:\n",
    "                date_in_time_series=datetime(year, month, day)\n",
    "                if (date_in_time_series > today):\n",
    "                    break\n",
    "                \n",
    "                doc = document.copy()\n",
    "                doc['status'] = str(getStationState(date_in_time_series, station))             \n",
    "                doc['date'] = date_in_time_series\n",
    "                \n",
    "                # Add station data\n",
    "                data.append(doc)\n",
    "                collections_stations.insert_one(doc)\n",
    "            \n",
    "            if (date_in_time_series > today):\n",
    "                break\n",
    "                \n",
    "        if (date_in_time_series > today):\n",
    "            break            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_in_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.collections_stations.create_index([(\"loc\", pymongo.GEO2D), \\\n",
    "                                        (\"date\", pymongo.ASCENDING)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive data from mXn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get status of stations within bounding box for particular date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_min = -89.998914447\n",
    "lat_max = 83.64323665\n",
    "lon_min = -359.998835383\n",
    "lon_max = -0.037155605\n",
    "\n",
    "year = '2013'\n",
    "month = '12'\n",
    "day = '12'\n",
    "\n",
    "date_to_find_in_str = year + '-' + month + '-' + day\n",
    "date_to_find = datetime.strptime(date_to_find_in_str,'%Y-%m-%d')\n",
    "\n",
    "cursor_stations = db.collections_stations.find({ \"loc\" : \\\n",
    "                                                  { \"$geoWithin\" : \n",
    "                                                    { \"$box\" : [ [lon_min, lat_min],\\\n",
    "                                                               [lon_max, lat_max]] \\\n",
    "                                                    } }, \\\n",
    "                                                 \"date\" : {'$eq': date_to_find} \\\n",
    "                                                }, {'station_id': 1, 'status':1})\n",
    "\n",
    "list_station = []\n",
    "if cursor_stations:\n",
    "    for record in cursor_stations:\n",
    "#         print record\n",
    "        record.pop('_id', None)\n",
    "        list_station.append(record)\n",
    "#         try:\n",
    "#             res = {'id' : station['id'],\n",
    "#                    'status' : station['status'][year][month][day]\n",
    "#                   }\n",
    "#             list_station.append(record)\n",
    "#         except:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# properties('eval_path')\n",
    "\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path to json\n",
    "dataset = 'UNR_SPLICE'\n",
    "json_file_name = dataset + '_FILL.json'\n",
    "json_file = properties('eval_path') + json_file_name \n",
    "json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the json\n",
    "import json\n",
    "\n",
    "with open(json_file) as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    \n",
    "# http://stackoverflow.com/questions/5316720/python-how-to-convert-a-list-of-dictionaries-values-into-int-float-from-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert lat & long to float\n",
    "# [dict([a, int(x)] for a, x in b.iteritems()) for b in list]\n",
    "for dictionary in data['stations']:\n",
    "    dictionary['lat'] = float(dictionary['lat'])\n",
    "    dictionary['long'] = float(dictionary['long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "database_name=dataset+'_timeseries_database'\n",
    "database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete existing database\n",
    "client.drop_database(database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new database\n",
    "db = client.gps_timeseries_database\n",
    "db =client[database_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 collections\n",
    "# for meta\n",
    "collections_meta = db.collections_meta\n",
    "# for stations\n",
    "collections_stations = db.collections_stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta= {}\n",
    "meta['update_time'] = data['update_time']\n",
    "meta['data_source'] = data['data_source']\n",
    "meta['begin_date'] = data['begin_date']\n",
    "meta['end_date'] = data['end_date']\n",
    "meta['center_longitude'] = data['center_longitude']\n",
    "meta['center_latitude'] = data['center_latitude']\n",
    "meta['server_url'] = data['server_url']\n",
    "meta['stateChangeNumTxtFile'] = data['stateChangeNumTxtFile']\n",
    "meta['stateChangeNumJsInput'] = data['stateChangeNumJsInput']\n",
    "meta['allStationInputName'] = data['allStationInputName']\n",
    "meta['Filters'] = data['Filters']\n",
    "meta['video_url'] = data['video_url']\n",
    "meta['station_count'] = data['station_count']\n",
    "collections_meta.insert_one(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add station data\n",
    "collections_stations.insert_many(data['stations'])\n",
    "\n",
    "# Create index based on latitude, longitude\n",
    "db.collections_stations.create_index( [(\"lat\", pymongo.ASCENDING), \\\n",
    "                                     (\"lon\", pymongo.ASCENDING)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query to get network information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "cursor_meta = collections_meta.find()\n",
    "\n",
    "for i in cursor_meta:\n",
    "    i.pop('_id', None)\n",
    "    output = i;\n",
    "    \n",
    "    \n",
    "print output\n",
    "#     i.pop('_id', None)\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get status of stations within bounding box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# {\"age\" : {\"$gte\" : 18, \"$lte\" : 30}}\n",
    "# cursor = collections_stations.find( {\"lat\" : {\"$gte\" : 30, \"$lte\" : 60}, \\\n",
    "#                                      \"long\" : {\"$gte\" : -170, \"$lte\" : -80}})\n",
    "\n",
    "lat_min = 30\n",
    "lat_max = 60\n",
    "long_min = -170\n",
    "long_max = -80\n",
    "cursor_stations = collections_stations.find( {\"lat\" : {\"$gte\" : lat_min, \"$lte\" : lat_max}, \\\n",
    "                            \"long\" : {\"$gte\" : long_min, \"$lte\" : long_max}})\n",
    "\n",
    "if cursor_stations:\n",
    "    list_station = []\n",
    "    for i in cursor_stations:\n",
    "        i.pop('_id', None)\n",
    "        list_station.append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(list_station)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_res.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print str(df_stations[\"lat\"].min()) + \" : \" + str(df_stations[\"lat\"].max())\n",
    "print str(df_stations[\"long\"].min()) + \" : \" + str(df_stations[\"long\"].max())\n",
    "\n",
    "\n",
    "# -89.998914447 : 83.64323665\n",
    "# -359.998835383 : -0.037155605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputPath = os.path.join(os.getcwd(), \"test_output.json\")\n",
    "outputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(outputPath, 'w') as jsonfile:\n",
    "    jsonfile.write(json.dumps(output, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonfile.close()\n",
    "client.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# client.drop_database(database_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
