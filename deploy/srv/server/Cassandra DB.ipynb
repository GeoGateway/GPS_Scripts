{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All paths as in properties.py but with changed UNIX style paths for the code to execute across platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "V={}\n",
    "WORK_DIR = '/media/hru/Data/_Active_Projects/PythonRDAHMM/test'\n",
    "V['cron_path']=os.path.join(WORK_DIR, \"RDAHMM\",\"CRON_Download/\")  \n",
    "V['download_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Download/\")  \n",
    "V['script_path']=os.path.join(WORK_DIR,\"PythonRDAHMM/\") \n",
    "V['data_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Data/\")\n",
    "# temp_path is the temporary working directory for ingesting raw data\n",
    "V['temp_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"TEMP/\")\n",
    "V['model_path']=os.path.join(WORK_DIR,\"RDAHMM\",\"Model/\")\n",
    "V['eval_path']=os.path.join(WORK_DIR,\"daily/single/\")\n",
    "V['train_epoch']=\"2013-12-31\"\n",
    "V['rdahmm_bin']=os.path.join(WORK_DIR,\"RDAHMM\", \"rdahmm3\",\"bin\", \"rdahmm\")\n",
    "V['rdahmm_model_parm']=\"-data <inputFile> -T <dataCount> -D <dimensionCount> -N 5 -output_type gauss -anneal -annealfactor 1.1 -betamin 0.1 -regularize -omega 0 0 1 1.0e-6 -ntries 10 -seed 1234\"\n",
    "V['rdahmm_eval_parm']=\"-data <proBaseName>.all.input -T <dataCount> -D <dimensionCount> -N 5 -output_type gauss -A <modelBaseName>.A -B <modelBaseName>.B -pi <modelBaseName>.pi -minvalfile <modelBaseName>.minval -maxvalfile <modelBaseName>.maxval -rangefile <modelBaseName>.range -eval\"\n",
    "V['dygraphsJs']=os.path.join(WORK_DIR,\"PythonRDAHMM\",\"dygraphsJsCreator.pearl\")\n",
    "\n",
    "def properties(key):\n",
    "    return V[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the json file for Presentation layer\n",
    "This is the existing code obtained from PythonRDAHMM/create_summary_jsons.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, string, re, json\n",
    "from datetime import date, datetime, timedelta, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# command line input argument\n",
    "# dataSet = \"rawNeuTimeSeries.MEASURES_Combination\"\n",
    "dataSet = \"UNR_SPLICE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some useful global constants\n",
    "today = datetime.today()\n",
    "serverName = \"gf9.ucs.indiana.edu\"\n",
    "updateTime = str(today.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "beginDate = \"1994-01-01\"\n",
    "# endDate = str(today.strftime(\"%Y-%m-%d\"))\n",
    "endDate = '2016-10-19'\n",
    "centerLng = \"-119.7713889\"\n",
    "centerLat = \"36.7477778\"\n",
    "stateChangeNumTxtFile = \"stateChangeNums.txt\"\n",
    "stateChangeNumJsInput = \"stateChangeNums.txt.jsi\"\n",
    "allStationInputName = \"all_stations.all.input\"\n",
    "filters = \"Fill_Missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used to separate parts of the station name\n",
    "SEPARATOR_CHARACTER=\"_\"\n",
    "NO_DATA_TIME_STAMP=\"22:22:22\"\n",
    "FINAL_PATH=properties('eval_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationId(stationList, stationData):\n",
    "    #Get the station name.\n",
    "    stationName=stationList.split(SEPARATOR_CHARACTER)[2];\n",
    "\n",
    "    stationData['id'] = stationName\n",
    "    stationData['pro_dir'] = \"daily_project_\" + stationName + \"_\" + endDate\n",
    "    stationData['AFile'] = \"daily_project_\" + stationName + \".A\"\n",
    "    stationData['BFile'] = \"daily_project_\" + stationName + \".B\"\n",
    "    stationData['InputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input\"\n",
    "    stationData['RawInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.raw\"\n",
    "    stationData['SwfInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".plotswf.input\"\n",
    "    stationData['DygraphsInputFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".dygraphs.js\"\n",
    "    stationData['LFile'] = \"daily_project_\" + stationName + \".L\"\n",
    "    stationData['XPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.X.png\"\n",
    "    stationData['YPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Y.png\"\n",
    "    stationData['ZPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Z.png\"\n",
    "    stationData['XTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.X_tiny.png\"\n",
    "    stationData['YTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Y_tiny.png\"\n",
    "    stationData['ZTinyPngFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.input.Z_tiny.png\"\n",
    "    stationData['PiFile'] = \"daily_project_\" + stationName + \".pi\"\n",
    "    stationData['QFile'] = \"daily_project_\" + stationName + \"_\" + endDate + \".all.Q\"\n",
    "    stationData['MaxValFile'] = \"daily_project_\" + stationName + \".maxval\"\n",
    "    stationData['MinValFile'] = \"daily_project_\" + stationName + \".minval\"\n",
    "    stationData['RangeFile'] = \"daily_project_\" + stationName + \".range\"\n",
    "    stationData['ModelFiles'] = \"daily_project_\" + stationName + \".zip\"\n",
    "    stationData['RefFile'] = \"daily_project_\" + stationName + \".input.ref\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationStartDate(stationDir, stationData):\n",
    "    startFileName = stationDir + \"daily_project_\" + stationData['id'] + \".input.starttime\"\n",
    "    if (os.path.isfile(startFileName)):\n",
    "        with open(startFileName,\"r\") as startFile:\n",
    "            startDate = startFile.readline().rstrip()\n",
    "        startFile.close()\n",
    "    else:\n",
    "\t\tstartDate = \"1994-01-01\"\n",
    "    stationData['start_date'] = startDate\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStationRefLatLonHgt(stationDir, stationData):\n",
    "    refFileName = stationDir + stationData['RefFile']\n",
    "    refLat=\"\"\n",
    "    refLon=\"\"\n",
    "    refHgt=\"\"\n",
    "    if (os.path.isfile(refFileName)):\n",
    "        with open(refFileName,\"r\") as refFile:\n",
    "            refParts=refFile.readline().split(\" \")\n",
    "            refLat=refParts[0]\n",
    "            refLon=refParts[1]\n",
    "            refHgt=refParts[2].rstrip() # Have to chomp off the final \\n\n",
    "        refFile.close()\n",
    "    else:\n",
    "        refLat=\"1.0\"\n",
    "        refLon=\"2.0\"\n",
    "        refHgt=\"-1.0\"\n",
    "\n",
    "    stationData['lat'] = refLat      \n",
    "    stationData['long'] = refLon      \n",
    "    stationData['height'] = refHgt\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setStatusChanges(stationDir, stationData):\n",
    "    # Open the .all.Q and the .all.raw files.  We get the state from the first and\n",
    "    # the data from the second. \n",
    "    # TODO: for now, we assume these files always exist\n",
    "    qFileName = stationDir + stationData['QFile']\n",
    "    rawFileName = stationDir + stationData['RawInputFile']\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Bail out if the required files don't exist   \n",
    "    if((not os.path.isfile(qFileName)) or (not os.path.isfile(rawFileName))): \n",
    "        return \n",
    "    qFile = open(qFileName,\"r\")\n",
    "    rawFile = open(rawFileName,\"r\")\n",
    "\n",
    "    stateChanges = []\n",
    "    changeCount = 0\n",
    "    # Now step through the Q file looking for state changes\n",
    "    # If we find a state change, get the date from the raw file\n",
    "    # We will save these to the string stateChangeArray since we\n",
    "    # need to record in latest-first order\n",
    "    qline1 = qFile.readline()\n",
    "    rline1 = rawFile.readline()        \n",
    "    while True:\n",
    "        eventData = {}\n",
    "        qline2 = qFile.readline()\n",
    "        rline2 = rawFile.readline()\n",
    "        if not qline2: break\n",
    "        \n",
    "        # See if qline1 and qline2 are the same.  If so, extract the dates from rline1 and rline2\n",
    "        # The line splits below are specific to the raw file line format.\n",
    "        if (qline1.rstrip() != qline2.rstrip()):\n",
    "            eventdate = rline2.split(\" \")[1] \n",
    "            eventdate = eventdate.split(\"T\")[0]\n",
    "            oldstate = qline1.rstrip()\n",
    "            newstate = qline2.rstrip()\n",
    "            eventData['date'] = eventdate\n",
    "            eventData['from'] = oldstate\n",
    "            eventData['to'] = newstate \n",
    "            stateChanges.append(eventData)\n",
    "            changeCount += 1\n",
    "\n",
    "        # Make the previous \"next\" lines the \"first\" lines for the next comparison\n",
    "        qline1=qline2\n",
    "        rline1=rline2\n",
    "\n",
    "    stationData['status_changes'] = stateChanges\n",
    "    stationData['change_count'] = changeCount\n",
    "\n",
    "    # Clean up\n",
    "    qFile.close\n",
    "    rawFile.close\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setTimesNoData(stationDir, stationData):\n",
    "    rawFileName = stationDir + stationData['RawInputFile']\n",
    "\n",
    "    # Required file doesn't exist so bail out\n",
    "    if(not os.path.isfile(rawFileName)): return\n",
    "    rawFile = open(rawFileName, \"r\")\n",
    "    \n",
    "    noDataRanges = []\n",
    "    noDataCount = 0\n",
    "    noDataEvent = {}\n",
    "\n",
    "    # We need to set a no-data range from beginDate (for the epoch, 1994-01-01) to the day before\n",
    "    # our first data point for this station.  If the station has data before 1994-01-01, then \n",
    "    # ignore.\n",
    "    firstDataDateParts=rawFile.readline().split(\" \")[1].split(\"T\")[0].split(\"-\");\n",
    "\n",
    "    beginEpoch=date(1994,1,1)\n",
    "\n",
    "    #Convert this into a data object\n",
    "    dayMinusOne=date(int(firstDataDateParts[0]),int(firstDataDateParts[1]),int(firstDataDateParts[2]))\n",
    "    dayMinusOne-=timedelta(days=1)\n",
    "    if(dayMinusOne > beginEpoch): \n",
    "        dayMinusOneString=dayMinusOne.isoformat()\n",
    "        noDataEvent['to'] = dayMinusOneString\n",
    "        noDataEvent['from'] = beginDate\n",
    "        noDataCount += 1\n",
    "\n",
    "    #Reset the \"raw\" file to the beginning\n",
    "    rawFile.seek(0)\n",
    "\n",
    "    # Step through the file to find the starting and ending dates with no data.\n",
    "    # By convention, this occurs when the line has a timestamp T22:22:22.  Also, by\n",
    "    # convention, we will record the latest to earliest dates with no data.\n",
    "\n",
    "    while True:\n",
    "        noDataEvent = {}\n",
    "        nodata=False\n",
    "        rline1=rawFile.readline()\n",
    "        if not rline1: break\n",
    "\n",
    "        # Get the date and timestamp, following format conventions\n",
    "        fulleventdate1=rline1.split(\" \")[1]\n",
    "        eventdate1=fulleventdate1.split(\"T\")[0]\n",
    "        timestamp1=fulleventdate1.split(\"T\")[1]\n",
    "\n",
    "        # See if we have detected a no-data line\n",
    "        if(timestamp1==NO_DATA_TIME_STAMP):\n",
    "            nodata=True\n",
    "            #Keep eventdate1 in case this is an isolated no-data line.\n",
    "            eventdate_keep=eventdate1\n",
    "\n",
    "            # We have a no-data line, so step ahead until the \n",
    "            # no-data line ends.\n",
    "            while(nodata):\n",
    "                rline2=rawFile.readline()\n",
    "                if not rline2: break\n",
    "                fulleventdate2=rline2.split(\" \")[1]\n",
    "                eventdate2=fulleventdate2.split(\"T\")[0]\n",
    "                timestamp2=fulleventdate2.split(\"T\")[1]\n",
    "                if(timestamp2!=NO_DATA_TIME_STAMP):\n",
    "                    # Data exists for the second time stamp, so break out\n",
    "                    # The last no-data line was the previous line\n",
    "                    nodata=False\n",
    "                    break\n",
    "                else:\n",
    "                    # No data for this line either, so keep this timestamp\n",
    "                    # and start the while(nodata) loop again\n",
    "                    eventdate_keep=eventdate2\n",
    "\n",
    "            # We now know the range of no-data values, so insert this range, latest first\n",
    "\t    noDataEvent['to'] = eventdate_keep\n",
    "\t    noDataEvent['from'] = eventdate1\n",
    "\t    noDataRanges.append(noDataEvent)\n",
    "\t    noDataCount += 1\n",
    "            \n",
    "    # Finally, prepend the data-not-yet-available date range, from the last day of data\n",
    "    # until today's date.\n",
    "    today=date.today()\n",
    "    formattedToday=today.isoformat() \n",
    "    \n",
    "    #Reread the last event\n",
    "    rawFile.seek(0)\n",
    "    lastRawLine=rawFile.readlines()[-1]\n",
    "    lastRawDate=lastRawLine.split(\" \")[1].split(\"T\")[0]\n",
    "    lastDataDateParts=lastRawDate.split(\"-\")  # This is the last date\n",
    "    #Create a new date object out of the string we get from the file.\n",
    "    lastDataDatePlus1=date(int(lastDataDateParts[0]),int(lastDataDateParts[1]),int(lastDataDateParts[2]))\n",
    "    #Now increment this date one day.\n",
    "    lastDataDatePlus1+=timedelta(days=1)    \n",
    "    #Now convert to a string\n",
    "    lastDataDataP1String=lastDataDatePlus1.isoformat()\n",
    "\n",
    "    noDataEvent = {}\n",
    "    noDataEvent['to'] = formattedToday\n",
    "    noDataEvent['from'] = lastDataDataP1String\n",
    "    noDataRanges.append(noDataEvent)\n",
    "    noDataCount += 1\n",
    "    \n",
    "    stationData['time_nodata'] = noDataRanges\n",
    "    stationData['nodata_count'] = noDataCount\n",
    "    rawFile.close\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "projectDir = FINAL_PATH +dataSet\n",
    "if(os.path.isdir(projectDir)):\n",
    "# Open the JSON file that will contain the results\n",
    "    outputPath = FINAL_PATH+dataSet + \"_FILL.json\"\n",
    "    summaryData = {}\n",
    "        \n",
    "    summaryData['update_time'] = updateTime\n",
    "    summaryData['data_source'] = dataSet\n",
    "    summaryData['begin_date'] = beginDate\n",
    "    summaryData['end_date'] = endDate\n",
    "    summaryData['center_longitude'] = centerLng\n",
    "    summaryData['center_latitude'] = centerLat\n",
    "    summaryData['server_url'] = \"http://\" + serverName + \"/daily_rdahmmexec/daily/\" + dataSet\n",
    "    summaryData['stateChangeNumTxtFile'] = stateChangeNumTxtFile\n",
    "    summaryData['stateChangeNumJsInput'] = stateChangeNumJsInput\n",
    "    summaryData['allStationInputName'] = allStationInputName\n",
    "    summaryData['Filters'] = filters\n",
    "    summaryData['video_url'] = \"\"\n",
    "\n",
    "    stations = []\n",
    "    stationCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputPath\n",
    "\n",
    "import os, errno\n",
    "\n",
    "def silentremove(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
    "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
    "            raise # re-raise exception if a different error occured\n",
    "            \n",
    "silentremove(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for stationList in os.listdir(projectDir):\n",
    "    stationPath = projectDir + \"/\" + stationList + \"/\"\n",
    "    if (os.path.isdir(stationPath)):\n",
    "        stationData = {}\n",
    "\n",
    "        setStationId(stationList, stationData)\n",
    "        setStationStartDate(stationPath, stationData)\n",
    "        setStationRefLatLonHgt(stationPath, stationData)\n",
    "        setStatusChanges(stationPath, stationData)\n",
    "        setTimesNoData(stationPath, stationData)\n",
    "\n",
    "        stations.append(stationData)\n",
    "        stationCount += 1 \n",
    "\n",
    "summaryData['stations'] = stations\n",
    "summaryData['station_count'] = stationCount\n",
    "\n",
    "# with open(outputPath, 'w') as jsonfile:\n",
    "#     jsonfile.write(json.dumps(summaryData, sort_keys=True, indent=2))\n",
    "# jsonfile.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate client side processing to store state changes in last days\n",
    "This is python code translated from JavaScript code from https://github.com/GeoGateway/geogateway-portal/blob/master/html/js/gps-tools.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global params\n",
    "gpsStationState=[\"green\",\"red\",\"yellow\",\"lightblue\",\"blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_string_to_date(string):\n",
    "#     return datetime.strptime(string, '%Y-%m-%d').now().date()\n",
    "    return datetime.strptime(string, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkDateForData(selectedDate,noDataDates):\n",
    "    dataOnDate=True;\n",
    "    #   Selected date is after the last no-data date.\n",
    "    if selectedDate > convert_string_to_date(noDataDates[len(noDataDates) -1]['to']):\n",
    "        dataOnDate=True;\n",
    "\n",
    "    # Otherwise, check each no-data interval to see if the date falls within.\n",
    "    else:\n",
    "        for elem in noDataDates:\n",
    "            startDate = convert_string_to_date(elem['from'])\n",
    "            endDate = convert_string_to_date(elem['to'])\n",
    "\n",
    "            if (startDate <= selectedDate) & (endDate >= selectedDate):\n",
    "                dataOnDate=False\n",
    "                break\n",
    "\n",
    "    return dataOnDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Would be nice to throw an exception here \n",
    "def getPrecedingStateChange(selectedDate,statusChanges):\n",
    "    \n",
    "    stateLastDate = selectedDate\n",
    "    latestPossibleDate=convert_string_to_date(statusChanges[len(statusChanges)-1]['date'])\n",
    "    earliestPossibleDate=convert_string_to_date(statusChanges[0]['date'])\n",
    "\n",
    "#     This should actually throw an erorr since there is no earlier state change.\n",
    "    if(selectedDate <= earliestPossibleDate):\n",
    "#         stateLastDate=earliestPossibleDate;\n",
    "        stateLastDate=selectedDate\n",
    "    \n",
    "    elif (selectedDate >= latestPossibleDate):\n",
    "        stateLastDate=latestPossibleDate\n",
    "        \n",
    "    else:\n",
    "        for i, e in reversed(list(enumerate(statusChanges))):\n",
    "            stateChangeDate1 = convert_string_to_date(statusChanges[i-1]['date'])\n",
    "            stateChangeDate2 = convert_string_to_date(statusChanges[i]['date'])    \n",
    "#             The last state change date to find is the one\n",
    "#             on or before the curren date.\n",
    "            if (selectedDate >= stateChangeDate1) & (selectedDate < stateChangeDate2):\n",
    "                stateLastDate=stateChangeDate1\n",
    "#                 Dates are in order, so we can stop\n",
    "                break;\n",
    "        \n",
    "#         for(var i=statusChanges.length-1; i>0; i--) {\n",
    "#             var stateChangeDate1=new Date(statusChanges[i-1].date);\n",
    "#             var stateChangeDate2=new Date(statusChanges[i].date);\n",
    "#             //The last state change date to find is the one\n",
    "#             //on or before the curren date.\n",
    "#             if(selectedDate >= stateChangeDate1 \n",
    "#                && selectedDate < stateChangeDate2) {\n",
    "#                 stateLastDate=stateChangeDate1;\n",
    "#                 //Dates are in order, so we can stop\n",
    "#                 break;\n",
    "#             }\n",
    "#         }\n",
    "    return stateLastDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStationState(date,gpsStation):\n",
    "    #     theState=gpsStationState[0]  #This is the default.\n",
    "    theState=0\n",
    "    today=date\n",
    "    lastMonth=today - timedelta(30)\n",
    "    dayBefore=today - timedelta(1)\n",
    "\n",
    "    statusChanges=gpsStation['status_changes']\n",
    "    noDataDates=gpsStation['time_nodata']\n",
    "\n",
    "    earliestDataDate = convert_string_to_date(gpsStation['start_date'])\n",
    "    dataOnDate = checkDateForData(date,noDataDates)\n",
    "\n",
    "    #     Hopefully this big if-else construction correctly captures all the case.\n",
    "    #     It can be simplified later.\n",
    "\n",
    "    #     Provided date is before any data available for that station, so state is light blue\n",
    "    if (today < earliestDataDate):\n",
    "    #         theState=gpsStationState[3]  #light blue\n",
    "        theState=3\n",
    "    #     print (\"c1\")\n",
    "\n",
    "\n",
    "    #     Date falls within data range, there are no status changes, and data is available on date\n",
    "    elif (today >= earliestDataDate) & (len(statusChanges)==0) & (dataOnDate==True):\n",
    "    #         theState=gpsStationState[0]  # green\n",
    "        theState=0\n",
    "    #     print (\"c2\")\n",
    "\n",
    "    #     Date falls within data range, there are no state changes, and no data on selected date.\n",
    "    elif (today > earliestDataDate) & (len(statusChanges)==0) &(dataOnDate==False):\n",
    "    #         theState=gpsStationState[3]  # light blue\n",
    "        theState=3\n",
    "    #     print (\"c3\")\n",
    "\n",
    "\n",
    "    #     We have data on the date, but it proceeds the earliest state change\n",
    "    elif (today < convert_string_to_date(statusChanges[0]['date'])):\n",
    "    #         theState=gpsStationState[0]  # green\n",
    "        theState=0\n",
    "    #     print (\"c4\")\n",
    "\n",
    "    #     See if the date falls within 1 day or 1 month of a state change.\n",
    "    elif (len(statusChanges) > 0):\n",
    "        #     print (\"c5\")\n",
    "        #    Get nearest preceding state change date\n",
    "        stateLastDate=getPrecedingStateChange(date,statusChanges)\n",
    "\n",
    "    #         See if state change was yesterday.\n",
    "        if(stateLastDate > dayBefore):\n",
    "            #             theState=gpsStationState[1]  # red\n",
    "            theState=1\n",
    "\n",
    "    #         See if the station has changed state in between the\n",
    "    #         selected date and 30 days prior to the selected date.\n",
    "        elif (stateLastDate >= lastMonth) & (stateLastDate <= today):\n",
    "            #       See if we have no data within 24 hours of the selected date.            \n",
    "            if (dataOnDate == False):\n",
    "                #       Data is missing within last 24 hours and state has changed within last 30 days.\n",
    "                #                 theState=gpsStationState[4]  # blue\n",
    "                theState=4\n",
    "            else:\n",
    "                #       We have data on the date and state has changed within a 30 day window.\n",
    "                #                 theState=gpsStationState[2] # yellow\n",
    "                theState=2\n",
    "        #         No data is available on selected date for this station, and station is\n",
    "        #         not within either the 1 day or 30 day window.\n",
    "        elif(dataOnDate==False):\n",
    "            #             theState=gpsStationState[3]  # light blue\n",
    "            theState=3\n",
    "\n",
    "    return theState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE TO CASSANDRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.policies import TokenAwarePolicy\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy\n",
    "from cassandra.policies import RetryPolicy\n",
    "# import cql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from cassandra.cluster import Cluster\n",
    "cluster = Cluster(contact_points=['127.0.0.1'], \\\n",
    "                  port=9042, \\\n",
    "                  load_balancing_policy= TokenAwarePolicy(DCAwareRoundRobinPolicy(local_dc='datacenter1')), \\\n",
    "                  default_retry_policy = RetryPolicy()\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = cluster.connect('system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'system_traces',\n",
       " u'system_schema',\n",
       " u'system_auth',\n",
       " u'system',\n",
       " u'system_distributed']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = cluster.metadata.keyspaces.keys()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gps_unr_splice'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet = 'UNR_SPLICE'\n",
    "KEYSPACE='GPS_'+dataSet\n",
    "KEYSPACE=KEYSPACE.lower()\n",
    "KEYSPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.default_timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop existing database and create new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyspace_drop_stmt = session.prepare(\n",
    "    \"DROP KEYSPACE \" + KEYSPACE\n",
    ")\n",
    "\n",
    "if KEYSPACE in rows:\n",
    "    print (\"dropping existing keyspace...\")\n",
    "    session.execute(keyspace_drop_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating keyspace...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fc122803b90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"creating keyspace...\")\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE %s\n",
    "    WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': '1' }\n",
    "    \"\"\" % KEYSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"setting keyspace...\")\n",
    "# connection.setup(['127.0.0.1'], KEYSPACE)\n",
    "# connection.setup(['127.0.0.1'], KEYSPACE, protocol_version=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting keyspace...\n"
     ]
    }
   ],
   "source": [
    "print(\"setting keyspace...\")\n",
    "session = cluster.connect(KEYSPACE)\n",
    "# session.set_keyspace(KEYSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# session.execute(\"\"\"\n",
    "#     CREATE TYPE point (\n",
    "#         lon double,\n",
    "#         lat double);\n",
    "#     \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating table...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fc122815a10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"creating table...\")\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE time_series_stations (\n",
    "        date timestamp,\n",
    "        station_id text,\n",
    "        status int,\n",
    "        PRIMARY KEY (date, station_id)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "#     WITH CLUSTERING ORDER BY (date DESC);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fc14434cc10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE TABLE meta_stations (\n",
    "        station_id text PRIMARY KEY,\n",
    "        pro_dir text,\n",
    "        \"AFile\" text,\n",
    "        \"BFile\" text,\n",
    "        \"InputFile\" text,\n",
    "        \"RawInputFile\" text,\n",
    "        \"SwfInputFile\" text,\n",
    "        \"DygraphsInputFile\" text,\n",
    "        \"LFile\" text,\n",
    "        \"XPngFile\" text,\n",
    "        \"YPngFile\" text,\n",
    "        \"ZPngFile\" text,\n",
    "        \"XTinyPngFile\" text,\n",
    "        \"YTinyPngFile\" text,\n",
    "        \"ZTinyPngFile\" text,\n",
    "        \"PiFile\" text,\n",
    "        \"QFile\" text,\n",
    "        \"MaxValFile\" text,\n",
    "        \"MinValFile\" text,\n",
    "        \"RangeFile\" text,\n",
    "        \"ModelFiles\" text,\n",
    "        \"RefFile\" text,\n",
    "        start_date timestamp,\n",
    "        lat double,\n",
    "        lon double,\n",
    "        height double\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "print \"creating index...\"\n",
    "session.execute(\"\"\"\n",
    "    CREATE INDEX on %s .meta_stations (lat);\n",
    "\"\"\" % KEYSPACE)\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE INDEX on %s .meta_stations (lon);\n",
    "\"\"\" % KEYSPACE)\n",
    "\n",
    "#         loc frozen<point>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fc1227f9d90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE TABLE meta_network (\n",
    "        network_name text PRIMARY KEY,\n",
    "        update_time timestamp,\n",
    "        data_source text,\n",
    "        begin_date timestamp,\n",
    "        end_date timestamp,\n",
    "        center_longitude double,\n",
    "        center_latitude double,\n",
    "        server_url text,\n",
    "        \"stateChangeNumTxtFile\" text,\n",
    "        \"stateChangeNumJsInput\" text,\n",
    "        \"allStationInputName\" text,\n",
    "        \"Filters\" text,\n",
    "        video_url text,\n",
    "        station_count int\n",
    "    );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Alternative implementation using the Python wrapper for Cassandra DB</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# session.execute(\"\"\"\n",
    "#     DROP TABLE gps_unr_splice.time_series_stations;\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from cassandra.cqlengine import connection\n",
    "# # # Connect to the demo keyspace on our cluster running at 127.0.0.1\n",
    "# connection.setup(['127.0.0.1'], 'system', retry_connect=True)\n",
    "# session = connection.session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# session.cluster.metadata.keyspaces.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if KEYSPACE in connection.cluster.metadata.keyspaces.keys():\n",
    "#     print 'Dropping existing keyspace...'\n",
    "#     session.execute(\"DROP KEYSPACE \" + KEYSPACE)\n",
    "\n",
    "    \n",
    "# print 'Creating keyspace...'\n",
    "# session.execute(\"\"\"\n",
    "#     CREATE KEYSPACE %s\n",
    "#     WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': '1' }\n",
    "#     \"\"\" % KEYSPACE)\n",
    "\n",
    "# print(\"setting keyspace...\")\n",
    "# session.set_keyspace(KEYSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from cassandra.cqlengine import columns\n",
    "# from cassandra.cqlengine.columns import *\n",
    "# from cassandra.cqlengine.models import Model \n",
    "# from cassandra.cqlengine.usertype import UserType\n",
    "\n",
    "# from cassandra.cqlengine import connection\n",
    "# # from cqlengine import columns\n",
    "# # from cqlengine.models import Model\n",
    "# # from cqlengine import connection\n",
    "\n",
    "# from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # /https://datastax.github.io/python-driver/cqlengine/models.html#user-types\n",
    "# class Loc(UserType):\n",
    "#     lon = Double()\n",
    "#     lat = Double()\n",
    "\n",
    "# class Time_series_stations(Model):\n",
    "#     station_id = columns.Text(primary_key=True)\n",
    "#     date = columns.DateTime(primary_key=True, clustering_order=\"DESC\")\n",
    "#     status = columns.Integer()    \n",
    "    \n",
    "# class Meta_stations(Model):\n",
    "#     stations_id  = columns.Text(primary_key=True)\n",
    "#     pro_dir = columns.Text()\n",
    "#     AFile = columns.Text()\n",
    "#     BFile = columns.Text()\n",
    "#     InputFile = columns.Text()\n",
    "#     RawInputFile = columns.Text()\n",
    "#     SwfInputFile = columns.Text()\n",
    "#     DygraphsInputFile = columns.Text()\n",
    "#     LFile = columns.Text()\n",
    "#     XPngFile = columns.Text()\n",
    "#     YPngFile = columns.Text()\n",
    "#     ZPngFile = columns.Text()\n",
    "#     XTinyPngFile = columns.Text()\n",
    "#     YTinyPngFile = columns.Text()\n",
    "#     ZTinyPngFile = columns.Text()\n",
    "#     PiFile = columns.Text()\n",
    "#     QFile = columns.Text()\n",
    "#     MaxValFile = columns.Text()\n",
    "#     MinValFile = columns.Text()\n",
    "#     RangeFile = columns.Text()\n",
    "#     ModelFiles = columns.Text()\n",
    "#     RefFile = columns.Text()\n",
    "#     start_date = columns.DateTime()\n",
    "#     loc = UserDefinedType(Loc)\n",
    "#     height = columns.Text()\n",
    "    \n",
    "# class Meta_network(Model):\n",
    "#     network_name = columns.Text(primary_key=True)\n",
    "#     update_time= columns.DateTime()\n",
    "#     data_source= columns.Text()\n",
    "#     begin_date= columns.DateTime()\n",
    "#     end_date = columns.DateTime()\n",
    "#     center_longitude = columns.Double()\n",
    "#     center_latitude= columns.Double()\n",
    "#     server_url= columns.Text()\n",
    "#     stateChangeNumTxtFile= columns.Text()\n",
    "#     stateChangeNumJsInput= columns.Text()\n",
    "#     allStationInputName= columns.Text()\n",
    "#     Filters= columns.Text()\n",
    "#     video_url= columns.Text()\n",
    "#     station_count= columns.Integer()\n",
    "    \n",
    "# # users.create(name=\"Joe\", addr=address(street=\"Easy St.\", zipcode=99999))\n",
    "# # user = users.objects(name=\"Joe\")[0]\n",
    "# # print user.name, user.addr\n",
    "# # # Joe address(street=u'Easy St.', zipcode=99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from cassandra.cqlengine.management import sync_table\n",
    "# sync_table(Time_series_stations)\n",
    "# sync_table(Meta_stations)\n",
    "# sync_table(Meta_network)\n",
    "# # sync_table(Time_series_stations)\n",
    "\n",
    "# # sync_table(Time_series_stations, keyspaces=(KEYSPACE, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# meta_stations_columns = \"AFile,BFile,DygraphsInputFile,InputFile,LFile,MaxValFile, MinValFile, ModelFiles, PiFile,QFile,RangeFile,RawInputFile,RefFile,SwfInputFile,XPngFile,XTinyPngFile,YPngFile,YTinyPngFile,ZPngFile,ZTinyPngFile,height,station_id,lat,lon,pro_dir,start_date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_station_element_as_list(station):\n",
    "    dictionary = station.copy()\n",
    "    dictionary.pop('status_changes', None)\n",
    "    dictionary.pop('time_nodata', None)\n",
    "    dictionary.pop('change_count', None)\n",
    "    dictionary.pop('nodata_count', None)\n",
    "    dictlist = []\n",
    "    for key, value in sorted(dictionary.iteritems()):\n",
    "#         print key, \" : \" ,value\n",
    "        if (key == 'height') | (key == 'lat') | (key == 'long'):\n",
    "            dictlist.append(float(value))\n",
    "        elif key == 'start_date':\n",
    "            dictlist.append(datetime.strptime(value, \"%Y-%m-%d\"))\n",
    "        else:\n",
    "            dictlist.append(value)\n",
    "\n",
    "    return dictlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_date=datetime.strptime(beginDate, '%Y-%m-%d')\n",
    "end_date=datetime.strptime(endDate, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python\n",
    "from datetime import timedelta, date\n",
    "# http://www.datastax.com/dev/blog/4-simple-rules-when-using-the-datastax-drivers-for-cassandra\n",
    "from cassandra.query import BatchStatement\n",
    "\n",
    "timeseries_insert_stmt = session.prepare(\"\"\"\n",
    "        INSERT INTO Time_series_stations\n",
    "        (date, station_id, status) \n",
    "        VALUES (?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "meta_stations_insert_stmt = session.prepare(\"\"\"\n",
    "    INSERT INTO meta_stations\n",
    "    (\"AFile\",\"BFile\",\"DygraphsInputFile\",\n",
    "    \"InputFile\",\"LFile\",\"MaxValFile\", \n",
    "    \"MinValFile\", \"ModelFiles\", \n",
    "    \"PiFile\",\"QFile\",\"RangeFile\",\n",
    "    \"RawInputFile\",\"RefFile\",\n",
    "    \"SwfInputFile\",\"XPngFile\",\n",
    "    \"XTinyPngFile\",\"YPngFile\",\n",
    "    \"YTinyPngFile\",\"ZPngFile\",\n",
    "    \"ZTinyPngFile\",height,\n",
    "    station_id,lat,lon,\n",
    "    pro_dir,start_date)\n",
    "    VALUES (?, ?, ?, ?, ?, \n",
    "            ?, ?, ?, ?, ?, \n",
    "            ?, ?, ?, ?, ?, \n",
    "            ?, ?, ?, ?, ?, \n",
    "            ?, ?, ?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "meta_network_insert_stmt = session.prepare(\"\"\"\n",
    "    INSERT INTO meta_network\n",
    "    (network_name, \"Filters\", \"allStationInputName\",\n",
    "    begin_date, center_latitude, center_longitude, \n",
    "    data_source, end_date, server_url, \n",
    "    \"stateChangeNumJsInput\",\"stateChangeNumTxtFile\", \n",
    "    station_count, update_time, video_url)\n",
    "    VALUES (?, ?, ?, ?, ?, \n",
    "            ?, ?, ?, ?, ?, \n",
    "            ?, ?, ?, ?)   \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_network_list = [dataSet, \\\n",
    "                     summaryData['Filters'], \\\n",
    "                     summaryData['allStationInputName'], \\\n",
    "                     datetime.strptime(summaryData['begin_date'], \"%Y-%m-%d\"), \\\n",
    "                     float(summaryData['center_latitude']), \\\n",
    "                     float(summaryData['center_longitude']), \\\n",
    "                     summaryData['data_source'], \\\n",
    "                     datetime.strptime(summaryData['end_date'], \"%Y-%m-%d\"), \\\n",
    "                     summaryData['server_url'], \\\n",
    "                     summaryData['stateChangeNumJsInput'], \\\n",
    "                     summaryData['stateChangeNumTxtFile'], \\\n",
    "                     int(summaryData['station_count']), \\\n",
    "                     datetime.strptime(summaryData['update_time'], \"%Y-%m-%dT%H:%M:%S\"), \\\n",
    "                     summaryData['video_url']\n",
    "                    ]\n",
    "# meta_network_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fc1227b5150>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(meta_network_insert_stmt, meta_network_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, it is suggested to execute the queries in batch, when deployed in production there are too many queries to process and Cassandra DB fails to execute all the insert queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "for single_date in daterange(start_date, end_date):\n",
    "#     batch = BatchStatement()\n",
    "    \n",
    "    for station in stations:\n",
    "        session.execute(timeseries_insert_stmt, \\\n",
    "          [single_date, station['id'],getStationState(single_date, station)])        \n",
    "#         batch.add(timeseries_insert_stmt, \\\n",
    "#           [single_date, station['id'],getStationState(single_date, station)])\n",
    "#         https://datastax.github.io/python-driver/user_defined_types.html\n",
    "#         session.execute(meta_stations_insert_stmt, \\\n",
    "#           get_station_element_as_list(station))\n",
    "\n",
    "    # execute the batch\n",
    "#     session.execute(batch)\n",
    "#         Time_series_stations.create(station_id=station['id'], \\\n",
    "#                                 date=single_date, \\\n",
    "#                                status=getStationState(single_date, station))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fc122735b10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = BatchStatement()\n",
    "\n",
    "for station in stations:\n",
    "#     print station['id']\n",
    "    \n",
    "    batch.add(meta_stations_insert_stmt, \\\n",
    "          get_station_element_as_list(station))\n",
    "    \n",
    "session.execute(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIME SERIES QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://datastax.github.io/python-driver/faq.html\n",
    "# https://datastax.github.io/python-driver/api/cassandra/query.html\n",
    "from cassandra.query import named_tuple_factory\n",
    "session.row_factory = named_tuple_factory\n",
    "\n",
    "lon_min=-230\n",
    "lon_max=-90\n",
    "lat_min=-13\n",
    "lat_max=48\n",
    "\n",
    "year= 2013\n",
    "month=12\n",
    "day=31\n",
    "\n",
    "# http://www.datastax.com/dev/blog/allow-filtering-explained-2\n",
    "# http://stackoverflow.com/questions/28250534/cqlengine-queries-how-to-return-dictionary\n",
    "meta_stations_select_stmt = session.prepare(\"\"\"\n",
    "    select station_id, lat, lon from meta_stations \n",
    "    where lon >= ? and lon <= ? and lat >= ? and lat <= ?\n",
    "    allow FILTERING\n",
    "\"\"\")\n",
    "\n",
    "selected_stations = session.execute(meta_stations_select_stmt, \\\n",
    "                        [lon_min, lon_max, lat_min, lat_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(station_id=u'ESM1', lat=47.803480434, lon=-122.569173848),\n",
       " Row(station_id=u'00NA', lat=-12.466640097, lon=-229.156012745),\n",
       " Row(station_id=u'1NSU', lat=31.75080049, lon=-93.097603682),\n",
       " Row(station_id=u'01NA', lat=-12.478223997, lon=-229.017953041),\n",
       " Row(station_id=u'1LSU', lat=30.40742467, lon=-91.180261768)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_stations.current_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "futures = []\n",
    "timeseries_select_stmt = session.prepare(\"\"\"\n",
    "    SELECT status  \n",
    "    FROM time_series_stations\n",
    "    WHERE date = ? AND station_id = ?\n",
    "\"\"\")\n",
    "\n",
    "list_station = list()\n",
    "for e in list(selected_stations):\n",
    "    res ={ 'station_id' : str(e.station_id), \\\n",
    "         'lat' : str(e.lat), \\\n",
    "         'lon' : str(e.lon) \\\n",
    "         }\n",
    "    list_station.append(res)\n",
    "    futures.append(session.execute_async(timeseries_select_stmt, \\\n",
    "                              [datetime(year,month,day), str(e.station_id)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lat': '47.803480434',\n",
       "  'lon': '-122.569173848',\n",
       "  'station_id': 'ESM1',\n",
       "  'status': '3'},\n",
       " {'lat': '-12.466640097',\n",
       "  'lon': '-229.156012745',\n",
       "  'station_id': '00NA',\n",
       "  'status': '3'},\n",
       " {'lat': '31.75080049',\n",
       "  'lon': '-93.097603682',\n",
       "  'station_id': '1NSU',\n",
       "  'status': '0'},\n",
       " {'lat': '-12.478223997',\n",
       "  'lon': '-229.017953041',\n",
       "  'station_id': '01NA',\n",
       "  'status': '3'},\n",
       " {'lat': '30.40742467',\n",
       "  'lon': '-91.180261768',\n",
       "  'station_id': '1LSU',\n",
       "  'status': '0'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wait for them to complete and use the results\n",
    "\n",
    "for idx, future in enumerate(futures):\n",
    "    record = future.result()\n",
    "    \n",
    "#     print list(record)\n",
    "    t = list_station[idx]\n",
    "    t['status']= str(list(record)[0][0])\n",
    "    \n",
    "list_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cassandra.query import dict_factory\n",
    "session.row_factory = dict_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_network_select_stmt = session.prepare(\"\"\"\n",
    "    select * from meta_network \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nw = session.execute(meta_network_select_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Filters': u'Fill_Missing',\n",
       " u'allStationInputName': u'all_stations.all.input',\n",
       " u'begin_date': datetime.datetime(1994, 1, 1, 0, 0),\n",
       " u'center_latitude': 36.7477778,\n",
       " u'center_longitude': -119.7713889,\n",
       " u'data_source': u'UNR_SPLICE',\n",
       " u'end_date': datetime.datetime(2016, 10, 19, 0, 0),\n",
       " u'network_name': u'UNR_SPLICE',\n",
       " u'server_url': u'http://gf9.ucs.indiana.edu/daily_rdahmmexec/daily/UNR_SPLICE',\n",
       " u'stateChangeNumJsInput': u'stateChangeNums.txt.jsi',\n",
       " u'stateChangeNumTxtFile': u'stateChangeNums.txt',\n",
       " u'station_count': 5,\n",
       " u'update_time': datetime.datetime(2016, 11, 20, 21, 4, 40),\n",
       " u'video_url': ''}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATION META DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_id_to_find='00NA'\n",
    "\n",
    "meta_stations_select_stmt = session.prepare(\"\"\"\n",
    "    select * from meta_stations \n",
    "    where station_id = ?\n",
    "\"\"\")\n",
    "\n",
    "selected_station = session.execute(meta_stations_select_stmt, \\\n",
    "                        [station_id_to_find])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'AFile': u'daily_project_00NA.A',\n",
       " u'BFile': u'daily_project_00NA.B',\n",
       " u'DygraphsInputFile': u'daily_project_00NA_2016-10-19.dygraphs.js',\n",
       " u'InputFile': u'daily_project_00NA_2016-10-19.all.input',\n",
       " u'LFile': u'daily_project_00NA.L',\n",
       " u'MaxValFile': u'daily_project_00NA.maxval',\n",
       " u'MinValFile': u'daily_project_00NA.minval',\n",
       " u'ModelFiles': u'daily_project_00NA.zip',\n",
       " u'PiFile': u'daily_project_00NA.pi',\n",
       " u'QFile': u'daily_project_00NA_2016-10-19.all.Q',\n",
       " u'RangeFile': u'daily_project_00NA.range',\n",
       " u'RawInputFile': u'daily_project_00NA_2016-10-19.all.raw',\n",
       " u'RefFile': u'daily_project_00NA.input.ref',\n",
       " u'SwfInputFile': u'daily_project_00NA_2016-10-19.plotswf.input',\n",
       " u'XPngFile': u'daily_project_00NA_2016-10-19.all.input.X.png',\n",
       " u'XTinyPngFile': u'daily_project_00NA_2016-10-19.all.input.X_tiny.png',\n",
       " u'YPngFile': u'daily_project_00NA_2016-10-19.all.input.Y.png',\n",
       " u'YTinyPngFile': u'daily_project_00NA_2016-10-19.all.input.Y_tiny.png',\n",
       " u'ZPngFile': u'daily_project_00NA_2016-10-19.all.input.Z.png',\n",
       " u'ZTinyPngFile': u'daily_project_00NA_2016-10-19.all.input.Z_tiny.png',\n",
       " u'height': 104.8510462,\n",
       " u'lat': -12.466640097,\n",
       " u'lon': -229.156012745,\n",
       " u'pro_dir': u'daily_project_00NA_2016-10-19',\n",
       " u'start_date': datetime.datetime(2008, 3, 27, 0, 0),\n",
       " u'station_id': u'00NA'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = selected_station[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tp= session.prepare(\"\"\"\n",
    "#     SELECT * FROM time_series_stations\n",
    "#     WHERE date = ?\n",
    "# \"\"\")\n",
    "\n",
    "# tf= session.execute(tp, \\\n",
    "#                    [datetime(2016, 5, 12)])\n",
    "\n",
    "# list(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close conenction to Cassandra db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
