head	1.13;
access;
symbols;
locks; strict;
comment	@ * @;


1.13
date	99.07.22.16.52.37;	author granat;	state Exp;
branches;
next	1.12;

1.12
date	99.07.14.22.12.31;	author granat;	state Exp;
branches;
next	1.11;

1.11
date	98.06.29.22.12.05;	author granat;	state Exp;
branches;
next	1.10;

1.10
date	98.05.15.18.16.11;	author granat;	state Exp;
branches;
next	1.9;

1.9
date	98.04.21.17.37.20;	author roden;	state Exp;
branches;
next	1.8;

1.8
date	97.09.10.14.52.37;	author granat;	state Exp;
branches;
next	1.7;

1.7
date	97.01.29.21.45.09;	author agray;	state Exp;
branches;
next	1.6;

1.6
date	96.10.31.02.15.43;	author agray;	state Exp;
branches;
next	1.5;

1.5
date	96.09.23.22.45.42;	author agray;	state Exp;
branches;
next	1.4;

1.4
date	96.09.23.22.42.14;	author agray;	state Exp;
branches;
next	1.3;

1.3
date	96.07.19.17.55.19;	author agray;	state Exp;
branches;
next	1.2;

1.2
date	96.07.16.00.52.47;	author agray;	state Exp;
branches;
next	1.1;

1.1
date	96.07.16.00.43.28;	author agray;	state Exp;
branches;
next	;


desc
@module for parallel data i/o functions.
@


1.13
log
@added double and unsigned char versions of write_col_cascade()
@
text
@/*******************************************************************************
MODULE NAME
da_io_pp

ONE-LINE SYNOPSIS
Parallel general functions related to data input and output.

SCOPE OF THIS MODULE
Analogous to da_io in scope, except that functions in this module are
written to run on a parallel machine.  

SEE ALSO
Many of the functions in this module will be mirrors of their serial versions
in da_io.  

REFERENCE(S)
-

PROGRAM EXAMPLE(S)
1. /proj/cooltools/sc_pp, AG.
2. /proj/cooltools/kmeans_pp, RG.

NOTES
-

AG
*******************************************************************************/
#ifndef lint
static char rcsid[] = "$Id: da_io_pp.c,v 1.12 1999/07/14 22:12:31 granat Exp granat $";
#endif
/* 
 * $Log: da_io_pp.c,v $
 * Revision 1.12  1999/07/14 22:12:31  granat
 * added function to count total lines
 *
 * Revision 1.11  1998/06/29 22:12:05  granat
 * fixed bug in paralle control flow
 *
 * Revision 1.10  1998/05/15 18:16:11  granat
 * fixed bug in write_col_channel_cascade_pp and variants
 *
 * Revision 1.9  1998/04/21 17:37:20  roden
 * Took out #include <pvm3.h> because I think it's not needed here, and it
 * doesn't conform to da_platform.h control over presence/absence of pvm.
 *
 * Revision 1.8  1997/09/10 14:52:37  granat
 * revised many routines to reflect new experience
 *
 * Revision 1.7  1997/01/29 21:45:09  agray
 * added new includes.
 *
 * Revision 1.6  1996/10/31 02:15:43  agray
 * renamed from "da_data_pp" to "da_io_pp";
 * changed .h and .c formats throughout library;
 *
 * Revision 1.5  1996/09/23 22:45:42  agray
 * added pvm3.h inclusion.
 *
 * Revision 1.4  1996/09/23 22:42:14  agray
 * minor changes.
 *
 * Revision 1.3  1996/07/19 17:55:19  agray
 * added write_col_cascade_pp(), write_icol_cascade_pp(),
 * write_col_channel_cascade_pp(), write_icol_channel_cascade_pp()
 *
 * Revision 1.2  1996/07/16 00:52:47  agray
 * correction to comment.
 *
 * Revision 1.1  1996/07/16 00:43:28  agray
 * Initial revision
 *
 * */

/* C library */
#include <stdlib.h>
#include <stdio.h>
#include <math.h>

/* UT library */
#include "ut_error.h"
#include "ut_output.h"
#include "ut_types.h"

/* DA library */
#include "da_io.h"
#include "da_comm_pp.h"

/* this module's header */
#include "da_io_pp.h"


/*******************************************************************************
SPLIT_DATA_PP
Computes which portion of a dataset to read in, depending on which processor 
you are, for the purpose of splitting a large dataset over several processors.
my_numrows is the number of data (rows) this processor gets; startrow is the
index of the row at which this processor should start reading.
AG
*******************************************************************************/
int split_data_pp(int *startrow, int *my_numrows, int total_numrows, int pe, 
                  int numPE)

{
  int extrarows;

  extrarows = total_numrows%numPE;
  if (extrarows > pe)
  {
    *my_numrows = (int)(total_numrows/numPE)+1;
    *startrow = pe*(*my_numrows) + 1;
  }
  else
  {
    *my_numrows = (int)(total_numrows/numPE);
    *startrow = pe*(*my_numrows) + extrarows + 1;
  }

  return (UT_OK);
}


/*******************************************************************************
TOTAL_DATA_PP
Get the total number of rows stored across all processors.
RG
*******************************************************************************/
int total_data_pp(int numrows, int *total_numrows, int pe, int numPE)
{
  int p;
  int cc;
  int other_numrows;
  
  if (pe != 0) {
    /* Send out the local rows */
    cc = DA_send_msg(&numrows, 1, 0, pe, DA_INT);
    
    /* Now block until receive broadcast message from the controlling
       process giving the total number of rows */
    cc = DA_broadcast_msg(total_numrows, 1, DA_ALL_PES, 0, DA_INT);
  }
  else {
    *total_numrows = numrows;

    /* Receive the other numbers of rows */
    for (p = 1; p < numPE; p++) {
      cc = DA_recv_msg(&other_numrows, 1, p, p, DA_INT);
      *total_numrows += other_numrows;
    }

    /* Now broadcast the total number of rows */
    cc = DA_broadcast_msg(total_numrows, 1, DA_ALL_PES, 0, DA_INT);
  }
  
  return (UT_OK);
}


/*******************************************************************************
READ_MAT_CHANNEL_CASCADE
Parallel version of read_bin_matrix(), which reads part of a large matrix so
that the whole of the matrix is distributed over several processors.
RG
*******************************************************************************/
int read_mat_channel_cascade( char *datafile, int numrows, int numcols, 
                              float ***data, int startrow, int channels,
                              int pe, int numPE )
{
  int i;
  
  for (i = 0; i < numPE; i += channels) {
    if ((pe >= i) && (pe < (i + channels)))
      read_bin_subset2matrix(datafile, startrow, numrows, numcols, data);
    DA_barrier();
  }

  return( UT_OK );
}


/*******************************************************************************
WRITE_COL_CASCADE_PP
Parallel version of write_col(), which assumes that a large vector is stored
on several processors, in the order dictated by their processor numbers;
the processors each write their portion of the vector in turn to the same
destination file.
RG
*******************************************************************************/
int write_col_cascade_pp(outfile, dim, v, mode, pe, numPE)

  char   *outfile;  /* name of file to write to */
  int    dim;       /* length of the row vector to be printed */
  float  *v;        /* vector to be printed */
  char   *mode;     /* file i/o mode to use: e.g. "w", "r", or "a" */
  int    pe;        /* processor number */
{
  int     cc;
  int     dummy;

  /* check to see that the previous processors have already finished */

  DA_barrier();

  /* if this isn't the first processes, check to see if the previous
     process is finished writing */

  if (pe > 0)
    cc = DA_recv_msg( &dummy, 1, pe-1, numPE, DA_INT);

  if (pe == 0)
    write_col(outfile, dim, v, mode);
  else
    write_col(outfile, dim, v, "a");

  /* if this isn't the last process, send a message to the next process
     telling it to start writing */

  if (pe < (numPE - 1)) /* Not the last process */
    cc = DA_send_msg( &dummy, 1, pe+1, numPE, DA_INT);

  return (UT_OK);
}


/*******************************************************************************
WRITE_ICOL_CASCADE_PP
Similar to write_col_cascade_pp().
RG
*******************************************************************************/
int write_icol_cascade_pp(outfile, dim, v, mode, pe, numPE)

  char   *outfile;  /* name of file to write to */
  int    dim;       /* length of the row vector to be printed */
  int    *v;        /* vector to be printed */
  char   *mode;     /* file i/o mode to use: e.g. "w", "r", or "a" */
  int    pe;        /* processor number */
{
  int     cc;
  int     dummy;

  /* check to see that the previous processors have already finished */

  DA_barrier();

  /* if this isn't the first processes, check to see if the previous
     process is finished writing */

  if (pe > 0)
    cc = DA_recv_msg( &dummy, 1, pe-1, numPE, DA_INT);

  if (pe == 0)
    write_icol(outfile, dim, v, mode);
  else {
    write_icol(outfile, dim, v, "a");
  }

  /* if this isn't the last process, send a message to the next process
     telling it to start writing */

  if (pe < (numPE - 1)) /* Not the last process */
    cc = DA_send_msg( &dummy, 1, pe+1, numPE, DA_INT);

  return (UT_OK);
}


/*******************************************************************************
WRITE_DCOL_CASCADE_PP
Similar to write_col_cascade_pp().
RG
*******************************************************************************/
int write_dcol_cascade_pp(outfile, dim, v, mode, pe, numPE)

  char   *outfile;  /* name of file to write to */
  int    dim;       /* length of the row vector to be printed */
  double *v;        /* vector to be printed */
  char   *mode;     /* file i/o mode to use: e.g. "w", "r", or "a" */
  int    pe;        /* processor number */
{
  int     cc;
  int     dummy;

  /* check to see that the previous processors have already finished */

  DA_barrier();

  /* if this isn't the first processes, check to see if the previous
     process is finished writing */

  if (pe > 0)
    cc = DA_recv_msg( &dummy, 1, pe-1, numPE, DA_INT);

  if (pe == 0)
    write_dcol(outfile, dim, v, mode);
  else {
    write_dcol(outfile, dim, v, "a");
  }

  /* if this isn't the last process, send a message to the next process
     telling it to start writing */

  if (pe < (numPE - 1)) /* Not the last process */
    cc = DA_send_msg( &dummy, 1, pe+1, numPE, DA_INT);

  return (UT_OK);
}


/*******************************************************************************
WRITE_CCOL_CASCADE_PP
Similar to write_col_cascade_pp().
RG
*******************************************************************************/
int write_ccol_cascade_pp(outfile, dim, v, mode, pe, numPE)

  char   *outfile;  /* name of file to write to */
  int    dim;       /* length of the row vector to be printed */
  unsigned char *v; /* vector to be printed */
  char   *mode;     /* file i/o mode to use: e.g. "w", "r", or "a" */
  int    pe;        /* processor number */
{
  int     cc;
  int     dummy;

  /* check to see that the previous processors have already finished */

  DA_barrier();

  /* if this isn't the first processes, check to see if the previous
     process is finished writing */

  if (pe > 0)
    cc = DA_recv_msg( &dummy, 1, pe-1, numPE, DA_INT);

  if (pe == 0)
    write_ccol(outfile, dim, v, mode);
  else {
    write_ccol(outfile, dim, v, "a");
  }

  /* if this isn't the last process, send a message to the next process
     telling it to start writing */

  if (pe < (numPE - 1)) /* Not the last process */
    cc = DA_send_msg( &dummy, 1, pe+1, numPE, DA_INT);

  return (UT_OK);
}


/*******************************************************************************
WRITE_COL_CHANNEL_CASCADE_PP
Parallel version of write_col(), which assumes that a large vector is stored
on several processors, in the order dictated by their processor numbers.
Similar to write_col_cascade_pp(), except that this function will only use
up to the number of i/o channels specified, to avoid thrashing the i/o. 
It is generally faster than write_col_cascade_pp().
AG
******************************************************************************/
int write_col_channel_cascade_pp(outfile, dim, v, num_channels, mode, pe, numPE)

  char   *outfile;      /* name of file to write to */
  int    dim;           /* length of the row vector to be printed */
  float  *v;            /* vector to be printed */
  int    num_channels;  /* number of i/o channels to occupy */
  char   *mode;         /* file i/o mode to use: e.g. "w", "r", or "a" */
  int    pe;            /* processor number */
  int    numPE;         /* total number of processors */
{
  int     i;

  DA_barrier();
  for(i = 0; i < numPE; i += num_channels)
  {
    /* only print in groups of num_channels processors */
    if ((pe >= i) && (pe < (i + num_channels)))
    {
      /* print our portion of the column */
      if (pe == 0)
        write_col(outfile, dim, v, mode);
      else
        write_col(outfile, dim, v, "a");

      DA_barrier();
    }
  }

  return (UT_OK);
}


/*******************************************************************************
WRITE_ICOL_CHANNEL_CASCADE_PP
Similar to write_icol_channel_cascade_pp().
AG
*******************************************************************************/
int write_icol_channel_cascade_pp(outfile, dim, v, num_channels, mode, pe,numPE)

  char   *outfile;      /* name of file to write to */
  int    dim;           /* length of the row vector to be printed */
  int    *v;            /* vector to be printed */
  int    num_channels;  /* number of i/o channels to occupy */
  char   *mode;         /* file i/o mode to use: e.g. "w", "r", or "a" */
  int    pe;            /* processor number */
  int    numPE;         /* total number of processors */
{
  int     i;

  DA_barrier();
  for(i=0; i<numPE; i+=num_channels)
  {
    /* only print in groups of num_channels processors */
    if ((pe >= i) && (pe < (i + num_channels)))
    {
      /* print our portion of the column */
      if (pe==0)
        write_icol(outfile, dim, v, mode);
      else
        write_icol(outfile, dim, v, "a");

      DA_barrier();
    }
  }

  return (UT_OK);
}
@


1.12
log
@added function to count total lines
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_io_pp.c,v 1.11 1998/06/29 22:12:05 granat Exp granat $";
d33 3
d256 84
@


1.11
log
@fixed bug in paralle control flow
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_io_pp.c,v 1.10 1998/05/15 18:16:11 granat Exp granat $";
d33 3
d120 36
d215 1
a215 1
    cc = DA_send_msg( &pe, 1, pe+1, numPE, DA_INT);
d249 1
a249 1
  else
d251 1
d257 1
a257 1
    cc = DA_send_msg( &pe, 1, pe+1, numPE, DA_INT);
@


1.10
log
@fixed bug in write_col_channel_cascade_pp and variants
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_io_pp.c,v 1.9 1998/04/21 17:37:20 roden Exp granat $";
d33 3
d154 1
a154 1
  int     i, cc;
a155 1
  FILE    *fp;
d195 1
a195 1
  int     i,cc;
a196 1
  FILE    *fp;
d244 1
a244 1
  barrier();
d256 1
a256 1
      barrier();
d281 1
a281 1
  barrier();
d285 1
a285 1
    if ((pe >= i) && (pe <= (i + num_channels)))
d293 1
a293 1
      barrier();
@


1.9
log
@Took out #include <pvm3.h> because I think it's not needed here, and it
doesn't conform to da_platform.h control over presence/absence of pvm.
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_io_pp.c,v 1.8 1997/09/10 14:52:37 granat Exp roden $";
d33 4
d125 2
a126 2
  for (i=0;i<numPE;i+=channels) {
    if ((pe>=i)&&(pe<(i+channels)))
d244 1
a244 1
  for(i=0; i<numPE; i+=num_channels)
d247 1
a247 1
    if ((pe >= i) && (pe <= (i + num_channels)))
d250 1
a250 1
      if (pe==0)
@


1.8
log
@revised many routines to reflect new experience
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_io_pp.c,v 1.7 1997/01/29 21:45:09 agray Exp granat $";
d33 3
a64 3

/* PVM library */
#include <pvm3.h>
@


1.7
log
@added new includes.
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_io_pp.c,v 1.6 1996/10/31 02:15:43 agray Exp agray $";
d33 3
d110 22
d137 1
a137 1
AG
d148 1
d152 10
a161 6
  barrier();
  for(i=0;i<pe;i++)
    cc = verify_recv_msg(i,i);
  
  /* print our portion of the column */
  if (pe==0)
d166 5
a170 3
  /* tell everyone this processor is done */
  if (numPE > 0)
    cc = broadcast_msg(&pe,1,DA_ALL_PES,pe,DA_INT);
d179 1
a179 1
AG
d190 1
d194 10
a203 6
  barrier();
  for(i=0;i<pe;i++)
    cc = verify_recv_msg(i,i);
  
  /* print our portion of the column */
  if (pe==0)
d208 5
a212 3
  /* tell everyone this processor is done */
  if (numPE > 0)
    cc = broadcast_msg(&pe,1,DA_ALL_PES,pe,DA_INT);
a293 2


@


1.6
log
@renamed from "da_data_pp" to "da_io_pp";
changed .h and .c formats throughout library;
@
text
@d29 1
a29 1
static char rcsid[] = "$Id: da_io_pp.c,v 1.5 1996/09/23 22:45:42 agray Exp agray $";
d33 4
d64 2
@


1.5
log
@added pvm3.h inclusion.
@
text
@d2 2
d5 2
a6 8
  Title:     da_data_pp
  Author:    Alexander Gray
  Function:  Data analysis routines, Numerical Recipes style.
             This file contains data i/o routines for use on parallel machines.
             The PVM parallel programming library is used.
  Reference: -
  How used:  First use - ~/unsup/sc/sc.c.
  Notes:     - 
d8 19
d29 1
a29 1
static char rcsid[] = "$Id: da_data_pp.c,v 1.4 1996/09/23 22:42:14 agray Exp agray $";
d32 4
a35 1
 * $Log: da_data_pp.c,v $
d51 1
d56 1
d59 1
d62 7
a68 3
#include "da_data.h"
#include "da_msg_pp.h"
#include "da_data_pp.h"
d71 6
a76 5
 SPLIT_DATA_PP
 Computes which portion of a dataset to read in, depending on which processor 
 you are, for the purpose of splitting a large dataset over several processors.
 my_numrows is the number of data (rows) this processor gets; startrow is the
 index of the row at which this processor should start reading.
d101 6
a106 5
 WRITE_COL_CASCADE_PP
 Parallel version of write_col(), which assumes that a large vector is stored
 on several processors, in the order dictated by their processor numbers;
 the processors each write their portion of the vector in turn to the same
 destination file.
d139 3
a141 2
 WRITE_ICOL_CASCADE_PP
 Similar to write_col_cascade_pp().
d174 8
a181 7
 WRITE_COL_CHANNEL_CASCADE_PP
 Parallel version of write_col(), which assumes that a large vector is stored
 on several processors, in the order dictated by their processor numbers.
 Similar to write_col_cascade_pp(), except that this function will only use
 up to the number of i/o channels specified, to avoid thrashing the i/o. 
 It is generally faster than write_col_cascade_pp().
*******************************************************************************/
d215 3
a217 2
 WRITE_ICOL_CHANNEL_CASCADE_PP
 Similar to write_icol_channel_cascade_pp().
@


1.4
log
@minor changes.
@
text
@d14 1
a14 1
static char rcsid[] = "$Id: da_data_pp.c,v 1.3 1996/07/19 17:55:19 agray Exp agray $";
d18 3
d36 2
@


1.3
log
@added write_col_cascade_pp(), write_icol_cascade_pp(),
write_col_channel_cascade_pp(), write_icol_channel_cascade_pp()
@
text
@d14 1
a14 1
static char rcsid[] = "$Id: da_data_pp.c,v 1.2 1996/07/16 00:52:47 agray Exp agray $";
d18 4
d33 5
d47 2
a48 1
int split_data_pp(int *startrow, int *my_numrows, int pe, int numPE)
a49 4
  int    *startrow;   /* row index this processor should start reading from */
  int    *my_numrows; /* total number of rows this processor should read */
  int    pe;          /* processor number of current process */
  int    numPE;       /* total number of processors */
d53 1
a53 1
  extrarows = numrows%numPE;
d56 2
a57 2
    my_numrows = (int)(numrows/numPE)+1;
    startrow = pe*my_numrows + 1;
d61 2
a62 2
    my_numrows = (int)(numrows/numPE);
    startrow = pe*my_numrows + extrarows + 1;
d76 1
a76 1
int write_col_cascade_pp(outfile, dim, v, mode, pe)
d84 1
d110 1
a110 1
int write_icol_cascade_pp(outfile, dim, v, mode, pe)
d118 1
@


1.2
log
@correction to comment.
@
text
@d14 1
a14 1
static char rcsid[] = "$Id: da_data_pp.c,v 1.1 1996/07/16 00:43:28 agray Exp agray $";
d18 3
d38 1
a38 1
int split_data_pp(int pe, int numPE, int *startrow, int *my_numrows)
d40 2
a43 2
  int    *startrow;   /* row index this processor should start reading from */
  int    *my_numrows; /* total number of rows this processor should read */
d61 147
@


1.1
log
@Initial revision
@
text
@d9 1
a9 1
  How used:  First use - ~/sc/sc.c.
d14 1
a14 1
static char rcsid[] = "$Id$";
d17 4
a20 1
 * $Log$
@
