\documentstyle[11pt,epsf]{article}
%\documentstyle[16pt,epsf]{article}

%\usepackage{slitex}
%\documentstyle{slides}


\begin{document}

\title{Designing and Building a Framework for Code Reuse}

\author{Alex Gray}

\date{7 October 1996}

\maketitle


\begin{abstract}
	Code reuse should not need to be explained.  Assuming that you have an
environment which demands lots of code, over time, which ends up needing to 
solve problems and sub-problems which crop up over and over in some form,
code reuse saves time and mental energy, and reduces bugs and cost over time.
This is true on a one-programmer scale, and especially true on a multi-
programmer scale.

	I will talk about what I perceive to be the ISSUES in building a framework 
for code reuse.  Then I'll present one set of solutions to these issues which
comprises a library I've been building and using for all of my C software
development since January.  
This set of solutions is based on common-sense 
principles of software engineering applied to the particular needs of the MLS 
group.  

	My intention in this talk is to illustrate, using a real example, the 
process of designing a framework for code reuse which can be used to create a 
group or multi-group resource.  
The principles I will outline will encompass 
both C/C++ and Matlab code, and should be illustrative for anyone contemplating
building a large reusable code infrastructure.
\end{abstract}

\pagebreak
\part{Intro}

\section*{Sources/Blame}
These are my opinions based on my experience in programming, and my observations
	of several good programmers (Joe Roden, Dennis Decoste, Ramesh Subramonian,
	Rob Farber, Paul Hilfinger, Kurt Partridge, Gordon Gray) and lots and lots
	of good, bad, and medium code.

\section*{Overview}
So: I designed and wrote a library.  But instead of just presenting its
characteristics, I'm going to build it from first principles so you can see
why it's the way it is. My library is being used already as a multi-programmer 
resource by 3 people in the group.

After seeing what i have to say, you may either wish to use my library and
its framework directly, or perhaps use it as a starting point for doing your
own thing.  Or use it as an anti-example.

\section*{Outline}
\begin{enumerate}
\item Intro
\item Issues in Code Reuse:  The Art of Software Development
\item The Choices I Made:  The DA Library Framework
\item Sharing Code: Multiple Programmers
\end{enumerate}

\pagebreak
\part{Issues in Code Reuse: The Art of Software Development}

Doing it properly requires careful thought, direct experience, and good
computer science intuition.

\section*{0  Easy:}
\begin{itemize}
\item {\bf knowing the syntax and semantics, how to compile}

	e.g. to define a structure, say "struct $<$name$>$ \{ ... \};"
\end{itemize}

absolutely anyone can do this

\pagebreak
\section*{1  Takes experience:}
\begin{itemize}
\item {\bf finding useful idioms}

		e.g. to use a structure like a basic type, say "typedef struct $<$name$>$
			{ ... } $<$newname$>$;"

\item {\bf safeguarding against common pitfalls, learning common causes of bugs}

		e.g. to avoid seg. faults, have cases that check whether a pointer is
			NULL

\item {\bf learning solutions to common problems}

		e.g. to make a two-dimensional array, say "$<$array$>$ = ($<$type$>$**) 
			malloc($<$maxsize$>$ * sizeof($<$type$>$*)); then say "for (i..."

\item {\bf having a base of coded solutions, for copy-and-modify}

		e.g. any time you want to process command-line arguments, just copy
			the part of main() in the last program you wrote that does that,
			and modify it according to your current needs

\item {\bf coding clearly}

		e.g. indenting, source code commenting, simple flow of control

\item {\bf coding consistently}

		e.g. consistent naming of functions, formatting, levels of abstraction

\item {\bf coding generally}

		e.g. instead of making a function to create a particular array which
			has a particular size, make it take a size parameter

\item {\bf using good tools}

		e.g. gdb, purify, emacs

\item {\bf making code efficient}

		e.g. macros, eliminating unnecessary pointer dereferences, buffering
			and contiguity, doing operations in-situ to save memory

\item {\bf realizing an algorithm in code naturally}

		e.g. keep variables consistent with notation of the reference, keep the
			conceptual steps identifiable

	Requires attention to the environment you are in:

\item {\bf learning the needs of the environment you are in}

		e.g. time constraints, language constraints, machine constraints, 
			maintenance needs, speed needs

\item {\bf observing the common programming problems}

		e.g. particular functions, how they are used, what executables are 
			needed, what program options are usually needed
\end{itemize}

\pagebreak
\section*{2  Takes experience and intuition:}

\begin{itemize}
\item {\bf how much to trade space vs. time for your problems and environment}

		e.g. can we normally afford to read everything into RAM?  can we 
			normally afford to read directly from disk?

\item {\bf how much to trade development time vs. code quality for your 
	problems}

		e.g. what are the deadlines like?  how much time is there to design,
			test, add features, ...?

\item {\bf how much to trade abstraction vs. transparency in your code base}

		e.g. do you make read\_dataset() a function or not?  do you typedef
			float** to matrix or not?

\item {\bf how much to trade flexibility vs. consistency in your code base}

		e.g. do you use a function naming convention or not?  do you allow
			TRUE and FALSE to exist or not?

\item {\bf how much to trade proprietariness vs. external dependence in your 
	code}

		base (aka customization vs. standardization)
		e.g. do you depend on a standard library or not?  do you adopt someone
			else's naming or formatting conventions or not?

\item {\bf how much to trade optimization vs. clarity in your code base}

		e.g. do you call the read\_dataset() function or just do it in line?
			do you store three different conceptual values in the same variable
				and do sequential computations or do you use three different
				variables?

\item {\bf how much to trade optimization vs. portability in your code base}

		e.g. do you use in-line assembly code for inner loops or not?  do you
			use machine-specific routines or not?
\end{itemize}

\pagebreak
\section*{3  NOW we have the ability to decide:}

\begin{itemize}
\item {\bf what mechanism for code reuse to create}

		e.g. 4GL/code dumper, library, visual programming language

\item {\bf what conceptual language you want to create}

		e.g. filter-based, connection-based, interactive, object-based

\item {\bf what the levels of abstraction are}

		e.g. how much does each function do, how much is functionality hidden
			at each level

\item {\bf how to organize and index your code}

		e.g. size and scope of modules, where to draw library boundaries

\item {\bf what functionality is in the library}

		e.g. definition of scope of the library, what's definitely not in the
			library

\item {\bf what conventions to set up and enforce}

		e.g. naming, command-line argument processing, error codes, error 
			reporting, scope of functionality of functions and programs

\item {\bf what language to use}

		e.g. C, C++, Matlab, Java, mixtures of languages

\item {\bf how to manage the building of the code base}

		e.g. all at once, incremental
\end{itemize}


\pagebreak
\part{The choices I made:  The DA Library Framework}

\section*{Environment:}
\begin{itemize}
\item {\bf lots of quick prototypes}
\item {\bf variety of problems}
\item {\bf all related to data analysis and manipulation}
\item {\bf code users are the data analysis experts (people in the group)}
\item {\bf data mining - big datasets}
\item {\bf matlab is already used a lot }
\end{itemize}

\section*{Overriding principle \#1: simplicity for efficiency}
\begin{itemize}
\item {\bf keep things as simple as possible}

	\begin{itemize}
	\item easier to learn, to maintain, to reason about
	\end{itemize}

\item {\bf 80/20 rule}
	
	\begin{itemize}
	\item only the first 80\% is worth the effort, due to 
	uncertainty	about future needs (aka "don't overoptimize")

	\item corrolary: you can usually do the first 80\% by doing simple things

	\item rapid prototyping ("move then consider") is the most efficient way to 
		explore a solution space
		(seeing as you go is also a corrolary: starting with something simple
		points the direction for the rest of what you need; then you're done)
	\end{itemize}
\end{itemize}

\section*{Overriding principle \#2: the past predicts the present}
\begin{itemize}
\item what you last did has a lot to do with what you want to do now

	\begin{itemize}
	\item i.e., copy-and-modify is the most efficient way to write new code
	\end{itemize}

\item this implies that effort should be put into:

	\begin{itemize}
	\item indexing and organizing old code
	\item making old code understandable and trusted for reuse
	\item using a consistent style and conventions so that you can use old 
		snippets of code verbatim
	\end{itemize}

\item however, you don't want to limited by the past

	\begin{itemize}
	\item so you keep the conventions
		to a minimum and in general maintain as much flexibility as possible
	\end{itemize}

\end{itemize}

\section*{Overriding principle \#3: avoid the big time productivity killers}

	the killers are things that waste your time in huge chunks all at once;
	they are:

\begin{itemize}
\item {\bf 	bugs in the underlying foundation}

	\begin{itemize}
	\item this requires poking deep into unknown territory
	\end{itemize}

\item {\bf 	incompatibilities in the underlying foundation}

	\begin{itemize}
	\item this requires porting and possibly other hacks
	\end{itemize}

\end{itemize}

\pagebreak
\section*{3  Highest-level decisions:}
\begin{itemize}
\item {\bf what mechanism for code reuse to create}

	\begin{itemize}
	\item a library
	\item this is programmer-oriented, rather than user-oriented
	\item allows maximum flexibility for creating a variety of solutions
	\item we don't have the solution space focused enough to warrant an 
		overarching interface
	\item even if we want a code dumper, we'll need a library first
	\end{itemize}

\item {\bf what conceptual language you want to create}

	\begin{itemize}
	\item functions on matrices and vectors
	\item matlab-like
	\item this dovetails nicely with the wide use of matlab in the group for
			prototyping functions
	\item historical note:
			got inspiration from Mike Burl, who had a handful of functions
				that did linear algebra operations, e.g. mat\_vec\_mult(), 
				which sometimes called NR functions
	\item a matlab-like C library has these advantages over matlab:
		\begin{itemize}	
		\item faster
		\item has the possibility of complex structures
		\item you can use only the data types you need
		\item no memory limit; can also play with disk as needed
		\item can write parallel code
		\item more portable
		\item free
		\item the style of programming encourages memory efficiency, not the 
				reverse
		\end{itemize}
		
	\item it has these disadvantages compared to matlab:
		\begin{itemize}			
		\item takes longer to write something in C
				(however, I claim that this approach closes that gap signifi-
				cantly)
		\item not as many pre-written functions as in matlab
				(again...)
		\item matlab has graphics, an interpreter - this is what makes it great
				for prototyping
		\item note that matlab and c already communicate both ways
		\begin{enumerate}
			\item c can use matlab code via a matlab2c dumper

				this is okay but a theorem about translators is that they do
					not preserve understandability very well and in this case
					there is no reason to think it makes things more efficient;
				so i wouldn't recommend its use most of the time; it only saves
					you interpretation time

			\item matlab can use c code through .mex files

				this is where the matlab-like model used in my c code can make
					this interfacing trivial
		\end{enumerate}
		\end{itemize}

	\item so by linking C and matlab with a common conceptual language, we 
			can conceive of a prototype --$>$ fast implementation 
			pipeline
	\item we also achieve a general union of mental models between the two 
			media
		\end{itemize}

\item {\bf what functionality is in the library}
	\begin{itemize}

	\item layers:
		\begin{verbatim}
			    CoolTools
			~~~~~~~~~~~~~~~~~
			       DA
			~~~~~~~~~~~~~~~~~
			  NR, PVM   util
		\end{verbatim}
	\item util contains very low-level operations which are very broadly useful

			e.g. open\_file(), log(), log\_printf(), get\_args()
	\item NR contains low-level numerical operations

			e.g. ludcmp(), svd()
	\item PVM contains portable, low-level parallel programming operations

			e.g. pvm\_send(), pvm\_recv()
	\item DA contains low-level data analysis operations all the way up to high-
			level

			e.g. read\_matrix(), invert\_matrix(), mult\_mat(), pca(), 
				compute\_hmm\_likelihood(), est\_hmm\_params(), learn\_hmm()
	\item CoolTools are often just program shells which handle input and output,
			and call one high-level function in the DA lib.; this blurring of
			the line between program and function is very useful for integrating
			code in new places
	\end{itemize}

\item {\bf what the levels of abstraction are}
	\begin{itemize}
	\item so there are strong and useful conceptual separations at the library
			levels, and other separations at the module level within DA and util
	\item the guiding principle is to have the programmer, at any given place,
			see what he needs to see but no more
	\item so abstraction is spread out at all levels, at an equal proportion of
			abstraction to transparency
	\item this is most easily learned by emulating existing parts of the code
	\end{itemize}

\item {\bf how to organize and index your code}
	\begin{itemize}
	\item right now, there are these modules:
		\begin{verbatim}
			da\_data.c      da\_data\_pp.c
			da\_dist.c      da\_dist\_pp.c
			da\_unsup.c     da\_unsup\_pp.c
			                da\_msg\_pp.c
			da\_linalg.c
			da\_rand.c
			da\_prob.c
		\end{verbatim}
	\item there is a convention that equivalent parallel functions mirror the
			serial versions, which are always written first; similarly for the
			modules
	\item the idea is to have these module boundaries evolve on an as-needed
			basis, since the whole library can grow in arbitrary directions,
			depending on what code needs writing
	\end{itemize}
\item {\bf what conventions to set up and enforce}
	\begin{itemize}
	\item there is a small and simple set of conventions for naming various
			things and commenting
	\item other than that, there are recommended, existing mechanisms for 
			things like command-line argument processing, error codes, and
			error reporting, based on the util library
	\item so the constraints on programming individuality are designed to be just
			enough to maintain minimal consistency, but not more than that
	\end{itemize}
\item {\bf what language to use}
	\begin{itemize}
	\item C
	\item chose this because it is the most portable language on earth, everyone
			knows it, and it has no problems with efficiency or expressiveness,
			vast infrastructure
	\item the only time you need to go beyond C is to buy some extra capability
	\item C++ - extra capabilities may or may not add a lot for this type of code,
			infrastructure is significant but still not ubiquitous - demands
			some investigation though
	\item Java - extra capability, taking a big risk as far as portability;
			can probably get the extra capability without the risk by just using
			it for interfaces and calling C code underneath
	\item Matlab - previously discussed; seems complementary
	\end{itemize}
\item {\bf how to manage the building of the code base}
	\begin{itemize}
	\item whenever you write a program, you abstract out what are general
			functions, and put them into the library - you try to make your
			program depend as little as appropriate on non-library code
	\item this way, two things happen:
	\begin{enumerate}
		\item the library grows by almost the amount of new code you had to write,
				so all that work can be reused later
		\item the conscious thought you put into writing code for the library is
				no more than what you had to do anyway, except for putting it
				into the right places
	\end{enumerate}
	\item over time, the library grows to cover everything that has been done in
			the past, yet takes very little conscious effort to expand, since
			it is done on an as-needed basis
	\end{itemize}
\end{itemize}

\pagebreak
\section*{2  Code-level decisions:}
\begin{itemize}
\item {\bf how much to trade space vs. time for your problems and environment}
	\begin{itemize}
	item fact: disk access is about 1 million times as slow as memory access
	\item so computationally intensive algorithms (which all of ours tend to be)
			never want to touch disk unless it is absolutely necessary
	\item so my whole code base is designed with RAM maximization, including
			parallelization, in mind
	\item all memory allocation is done at the top level and passed in, so that
			all RAM usage is explicit - this coding style forces the user to
			be memory conscious
	\item the allocation and free'ing routines also allow tracking of RAM usage
			at any given point
	\item routines for reading in data in a buffered style implement the quick 
			piping of data to RAM
	\item in general, i will lean toward using more space to reduce time since i
			have these ways of increasing/maximizing RAM
	\end{itemize}
\item {\bf how much to trade development time vs. code quality for your 
	problems}
	\begin{itemize}
	\item fairly short turnaround times are required, allowing enough time to
			get functionality going and validate
	\item features tend to be of a standard type, so templatization is especially
			powerful here
	\item an evolutionary approach to building the library means that you can put
			in quick versions of functions in the library to get code working, 
			then underlying functions get improved later then all software
			depending on that part of library improve; this is a way to improve
			code later without having to change it at the high level - it
			relies on good use of abstraction
	\end{itemize}
\item {\bf how much to trade abstraction vs. transparency in your code base}
	\begin{itemize}
	\item don't hide data structures
		\begin{itemize}
		\item thm: data structures are the conceptual foundation of the whole
				program, so making these simple makes the program simple, and
				making these complex makes the program complex
		\item hiding what the data really is (like a float** typedef'ed to a
				matrix) is an annoying obstacle to anyone who wants to start
				doing something with that data structure, like debugging on it
		\item exception - when there is an absolutely natural 'object', or a
				large group of structures that are related, and it becomes 
				cumbersome or unclear not to define a more complex structure
		\end{itemize}
	\item hide functions wherever possible;
		\begin{itemize}
		\item the simplest way to understand a function is to know its input, 
				output, and conceptually what it does - everything else can be
				hidden as long as the name is descriptive
		\end{itemize}
	\item these two ideas make a program into a conceptual box-and-arrow flow
			diagram
	\end{itemize}
\item {\bf how much to trade flexibility vs. consistency in your code base}
	\begin{itemize}
	\item define just a few constants like error codes and true/false, a 
			naming convention for similar functions, and enforcing liberal
			commenting - that's it, besides some prewritten mechanisms that are
			recommended for use
	\item this is probably the minimum for the consistency i want - the indivi-
			dual programmer is otherwise allowed to be an individual
	\item this is very important for keeping multiple users happy and to allow
			adaptation of styles for different tasks
	\end{itemize}
\item {\bf how much to trade proprietariness vs. external dependence in your 
	code}
	\begin{itemize}
	\item base (aka customization vs. standardization)
	\item we leverage a large base of standard code - NR lib.;
			this is done mainly because that would be time-consuming and error-
				prone for me to write myself; for most other things, this is not
				the case; but where it is, relying on external code can make 
				sense
	\item NR dependence is only okay because:
		\begin{itemize}
		\item it is almost free (\$50)
		\item it is a ubiquitous standard
		\item it has great documentation compared to similar packages (the book)
		\item the massive user base ensures the pointing out and fixing of bugs
		\item it is highly portable
		\item it actually exists in multiple languages
		\item it also uses a very minimal set of conventions, so it imposes little
		\item we can build upon it by using its own data structures, which are 
				actually pretty good (contiguous allocation, simple indexing,
				pointer indexing)
		\item in general these underlying NR functions can be used as is and don't
			have to change; it is the higher level concepts and algorithms that
			you want proprietary so you can change them
		\item all the same goes for PVM; we may actually move to the other one
		\end{itemize}
	\end{itemize}
\item {\bf how much to trade optimization vs. clarity in your code base}
	\begin{itemize}
	\item optimizations which start to obfuscate the meaning of the code, or its
			translation to the original algorithm, are avoided unless absolutely
			needed, and in this case these parts of the code are thoroughly
			documented and isolated if machine-specific
	\item clarity is far more important in the long run
	\item transparent algorithm implementation is what one needs in an algorithm-
			development house
	\item optimization is only necessary for highly specific situations, usually
	\item we normally just want to prototype something, so optimization should
			only be done on an as-needed basis
	\item optimization is a never-ending process, so doing it as needed also tells
			you when to stop
	\end{itemize}
\item {\bf how much to trade optimization vs. portability in your code base}
	\begin{itemize}
	\item again, optimization loses out - portability is more important
	\item the need to port doesn't seem to happen often, but when it does, you
			may be in big trouble if you haven't designed for it
	\item examples - cray, hp convex, pc workstations are on the horizon
	\end{itemize}
\end{itemize}

\pagebreak
\section*{1  Simple stuff:}
\begin{itemize}
\item {\bf finding useful idioms}
	\begin{itemize}
	\item these are used by convention throughout the library; obscure idioms are
			avoided
	\end{itemize}
\item {\bf safeguarding against common pitfalls, learning common causes of bugs}
	\begin{itemize}
	\item done by convention throughout the library
	\end{itemize}
\item {\bf learning solutions to common problems}
	\begin{itemize}
	\item these are implemented as low-level routines which reside in the bottom
			layer of abstraction
	\end{itemize}
\item {\bf having a base of coded solutions, for copy-and-modify}
	\begin{itemize}
	\item this is done by having all the pieces of old solutions visible, from
			util fns. up to CoolTools		
	\end{itemize}
\item {\bf coding clearly}
	\begin{itemize}
	\item done by convention throughout the library
	\end{itemize}
\item {\bf coding consistently}
	\begin{itemize}
	\item done by convention throughout the library
	\end{itemize}
\item {\bf coding generally}
	\begin{itemize}
	\item done by convention throughout the library
	\end{itemize}
\item {\bf using good tools}
	\begin{itemize}
	\item up to the user
	\end{itemize}
\item {\bf making code efficient}
	\begin{itemize}
	\item simple and obvious things are done by convention throughout the library
	\end{itemize}
\item {\bf realizing an algorithm in code naturally}
	\begin{itemize}
	\item keep variables consistent with notation of the reference, keep the
			conceptual steps identifiable
	\item this is one reason I allow lots of flexibility in naming intra-function
			entities (variables)
	\end{itemize}
\end{itemize}

	The environment:
\begin{itemize}
\item {\bf meeting the needs of the environment you are in}
	\begin{itemize}
	\item allows quicker construction of working code overall, due to
			using pre-written, pre-tested chunks of code of various sizes;
			facilitating copy-and-modify at all levels, including main(), the
				Makefile, the command-line arguments;
			lots of examples to draw upon for quickly forming a mental model
	\item C and matlab are used here
	\item workstations are pretty fast - don't need to super-optimize for time, 
			but need to be concerned about memory due to massive datasets
	\item we have parallel capability, so our setup should allow natural parallel-
			ization
	\item we need to write code that can be understood between users and 
			modified by others
	\end{itemize}
\item {\bf facilitating the solution of the common programming problems}
	\begin{itemize}
	\item this is handled by doing everything in terms of the library; then all
			past code is available
	\end{itemize}
\end{itemize}

\pagebreak
\part{Sharing Code:  Multiple Programmers}

There are several issues which arise when trying to share code:

\begin{itemize}
\item {\bf how are additions and modifications to the code base managed?}

		e.g. not at all, by democracy, by republic, by dictatorship
\item {\bf how is the overall organization of the code base evolved and 
	maintained?}

		e.g. not at all, by democracy, by republic, by dictatorship
\item {\bf how are changes to existing code made known to all users?}

		e.g. not at all, change log, memos sent
\item {\bf how is a single code base modified by multiple users asynchronously?}

		e.g. free-for-all, memos sent and agreements made, version control
\item {\bf how is knowledge about the code base represented?}

		e.g. what's in the library, how to modify it, how to use it, what the
			current state is, bugs
\end{itemize}

\pagebreak
My set of solutions:

\begin{itemize}
\item {\bf how are additions and modifications to the code base managed?}
	\begin{itemize}
	\item by a library manager (conscientious dictator)
	\item this is necessary to maintain integrity and consistency of the code base
	\item this reduces discussion to a minimum in most cases; only issues with
			any substance are brought up for discussion by the library manager
	\item this gives some quality assurance mechanism for new code
	\item assigns responsibility to someone for making sure the library is kept
			in good shape
	\end{itemize}
\item {\bf how is the overall organization of the code base evolved and 
	maintained?}
	\begin{itemize}
	\item by the library manager
	\item someone has to be constantly monitoring the overall evolution of the
			library, or it will become a mess quickly
	\end{itemize}
\item {\bf how are changes to existing code made known to all users?}
	\begin{itemize}
	\item RCS logs in each source file
	\item e-mail list - changes which affect users will be broadcast
	\end{itemize}
\item {\bf how is a single code base modified by multiple users asynchronously?}
	\begin{itemize}
	\item RCS
	\end{itemize}
\item {\bf how is knowledge about the code base represented?}
	\begin{itemize}
	\item significant commenting in all source code, including descriptions for
			each function
	\item web page FAQ - includes how to use, how to modify, pointers to docs
			below
	\item index.txt - this is automatically created with a make-index script
	\item help script - just grabs one thing from the index.txt file
	\item conventions.txt
	\item design.txt
	\item todo.txt
	\item lots of examples - all the CoolTools
	\end{itemize}
\end{itemize}

ALSO SHOW:
list of functions in the library, by module
list of cooltools made with library

\end{document}
